from shiny import App, ui, render, reactive, Session
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import numpy as np

# Set Mapbox token for map functionality
# Using a public token for testing - in production, use your own token
px.set_mapbox_access_token("pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw")
import xarray as xr
import os
import re  # For regular expressions to parse filenames
from shinywidgets import output_widget, render_widget
from ipyleaflet import Map, GeoJSON, DrawControl, TileLayer
from shapely.geometry import shape, Polygon
try:
    from scipy.stats import linregress  # Preferred for trendlines
except Exception:  # Fallback if SciPy is unavailable
    linregress = None

# Configuration
DATA_DIR = "."

def compute_trendline(x_values, y_values):
    """Compute linear trendline for given x and y arrays.
    Returns (slope, intercept, x_line, y_line, p_value). Skips NaNs and requires >=2 points.
    p_value indicates statistical significance of the trend.
    """
    import numpy as np
    try:
        x_arr = np.asarray(x_values, dtype=float)
        y_arr = np.asarray(y_values, dtype=float)
        mask = np.isfinite(x_arr) & np.isfinite(y_arr)
        x_clean = x_arr[mask]
        y_clean = y_arr[mask]
        if x_clean.size < 2:
            return None
        if linregress is not None:
            res = linregress(x_clean, y_clean)
            slope, intercept, p_value = res.slope, res.intercept, res.pvalue
        else:
            slope, intercept = np.polyfit(x_clean, y_clean, 1)
            p_value = None  # p-value not available without scipy
        x_line = np.array([x_clean.min(), x_clean.max()])
        y_line = slope * x_line + intercept
        return slope, intercept, x_line, y_line, p_value
    except Exception:
        return None
import geojson
import pickle  # For caching grid cells
import hashlib  # For stable hashing
import traceback

# GLOBAL CACHE: Regional timeseries data to avoid re-extracting large polygons
# Key: (gridboxes_tuple, variable, model, frequency, ssp)
# Value: DataFrame with Date and variable columns
_REGIONAL_TIMESERIES_CACHE = {}

# GLOBAL CACHE: Spatial change calculations for Variable Map page
# Key: (variable, model, scenario, start_year, end_year, change_type)
# Value: Dictionary mapping gridbox_id to {abs_change, pct_change, start_val, end_val}
_SPATIAL_CHANGE_CACHE = {}

from datetime import datetime
import signal
import time

# Configure logging for production/development control
import logging
logging.basicConfig(
    level=logging.INFO,  # Set to logging.DEBUG for development, logging.INFO for production
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)


# --- Constants and File Paths ---
def load_netcdf_file_list():
    try:
        with open("netcdf_files.txt", "r") as f:
            files = [line.strip() for line in f if line.strip()]
            # Filter out Livneh files to exclude them from the dashboard
            filtered_files = [f for f in files if "livneh" not in f.lower()]
            logging.info(f"Found {len(files)} NetCDF files in netcdf_files.txt")
            logging.info(f"Filtered out Livneh files, using {len(filtered_files)} files for dashboard")
            return filtered_files
    except FileNotFoundError:
        logging.error("Error: netcdf_files.txt not found")
        return []

NETCDF_FILE_LIST = load_netcdf_file_list()
# Latitude and Longitude bounds for the Santa Barbara region
SB_LAT_MIN, SB_LAT_MAX = 33.39, 35.14
SB_LON_MIN, SB_LON_MAX = -120.77, -118.96

# Cache file for common grid cells
GRID_CACHE_FILE = "grid_cells_cache.pkl"

def save_grid_cells_cache(common_cells, file_list_hash):
    """Save common grid cells to cache file"""
    try:
        cache_data = {
            'common_cells': common_cells,
            'file_list_hash': file_list_hash,
            'timestamp': os.path.getmtime("netcdf_files.txt") if os.path.exists("netcdf_files.txt") else 0
        }
        with open(GRID_CACHE_FILE, 'wb') as f:
            pickle.dump(cache_data, f)
        logging.info(f"Saved {len(common_cells)} grid cells to cache")
    except Exception as e:
        logging.error(f"Error saving grid cache: {e}")

def compute_file_list_hash():
    """Compute hash of current file list for cache validation"""
    # Use hashlib for stable hashing
    file_list_str = '\n'.join(sorted(NETCDF_FILE_LIST))
    return hashlib.md5(file_list_str.encode()).hexdigest()

def load_grid_cells_cache():
    """Load common grid cells from cache file"""
    try:
        if not os.path.exists(GRID_CACHE_FILE):
            logging.info("Cache file does not exist")
            return None, None
        
        with open(GRID_CACHE_FILE, 'rb') as f:
            cache_data = pickle.load(f)
        
        # Only check file list hash for cache validity
        current_hash = compute_file_list_hash()
        cached_hash = cache_data['file_list_hash']
        
        logging.debug(f"DEBUG: Current file list hash: {current_hash}")
        logging.debug(f"DEBUG: Cached file list hash: {cached_hash}")
        logging.debug(f"DEBUG: Hash match: {current_hash == cached_hash}")
        
        if current_hash == cached_hash:
            logging.info(f"Loaded {len(cache_data['common_cells'])} grid cells from cache")
            return cache_data['common_cells'], cache_data['file_list_hash']
        else:
            logging.info("Cache invalid - file list changed")
            return None, None
    except Exception as e:
        logging.error(f"Error loading grid cache: {e}")
        return None, None

# --- Helper Functions for NetCDF Data Parsing and Metadata ---
def parse_filename(filename):
    """
    Parses a NetCDF filename to extract variable, model, and SSP.
    Example: '06083_tasmin_day_TaiESM1_ssp370_r1i1p1f1.nc'
    Handles filenames with '(1)' like '...MPI-ESM1-2-HR_ssp585_r1i1p1f1 (1).nc'
    """
    # Remove the .nc extension and any trailing '(number)'
    base_filename = os.path.splitext(filename)[0]
    base_filename = re.sub(r' \((\d+)\)$', '', base_filename)  # remove (1) if present

    parts = base_filename.split('_')
    # Handle the original file names which don't have the '06083' prefix
    if len(parts) >= 4 and parts[0] in ["tasmin", "tasmax", "pr", "hursmax", "hursmin", "huss", "rsds", "wspeed"]:
        # Assuming format like: var_day_model_ssp_r1i1p1f1
        variable = parts[0]
        model = parts[2]
        ssp = parts[3]
        return variable, model, ssp
    elif len(parts) >= 5:  # For new files like 06083_hursmax_day_GFDL-ESM4_ssp245_r1i1p1f1.nc
        # Assuming format like: 06083_var_day_model_ssp_r1i1p1f1
        variable = parts[1]  # e.g., 'tasmin', 'pr'
        model = parts[3]     # e.g., 'TaiESM1', 'GFDL-ESM4'
        ssp = parts[4]       # e.g., 'ssp370', 'ssp245'
        return variable, model, ssp
    return None, None, None

def get_variable_metadata(variable_name):
    """
    Returns display name, units, and a default color for a given climate variable.
    """
    metadata = {
        # Projected Climate Variables
        "tasmin": {"display_name": "Minimum Temperature Projections", "units": "Â°F", "color": "blue"},
        "tasmax": {"display_name": "Maximum Temperature Projections", "units": "Â°F", "color": "red"},
        "pr": {"display_name": "Precipitation Projections", "units": "inches/day", "color": "green"},
        "hursmax": {"display_name": "Maximum Relative Humidity Projections", "units": "%", "color": "purple"},
        "hursmin": {"display_name": "Minimum Relative Humidity Projections", "units": "%", "color": "darkorange"},
        "huss": {"display_name": "Specific Humidity Projections", "units": "kg/kg", "color": "brown"},
        "rsds": {"display_name": "Surface Downwelling Shortwave Radiation Projections", "units": "W/mÂ²", "color": "goldenrod"},
        "wspeed": {"display_name": "Wind Speed Projections", "units": "m/s", "color": "teal"},
        
        # Observed Climate Variables
        "obs_pr": {"display_name": "Observed Precipitation", "units": "inches/day", "color": "darkgreen"},
        "obs_tmax": {"display_name": "Observed Maximum Temperature", "units": "Â°F", "color": "darkred"},
        "obs_tmin": {"display_name": "Observed Minimum Temperature", "units": "Â°F", "color": "darkblue"},
    }
    
    result = metadata.get(variable_name, {"display_name": variable_name, "units": "Unknown", "color": "gray"})
    return result

def get_variable_units_for_frequency(variable_name, frequency):
    """
    Returns the appropriate units for a variable based on its aggregation frequency.
    For precipitation, units change when aggregated (daily -> monthly/annual).
    """
    base_metadata = get_variable_metadata(variable_name)
    base_units = base_metadata.get('units', 'Unknown')
    
    # For precipitation variables, units change when aggregated
    if variable_name in ["pr", "obs_pr"]:
        if frequency in ["monthly", "annual"]:
            # Aggregated precipitation: remove "/day" from units
            if base_units.endswith("/day"):
                return base_units.replace("/day", "")
            elif base_units.endswith(" inches/day"):
                return base_units.replace(" inches/day", " inches")
            else:
                return "inches"  # fallback
        else:
            return base_units  # daily frequency keeps original units
    else:
        # All other variables keep the same units regardless of frequency
        return base_units

def get_variable_display_name(variable_name):
    """Get the user-friendly display name for a climate variable"""
    if not variable_name:
        return ""
    
    # Metadata dictionary with display names for variables
    metadata = {
        # Projected Climate Variables
        "tasmin": "Minimum Temperature Projections",
        "tasmax": "Maximum Temperature Projections",
        "pr": "Precipitation Projections",
        "hursmax": "Maximum Relative Humidity Projections",
        "hursmin": "Minimum Relative Humidity Projections",
        "huss": "Specific Humidity Projections",
        "rsds": "Surface Downwelling Shortwave Radiation Projections",
        "wspeed": "Wind Speed Projections",
        
        # Observed Climate Variables
        "obs_pr": "Observed Precipitation",
        "obs_tmax": "Observed Maximum Temperature",
        "obs_tmin": "Observed Minimum Temperature"
    }
    
    # Try to get the display name from the metadata
    display_name = metadata.get(variable_name)
    
    # If no display name found, create a generic one
    if not display_name:
        # Remove 'obs_' prefix if present
        clean_var = variable_name.replace('obs_', '')
        
        # Split the variable name and capitalize words
        words = clean_var.split('_')
        words = [word.capitalize() for word in words]
        
        # Add appropriate suffix
        if variable_name.startswith('obs_'):
            display_name = f"Observed {' '.join(words)}"
        else:
            display_name = f"{' '.join(words)} Projections"
    
    return display_name

def create_variable_choices(available_variables):
    """Create variable choices with display names for dropdowns
    Returns a dict where keys are technical names (values returned) and values are display names (labels shown)
    """
    if not available_variables:
        return {"": "Select a variable..."}
    
    # Create choices as a dictionary with technical names as keys and display names as values
    # This is the correct format for Shiny: {value: label}
    choices = {"": "Select a variable..."}  # Start with blank option
    
    # Group variables by type (projected vs observational)
    proj_vars = [var for var in available_variables if not var.startswith('obs_')]
    obs_vars = [var for var in available_variables if var.startswith('obs_')]
    
    # Add projected variables if present
    if proj_vars:
        for var in sorted(set(proj_vars)):
            display_name = get_variable_display_name(var)
            choices[var] = display_name  # REVERSED: technical name as key, display name as value
    
    # Add observational variables if present
    if obs_vars:
        for var in sorted(set(obs_vars)):
            display_name = get_variable_display_name(var)
            choices[var] = display_name  # REVERSED: technical name as key, display name as value
    
    return choices

def get_variable_from_display_name(display_name, available_variables):
    """Get the technical variable name from a display name"""
    logging.debug(f"DEBUG: get_variable_from_display_name - Input display_name: {display_name}")
    logging.debug(f"DEBUG: get_variable_from_display_name - Available variables: {available_variables}")
    
    if not display_name:
        return ""
    
    # Mapping of display names to technical variable names
    display_to_var = {
        # Projected Climate Variables
        "Minimum Temperature Projections": "tasmin",
        "Maximum Temperature Projections": "tasmax",
        "Precipitation Projections": "pr",
        "Maximum Relative Humidity Projections": "hursmax",
        "Minimum Relative Humidity Projections": "hursmin",
        "Specific Humidity Projections": "huss",
        "Surface Downwelling Shortwave Radiation Projections": "rsds",
        "Wind Speed Projections": "wspeed",
        
        # Observed Climate Variables
        "Observed Precipitation": "obs_pr",
        "Observed Maximum Temperature": "obs_tmax",
        "Observed Minimum Temperature": "obs_tmin"
    }
    
    # First, try direct mapping
    var = display_to_var.get(display_name)
    logging.debug(f"DEBUG: get_variable_from_display_name - Direct mapped var: {var}")
    
    # If found and in available variables, return it
    if var and var in available_variables:
        logging.debug(f"DEBUG: get_variable_from_display_name - Returning direct mapped var: {var}")
        return var
    
    # If not found or not in available variables, try a more flexible approach
    for var in available_variables:
        display_var = get_variable_display_name(var)
        logging.debug(f"DEBUG: get_variable_from_display_name - Checking var: {var}, display_var: {display_var}")
        if display_var == display_name:
            logging.debug(f"DEBUG: get_variable_from_display_name - Returning matched var: {var}")
            return var
    
    # If no match found, return the display name as-is (fallback)
    logging.debug(f"DEBUG: get_variable_from_display_name - No match found, returning display_name: {display_name}")
    return display_name

def _are_units_compatible(units1, units2):
    """Check if two units are compatible for plotting together"""
    if not units1 or not units2:
        return False
    
    # Normalize units for comparison
    units1_norm = units1.lower().strip()
    units2_norm = units2.lower().strip()
    
    # Temperature units
    temp_units = ['Â°c', 'c', 'celsius', 'Â°f', 'f', 'fahrenheit', 'k', 'kelvin']
    if units1_norm in temp_units and units2_norm in temp_units:
        logging.debug(f"DEBUG: Compatible temperature units - {units1} vs {units2}")
        return True
    
    # Relative humidity units (percentage)
    relative_humidity_units = ['%', 'percent']
    
    # Specific humidity units (mass ratio)
    specific_humidity_units = ['kg/kg', 'g/kg', 'kg kg-1', 'g g-1', '1']
    
    # Only allow compatibility within the same humidity type
    if units1_norm in relative_humidity_units and units2_norm in relative_humidity_units:
        logging.debug(f"DEBUG: Compatible relative humidity units - {units1} vs {units2}")
        return True
    if units1_norm in specific_humidity_units and units2_norm in specific_humidity_units:
        logging.debug(f"DEBUG: Compatible specific humidity units - {units1} vs {units2}")
        return True
    
    # Precipitation units
    precip_units = ['mm/day', 'mm day-1', 'mm d-1', 'kg m-2 s-1', 'mm/s', 'inches/day', 'inches']
    if units1_norm in precip_units and units2_norm in precip_units:
        logging.debug(f"DEBUG: Compatible precipitation units - {units1} vs {units2}")
        return True
    
    # Radiation units
    radiation_units = ['w/m2', 'w m-2', 'w/mÂ²', 'w m-Â²', 'wm-2', 'wm-Â²', 'W m-2']
    if units1_norm in radiation_units and units2_norm in radiation_units:
        logging.debug(f"DEBUG: Compatible radiation units - {units1} vs {units2}")
        return True
    
    # Wind speed units
    wind_units = ['m/s', 'm s-1', 'ms-1', 'km/h', 'km h-1', 'mph']
    if units1_norm in wind_units and units2_norm in wind_units:
        logging.debug(f"DEBUG: Compatible wind speed units - {units1} vs {units2}")
        return True
    
    # Pressure units
    pressure_units = ['pa', 'hpa', 'mbar', 'bar', 'millibar']
    if units1_norm in pressure_units and units2_norm in pressure_units:
        logging.debug(f"DEBUG: Compatible pressure units - {units1} vs {units2}")
        return True
    
    logging.debug(f"DEBUG: Incompatible units - {units1} vs {units2}")
    return False

def check_unit_compatibility(var1, var2, filepaths_map=None, freq1=None, freq2=None):
    """Check if two variables have compatible units for plotting together using standardized units
    
    Args:
        var1: First variable name
        var2: Second variable name
        filepaths_map: Optional map of file paths (ignored - using standardized units)
        freq1: Optional frequency for var1 ('daily', 'monthly', 'annual')
        freq2: Optional frequency for var2 ('daily', 'monthly', 'annual')
    """
    if not var1 or not var2:
        logging.debug(f"DEBUG: check_unit_compatibility - One or both variables are None: var1={var1}, var2={var2}")
        return False
    
    # For precipitation, frequency compatibility:
    # - daily: inches/day (rate) - cannot mix with monthly/annual
    # - monthly: inches (total) - CAN mix with annual
    # - annual: inches (total) - CAN mix with monthly
    if var1 in ["pr", "obs_pr"] and var2 in ["pr", "obs_pr"]:
        if freq1 and freq2:
            # Daily data (rate) cannot be mixed with aggregated data (totals)
            if freq1 == 'daily' and freq2 in ['monthly', 'annual']:
                logging.debug(f"DEBUG: Incompatible - daily precipitation (inches/day) cannot mix with {freq2} precipitation (inches)")
                return False
            if freq2 == 'daily' and freq1 in ['monthly', 'annual']:
                logging.debug(f"DEBUG: Incompatible - daily precipitation (inches/day) cannot mix with {freq1} precipitation (inches)")
                return False
            # Monthly and annual can be plotted together (both are totals in inches)
            logging.debug(f"DEBUG: Compatible - precipitation variables with frequencies {freq1} and {freq2}")
        else:
            logging.debug(f"DEBUG: Compatible - both precipitation variables (frequency check skipped)")
        return True
    
    # Same variable with same or unspecified frequency is always compatible
    if var1 == var2:
        logging.debug(f"DEBUG: Compatible - same variable: {var1}")
        return True
    
    # Use standardized units from variable metadata instead of file units
    logging.debug("DEBUG: Checking compatibility using standardized units")
    metadata1 = get_variable_metadata(var1)
    metadata2 = get_variable_metadata(var2)
    
    units1 = metadata1.get('units', 'Unknown')
    units2 = metadata2.get('units', 'Unknown')
    
    logging.debug(f"DEBUG: Standardized units - {var1}: {units1}, {var2}: {units2}")
    
    # Same units can be plotted together
    if units1 == units2:
        logging.debug(f"DEBUG: Compatible - same standardized units: {units1}")
        return True
    
    # Check for unit compatibility (e.g., Â°C vs Â°F, kg/kg vs g/kg)
    if _are_units_compatible(units1, units2):
        logging.debug(f"DEBUG: Compatible - compatible standardized units: {units1} vs {units2}")
        return True
    
    # If units are not compatible, they cannot be plotted together
    logging.debug(f"DEBUG: Incompatible - different standardized units: {units1} vs {units2}")
    return False

def _get_actual_units_from_files(variable, filepaths_map):
    """Extract actual units from NetCDF files for a given variable"""
    # Handle observed variables with known target units
    if variable in ("obs_tmin", "obs_tmax"):
        return "Â°F"
    if variable == "obs_pr":
        return "inches/day"

    if not filepaths_map:
        return None
    
    # Find a file with this variable
    sample_file = None
    for (var, model, ssp), filepath in filepaths_map.items():
        if var == variable:
            sample_file = filepath
            break
    
    if not sample_file:
        logging.debug(f"DEBUG: No file found for variable {variable}")
        return None
    
    try:
        # Open the file and check the variable's units attribute
        with xr.open_dataset(sample_file, decode_times=False) as ds:
            if variable in ds.data_vars:
                var_data = ds[variable]
                if hasattr(var_data, 'attrs') and 'units' in var_data.attrs:
                    units = var_data.attrs['units']
                    logging.debug(f"DEBUG: Found units for {variable}: {units}")
                    return units
                else:
                    logging.debug(f"DEBUG: No units attribute found for {variable}")
                    return None
            else:
                logging.debug(f"DEBUG: Variable {variable} not found in dataset")
                return None
    except Exception as e:
        logging.debug(f"DEBUG: Error reading units from {sample_file}: {e}")
        return None

def _fallback_unit_compatibility(var1, var2):
    """Fallback compatibility check using logical grouping when actual units can't be determined"""
    logging.debug(f"DEBUG: Fallback compatibility check for {var1} vs {var2}")
    
    # Observed vs projection temperature variables are compatible
    if ((var1 in ['obs_tmax', 'obs_tmin'] and var2 in ['tasmax', 'tasmin']) or
        (var2 in ['obs_tmax', 'obs_tmin'] and var1 in ['tasmax', 'tasmin'])):
        logging.debug(f"DEBUG: Compatible - observed temperature vs projection temperature")
        return True
    
    # Observed precipitation vs projection precipitation are compatible
    if ((var1 == 'obs_pr' and var2 == 'pr') or (var2 == 'obs_pr' and var1 == 'pr')):
        logging.debug(f"DEBUG: Compatible - observed precipitation vs projection precipitation")
        return True
    
    # Temperature variables can be plotted together
    if var1 in ['tasmax', 'tasmin'] and var2 in ['tasmax', 'tasmin']:
        logging.debug(f"DEBUG: Compatible - both temperature variables")
        return True
    
    # Relative humidity variables can be plotted together
    if var1 in ['hursmax', 'hursmin'] and var2 in ['hursmax', 'hursmin']:
        logging.debug(f"DEBUG: Compatible - both relative humidity variables")
        return True
    
    # Specific humidity variables can be plotted together (but not with relative humidity)
    if var1 in ['huss'] and var2 in ['huss']:
        logging.debug(f"DEBUG: Compatible - both specific humidity variables")
        return True
    
    # Precipitation and radiation are usually incompatible with others
    if var1 in ['pr', 'rsds'] or var2 in ['pr', 'rsds']:
        if var1 != var2:
            logging.debug(f"DEBUG: Not compatible - precipitation/radiation with other variables")
            return False
    
    # Wind speed is usually incompatible with others
    if var1 in ['wspeed'] or var2 in ['wspeed']:
        if var1 != var2:
            logging.debug(f"DEBUG: Not compatible - wind speed with other variables")
            return False
    
    # For other combinations, treat as incompatible (no permissive fallback)
    logging.debug(f"DEBUG: Not compatible - different variable groups")
    return False

def get_grid_in_bounds(ds):
    lats = ds['lat'].values
    lons = ds['lon'].values
    cells = []
    lon_min, lon_max = SB_LON_MIN, SB_LON_MAX
    if np.any(lons > 180) and SB_LON_MIN < 0:
        lon_min += 360
        lon_max += 360
    
    logging.debug(f"DEBUG: Santa Barbara bounds: lat {SB_LAT_MIN}-{SB_LAT_MAX}, lon {SB_LON_MIN}-{SB_LON_MAX}")
    logging.debug(f"DEBUG: Dataset bounds: lat {lats.min():.3f}-{lats.max():.3f}, lon {lons.min():.3f}-{lons.max():.3f}")
    
    for lat_idx, lat in enumerate(lats):
        for lon_idx, lon in enumerate(lons):
            if (SB_LAT_MIN <= lat <= SB_LAT_MAX and
                lon_min <= lon <= lon_max):
                cells.append((lat_idx, lon_idx))
    
    logging.debug(f"DEBUG: Found {len(cells)} grid cells within Santa Barbara bounds")
    return cells

def find_common_valid_gridcells(filepaths):
    if not filepaths:
        logging.debug("DEBUG: No filepaths provided to find_common_valid_gridcells")
        return set()
    
    logging.debug(f"DEBUG: Processing {len(filepaths)} files to find common valid grid cells")
    
    try:
        with xr.open_dataset(filepaths[0], engine="netcdf4", decode_times=False) as ref_ds:
            initial_cells = get_grid_in_bounds(ref_ds)
            common_cells = set(initial_cells)
            logging.debug(f"DEBUG: Initial grid cells from bounds: {len(initial_cells)}")
    except Exception as e:
        logging.debug(f"DEBUG: Error opening reference file {filepaths[0]}: {e}")
        return set()
    
    processed_files = 0
    for filepath in filepaths[1:]:
        if not common_cells:
            logging.debug("DEBUG: No common cells remaining, stopping")
            break
            
        processed_files += 1
        if processed_files % 2 == 0:  # More frequent progress updates
            logging.debug(f"DEBUG: Processed {processed_files}/{len(filepaths)} files, {len(common_cells)} cells remaining")
            
        try:
            with xr.open_dataset(filepath, engine="netcdf4", decode_times=False) as ds:
                var = list(ds.data_vars.keys())[0]
                # More efficient: check if the variable has any non-null data at all first
                if ds[var].isnull().all():
                    logging.debug(f"DEBUG: File {filepath} has no valid data, skipping")
                    continue
                    
                mask = ~ds[var].isnull().all(dim='time')
                valid_cells = set()
                
                # Only check cells that are still in common_cells
                for lat_idx, lon_idx in common_cells:
                    if (lat_idx < mask.sizes['lat'] and
                        lon_idx < mask.sizes['lon'] and
                        mask.isel(lat=lat_idx, lon=lon_idx).item()):
                        valid_cells.add((lat_idx, lon_idx))
                
                common_cells = common_cells.intersection(valid_cells)
                
        except Exception as e:
            logging.debug(f"DEBUG: Error processing {filepath}: {e}")
            # Continue processing other files instead of returning empty set
            continue
    
    result = sorted(common_cells)
    logging.debug(f"DEBUG: Final result: {len(result)} common valid grid cells")
    return result

# --- Pre-computation and Data Loading Functions ---
# Cache for observational gridbox mappings to avoid repeated calculations
_obs_gridbox_cache = {}
# Cache for map data to speed up loading
_map_data_cache = None


def load_observed_dataset(variable):
    """Load observed dataset for a given variable"""
    try:
        if variable == 'obs_tmax':
            obs_fp = "livneh_day_Tmax_clip_15Oct2014.1950-2013.nc"
        elif variable == 'obs_tmin':
            obs_fp = "livneh_day_Tmin_clip_15Oct2014.1950-2013.nc"
        elif variable == 'obs_pr':
            obs_fp = "livneh_day_Prec_clip_15Oct2014.1950-2013.nc"
        else:
            logging.debug(f"DEBUG: Unknown observed variable: {variable}")
            return None
        
        full_path = os.path.join(DATA_DIR, obs_fp)
        if not os.path.exists(full_path):
            logging.debug(f"DEBUG: Observed file not found: {full_path}")
            return None
        
        with xr.open_dataset(full_path, engine="netcdf4") as temp_ds:
            ds = temp_ds.load()
        return ds
    except Exception as e:
        logging.debug(f"DEBUG: Error loading observed dataset for {variable}: {e}")
        return None

def _find_common_observational_gridboxes():
    """Find common gridboxes across all observational datasets"""
    try:
        # Load all observational datasets
        obs_datasets = {}
        for var in ['obs_tmax', 'obs_tmin', 'obs_pr']:
            try:
                ds = load_observed_dataset(var)
                if ds is not None:
                    obs_datasets[var] = ds
            except Exception as e:
                logging.debug(f"DEBUG: Error loading observed dataset for {var}: {e}")
        
        if not obs_datasets:
            logging.debug("DEBUG: Could not load any observational datasets")
            return None
        
        # Extract spatial coordinates from the first dataset as reference
        first_var = list(obs_datasets.keys())[0]
        first_ds = obs_datasets[first_var]
        lat2d, lon2d, _, _ = _extract_spatial_coords(first_ds)
        
        if lat2d is None or lon2d is None:
            logging.debug("DEBUG: Could not extract spatial coordinates from observational dataset")
            return None
        
        # Find common valid gridboxes across all datasets
        common_gridboxes = []
        for i in range(lat2d.shape[0]):
            for j in range(lat2d.shape[1]):
                lat_val = lat2d[i, j]
                lon_val = lon2d[i, j]
                
                # Check if this gridbox is valid in all datasets
                is_valid = True
                for var, ds in obs_datasets.items():
                    try:
                        ds_lat2d, ds_lon2d, _, _ = _extract_spatial_coords(ds)
                        if ds_lat2d is None or ds_lon2d is None:
                            is_valid = False
                            break
                        
                        # Check if this lat/lon exists in this dataset
                        found = False
                        for di in range(ds_lat2d.shape[0]):
                            for dj in range(ds_lon2d.shape[1]):
                                if (abs(ds_lat2d[di, dj] - lat_val) < 0.001 and 
                                    abs(ds_lon2d[di, dj] - lon_val) < 0.001):
                                    found = True
                                    break
                            if found:
                                break
                        
                        if not found:
                            is_valid = False
                            break
                    except Exception as e:
                        logging.debug(f"DEBUG: Error checking gridbox for {var}: {e}")
                        is_valid = False
                        break
                
                # Add to common gridboxes if valid in all datasets
                if is_valid:
                    common_gridboxes.append((lat_val, lon_val))
                
        logging.debug(f"DEBUG: Found {len(common_gridboxes)} common observational gridboxes")
        return common_gridboxes
        
    except Exception as e:
        logging.debug(f"DEBUG: Error finding common observational gridboxes: {e}")
        return None

def _find_most_encompassing_obs_gridbox(projection_lat, projection_lon, obs_ds):
    """
    Find the observational gridbox that most encompasses the given projection gridbox.
    Only considers common gridboxes that exist in all 3 observational NetCDF files.
    Returns the lat_idx, lon_idx of the most encompassing observational gridbox.
    """
    try:
        # Create cache key based on coordinates and dataset info
        cache_key = (round(projection_lat, 4), round(projection_lon, 4), id(obs_ds))
        
        # Check cache first
        if cache_key in _obs_gridbox_cache:
            cached_result = _obs_gridbox_cache[cache_key]
            logging.debug(f"DEBUG: Using cached mapping for ({projection_lat:.4f}, {projection_lon:.4f}) -> ({cached_result[0]}, {cached_result[1]})")
            return cached_result
        
        # Get common observational gridboxes
        common_obs_gridboxes = _find_common_observational_gridboxes()
        if not common_obs_gridboxes:
            logging.debug(f"DEBUG: No common observational gridboxes found")
            return None, None
        
        # Extract spatial coordinates from observational dataset
        lat2d, lon2d, _, _ = _extract_spatial_coords(obs_ds)
        if lat2d is None or lon2d is None:
            logging.debug(f"DEBUG: Could not extract spatial coordinates from observational dataset")
            return None, None
        
        # Find the most encompassing observational gridbox from common ones
        best_lat_idx, best_lon_idx = None, None
        max_encompassment = -1
        
        logging.debug(f"DEBUG: Checking {len(common_obs_gridboxes)} common observational gridboxes for projection point ({projection_lat:.4f}, {projection_lon:.4f})")
        logging.debug(f"DEBUG: Sample obs gridbox indices: {list(common_obs_gridboxes[:5])}")
        
        valid_obs_count = 0
        bounds_failures = 0
        nan_failures = 0
        
        for obs_coords in common_obs_gridboxes:
            # common_obs_gridboxes contains coordinate pairs (lat, lon), not indices
            if len(obs_coords) != 2:
                bounds_failures += 1
                continue
            
            obs_lat, obs_lon = obs_coords
            
            # Skip invalid coordinates
            if np.isnan(obs_lat) or np.isnan(obs_lon):
                nan_failures += 1
                continue
                
            # Find the array indices that correspond to these coordinates
            # We need to find the closest lat/lon indices in the coordinate arrays
            lat_distances = np.abs(lat2d - obs_lat)
            lon_distances = np.abs(lon2d - obs_lon)
            
            # Find the minimum distance indices
            min_lat_idx, min_lon_idx = np.unravel_index(
                np.argmin(lat_distances + lon_distances), 
                lat2d.shape
            )
            
            # Verify the indices are within bounds
            if not (0 <= min_lat_idx < lat2d.shape[0] and 0 <= min_lon_idx < lat2d.shape[1]):
                bounds_failures += 1
                continue
                
            valid_obs_count += 1
            
            # Calculate how much this observational gridbox encompasses the projection point
            # Use a more robust distance-based approach that accounts for the Earth's curvature
            lat_diff = abs(obs_lat - projection_lat)
            lon_diff = abs(obs_lon - projection_lon)
            
            # Calculate distance in degrees (approximate)
            distance = np.sqrt(lat_diff**2 + lon_diff**2)
            
            # Calculate encompassment score (lower distance = higher encompassment)
            # Use a more generous scoring that allows for reasonable distances
            # Score of 1.0 for exact match, decreasing with distance
            encompassment_score = 1.0 / (1.0 + distance)
            
            # Additional bonus for very close matches (within 0.1 degrees)
            if distance < 0.1:
                encompassment_score *= 1.5
            
            if encompassment_score > max_encompassment:
                max_encompassment = encompassment_score
                best_lat_idx, best_lon_idx = min_lat_idx, min_lon_idx
        
        logging.debug(f"DEBUG: Processed {valid_obs_count} valid observational gridboxes, best score: {max_encompassment:.4f}")
        logging.debug(f"DEBUG: Bounds failures: {bounds_failures}, NaN failures: {nan_failures}")
        logging.debug(f"DEBUG: lat2d shape: {lat2d.shape}, lon2d shape: {lon2d.shape}")
        logging.debug(f"DEBUG: Projection point: ({projection_lat:.4f}, {projection_lon:.4f})")
        if best_lat_idx is not None and best_lon_idx is not None:
            logging.debug(f"DEBUG: Best match found at obs indices: ({best_lat_idx}, {best_lon_idx})")
        
        # Apply minimum encompassment threshold (corresponds to ~0.5 degree distance)
        # Reduced from 0.1 to 0.05 to allow more gridboxes to have observational data
        min_encompassment_threshold = 0.05
        
        if best_lat_idx is None or best_lon_idx is None or max_encompassment < min_encompassment_threshold:
            logging.debug(f"DEBUG: No valid observational gridbox found for projection point ({projection_lat:.4f}, {projection_lon:.4f}) - best score: {max_encompassment:.4f}, threshold: {min_encompassment_threshold}")
            return None, None
        
        # Get the actual coordinates of the selected observational gridbox
        obs_lat = lat2d[best_lat_idx, best_lon_idx]
        obs_lon = lon2d[best_lat_idx, best_lon_idx]
        
        logging.debug(f"DEBUG: Mapped projection ({projection_lat:.4f}, {projection_lon:.4f}) to most encompassing obs gridbox ({best_lat_idx}, {best_lon_idx}) at ({obs_lat:.4f}, {obs_lon:.4f}) - encompassment score: {max_encompassment:.4f}")
        
        # Validate the mapping by checking if the selected gridbox actually has data
        try:
            # Try to extract a small sample to verify data exists
            test_df = extract_timeseries(obs_ds, best_lat_idx, best_lon_idx, 'obs_tmax')
            if test_df is not None and not test_df.empty and not test_df['obs_tmax'].isna().all():
                logging.debug(f"DEBUG: âœ… Validation passed - selected obs gridbox has valid data")
            else:
                logging.debug(f"DEBUG: âš ï¸  Validation warning - selected obs gridbox may not have valid data")
        except Exception as e:
            logging.debug(f"DEBUG: âš ï¸  Validation error - could not verify data at selected obs gridbox: {e}")
        
        # Cache the result
        result = (best_lat_idx, best_lon_idx)
        _obs_gridbox_cache[cache_key] = result
        
        return result
        
    except Exception as e:
        logging.debug(f"DEBUG: Error finding most encompassing obs gridbox: {e}")
        return None, None
def _get_coordinates_from_grid_id(grid_id, filepaths_map):
    """
    Convert grid ID (lat_idx,lon_idx) to actual geographic coordinates.
    Returns (lat, lon) or (None, None) if conversion fails.
    """
    try:
        # Parse grid indices
        if ',' in grid_id:
            lat_str, lon_str = grid_id.split(',')
            lat_idx = int(lat_str)
            lon_idx = int(lon_str)
        else:
            return None, None
        
        # Use the first available file to get coordinate mapping
        if not filepaths_map:
            return None, None
            
        sample_file = next(iter(filepaths_map.values()))
        with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
            if 'lat' not in ds.coords or 'lon' not in ds.coords:
                return None, None
            
            # Check if indices are within bounds
            if not (0 <= lat_idx < ds['lat'].size and 0 <= lon_idx < ds['lon'].size):
                return None, None
            
            # Get actual geographic coordinates
            lat = float(ds['lat'].values[lat_idx])
            lon = float(ds['lon'].values[lon_idx])
            return lat, lon
    except Exception as e:
        logging.debug(f"DEBUG: Error converting grid ID to coordinates: {e}")
        return None, None
def _check_observational_data_availability(grid_id, variable, lat=None, lon=None):
    """
    Check if valid observational data exists for a given gridbox and variable.
    Returns True if valid data exists, False otherwise.
    
    Args:
        grid_id: Grid ID in format "lat_idx,lon_idx"
        variable: Variable name (obs_tmin, obs_tmax, obs_pr)
        lat: Optional latitude coordinate (if not provided, will be calculated from grid_id)
        lon: Optional longitude coordinate (if not provided, will be calculated from grid_id)
    """
    try:
        if variable not in ["obs_tmin", "obs_tmax", "obs_pr"]:
            return False
        
        # If coordinates are not provided, we can't proceed
        if lat is None or lon is None:
            logging.debug(f"DEBUG: No coordinates provided for observational data availability check")
            return False
        
        # Load the observational dataset
        ds = load_observed_dataset(variable)
        if ds is None:
            logging.debug(f"DEBUG: Could not load observed dataset for {variable}")
            return False
        
        # Try to find a valid observational gridbox for this projection point
        obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(lat, lon, ds)
        
        if obs_lat_idx is None or obs_lon_idx is None:
            logging.debug(f"DEBUG: No valid observational gridbox found for {variable} at grid {grid_id}")
            return False
        
        # Check if the data actually exists at this location
        try:
            # Try to extract a small sample of data to verify it exists
            df_sample = extract_timeseries(ds, obs_lat_idx, obs_lon_idx, variable)
            if df_sample is None or df_sample.empty:
                logging.debug(f"DEBUG: No data available for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
                return False
            
            # Check if the data has valid values (not all NaN)
            if df_sample[variable].isna().all():
                logging.debug(f"DEBUG: All data is NaN for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
                return False
            
            logging.debug(f"DEBUG: Valid observational data confirmed for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
            return True
            
        except Exception as e:
            logging.debug(f"DEBUG: Error checking data availability for {variable} at grid {grid_id}: {e}")
            return False
            
    except Exception as e:
        logging.debug(f"DEBUG: Error in _check_observational_data_availability: {e}")
        return False

def _precompute_map_data():
    """
    Pre-compute map data for instant loading using static file paths.
    Returns cached map data including grid cells and GeoJSON.
    """
    global _map_data_cache
    
    if _map_data_cache is not None:
        return _map_data_cache
    
    try:
        logging.debug("DEBUG: Pre-computing map data for instant loading...")
        
        # Use static file paths instead of reactive values for instant loading
        filepaths_list = []
        for filename in NETCDF_FILE_LIST:
            if os.path.exists(filename):
                filepaths_list.append(filename)
        
        if not filepaths_list:
            logging.debug("DEBUG: No files available for map pre-computation")
            return None
        
        # Check cache for grid cells
        current_hash = hashlib.md5(str(sorted(filepaths_list)).encode()).hexdigest()
        common_cells, cached_hash = load_grid_cells_cache()
        
        if common_cells is None or current_hash != cached_hash:
            logging.debug("DEBUG: Computing common grid cells for instant map loading...")
            # Use first 3 files for maximum efficiency - all files have identical grid structure
            sample_files = filepaths_list[:3]
            common_cells = find_common_valid_gridcells(sample_files)
            
            if common_cells:
                save_grid_cells_cache(common_cells, current_hash)
                logging.debug(f"DEBUG: Pre-computed {len(common_cells)} common grid cells")
            else:
                logging.debug("DEBUG: No common grid cells found during pre-computation")
                return None
        else:
            logging.debug(f"DEBUG: Using cached grid cells for instant map loading: {len(common_cells)} cells")
        
        # Create GeoJSON using first available file
        sample_file = filepaths_list[0]
        try:
            with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                
                _map_data_cache = {
                    'common_cells': common_cells,
                    'geojson_data': geojson_data,
                    'feature_ids': feature_ids,
                    'grid_feature_ids': [f"{lat},{lon}" for lat, lon in common_cells]
                }
                
                logging.debug("DEBUG: Instant map data pre-computation completed")
                return _map_data_cache
                
        except Exception as e:
            logging.debug(f"DEBUG: Error creating GeoJSON during pre-computation: {e}")
            return None
            
    except Exception as e:
        logging.debug(f"DEBUG: Error in instant map data pre-computation: {e}")
        return None

def _precompute_obs_gridbox_mappings(common_grid_indices, obs_ds):
    """
    Pre-compute mappings from all common projection gridboxes to their most encompassing observational gridboxes.
    This improves performance by avoiding repeated calculations.
    """
    try:
        logging.debug(f"DEBUG: Pre-computing observational gridbox mappings for {len(common_grid_indices)} common gridboxes...")
        
        # Extract spatial coordinates from projection dataset (for common gridboxes)
        # We need to get the coordinates of the common gridboxes
        # This is a simplified version - in practice, you'd need the projection dataset coordinates
        
        # For now, we'll let the mappings be computed on-demand with caching
        # This function serves as a placeholder for future optimization
        logging.debug(f"DEBUG: Observational gridbox mapping pre-computation completed (using on-demand caching)")
        
    except Exception as e:
        logging.debug(f"DEBUG: Error in pre-computing obs gridbox mappings: {e}")

def _get_variables_available_in_loop(loop_gridboxes, filepaths_map, cached_datasets):
    """
    Check which variables have valid data across ALL gridboxes of a loop.
    
    For observational variables, uses a fast spatial bounds check to determine
    if the loop region overlaps with observational data coverage. This is much
    faster than individual gridbox mapping and suitable for initial filtering.
    The actual projection-to-obs mapping happens during data extraction.
    
    Highly optimized to handle large polygons (e.g., 762 gridboxes) in < 1 second:
    - Batch processes all gridboxes at once
    - Uses spatial bounds checking instead of individual mapping
    - Loads each observational dataset only once
    - Minimal disk I/O
    
    Args:
        loop_gridboxes: List of (grid_id, weight) tuples
        filepaths_map: Dictionary mapping (var, model, ssp) to file paths
        cached_datasets: Cache of loaded datasets
    
    Returns:
        Set of variable names that have data in ALL gridboxes of the loop
    """
    try:
        import time
        start_time = time.time()
        logging.debug(f"DEBUG: Checking variable availability across ALL {len(loop_gridboxes)} gridboxes in loop...")
        
        # All standard variables to check
        all_vars = ['tasmax', 'tasmin', 'pr', 'obs_tmax', 'obs_tmin', 'obs_pr']
        
        # Pre-load all observational datasets ONCE (for efficiency)
        obs_datasets = {}
        for var in ['obs_tmax', 'obs_tmin', 'obs_pr']:
            try:
                obs_ds = load_observed_dataset(var)
                if obs_ds is not None:
                    obs_datasets[var] = obs_ds
            except:
                pass
        
        # Extract all gridbox indices for vectorized checking
        gridbox_indices = [(int(grid_id.split(',')[0]), int(grid_id.split(',')[1])) 
                          for grid_id, _ in loop_gridboxes]
        
        # OPTIMIZED: Batch process all gridboxes at once
        # Pre-extract all projection coordinates (vectorized operation)
        projection_coords = []
        for grid_id, _ in loop_gridboxes:
            coords = _get_coordinates_from_grid_id(grid_id, filepaths_map)
            projection_coords.append(coords)
        
        # For observational data availability, use a spatial bounds check instead of full mapping
        # This is much faster and sufficient for determining if obs data "exists" for the loop
        obs_vars_available = set()
        for var in ['obs_tmax', 'obs_tmin', 'obs_pr']:
            if var in obs_datasets:
                obs_ds = obs_datasets[var]
                try:
                    # Get observational data spatial extent
                    lat2d, lon2d, _, _ = _extract_spatial_coords(obs_ds)
                    if lat2d is not None and lon2d is not None:
                        obs_lat_min, obs_lat_max = float(lat2d.min()), float(lat2d.max())
                        obs_lon_min, obs_lon_max = float(lon2d.min()), float(lon2d.max())
                        
                        # Quick check: if ANY projection gridbox falls within obs bounds, assume available
                        # This is fast and conservative (better to show option and fail later than hide it)
                        any_in_bounds = False
                        for proj_lat, proj_lon in projection_coords:
                            if proj_lat is not None and proj_lon is not None:
                                # Add small buffer for edge cases
                                if (obs_lat_min - 0.5 <= proj_lat <= obs_lat_max + 0.5 and
                                    obs_lon_min - 0.5 <= proj_lon <= obs_lon_max + 0.5):
                                    any_in_bounds = True
                                    break
                        
                        if any_in_bounds:
                            obs_vars_available.add(var)
                except:
                    pass
        
        # Build per-gridbox availability sets
        # Model variables are always available for all gridboxes (if files exist)
        available_per_gridbox = []
        for grid_id, _ in loop_gridboxes:
            gridbox_vars = set()
            
            # Check model variables (fast - just checks if files exist)
            for var in ['tasmax', 'tasmin', 'pr']:
                if any(key[0] == var for key in filepaths_map.keys()):
                    gridbox_vars.add(var)
            
            # Add observational variables if spatially available
            gridbox_vars.update(obs_vars_available)
            
            available_per_gridbox.append(gridbox_vars)
        
        # Find intersection: variables available in ALL gridboxes
        if available_per_gridbox:
            common_vars = set.intersection(*available_per_gridbox)
            elapsed_time = time.time() - start_time
            logging.debug(f"DEBUG: âœ… Variable availability check completed in {elapsed_time:.3f}s")
            logging.debug(f"DEBUG: âœ… Variables available across ALL {len(loop_gridboxes)} gridboxes: {sorted(common_vars)}")
            
            # Log per-gridbox summary (compact format)
            unique_var_sets = {}
            for i, (grid_id, _) in enumerate(loop_gridboxes):
                var_set = frozenset(available_per_gridbox[i])
                if var_set not in unique_var_sets:
                    unique_var_sets[var_set] = []
                unique_var_sets[var_set].append(grid_id)
            
            for var_set, grid_ids in unique_var_sets.items():
                logging.debug(f"DEBUG:   {len(grid_ids)} gridbox(es) have: {sorted(var_set)}")
            
            # If no common variables found, warn but return all (let later validation handle it)
            if not common_vars:
                logging.debug(f"DEBUG: âš ï¸ WARNING: No variables available across all gridboxes! Returning all variables as fallback.")
                return set(all_vars)
            
            return common_vars
        else:
            logging.debug(f"DEBUG: No gridboxes checked, returning all variables as fallback")
            return set(all_vars)
        
    except Exception as e:
        logging.debug(f"DEBUG: âŒ Error in _get_variables_available_in_loop: {e}")
        import traceback
        traceback.print_exc()
        # Return all variables on error - let downstream validation handle it
        return {'tasmax', 'tasmin', 'pr', 'obs_tmax', 'obs_tmin', 'obs_pr'}

def _extract_spatial_coords(ds):
    """Return (lat2d, lon2d, lat_dim, lon_dim) from a dataset that may use
    lat/lon (1D or 2D) or y/x. Longitudes are adjusted to -180..180 if needed.
    """
    import numpy as np

    # Try common lat/lon names (case-insensitive variants too)
    lat_name_candidates = ["lat", "latitude", "Lat", "Latitude"]
    lon_name_candidates = ["lon", "longitude", "Lon", "Longitude"]

    lat_var = None
    lon_var = None
    for ln in lat_name_candidates:
        if ln in ds:
            lat_var = ds[ln]
            break
        if ln in ds.coords:
            lat_var = ds.coords[ln]
            break
    for ln in lon_name_candidates:
        if ln in ds:
            lon_var = ds[ln]
            break
        if ln in ds.coords:
            lon_var = ds.coords[ln]
            break

    # If lat/lon not found, try y/x (assume geographic degrees if ranges match)
    if lat_var is None or lon_var is None:
        if "y" in ds.coords and "x" in ds.coords:
            y = ds.coords["y"].values
            x = ds.coords["x"].values
            # Heuristic: treat y/x as degrees if plausible ranges
            if y.ndim == 1 and x.ndim == 1:
                lat2d, lon2d = np.meshgrid(y, x, indexing="ij")
                # Adjust lon if 0..360
                if np.nanmax(lon2d) > 180:
                    lon2d = np.where(lon2d > 180, lon2d - 360, lon2d)
                return lat2d, lon2d, "y", "x"

    if lat_var is None or lon_var is None:
        # As a last resort, search variables by CF attributes
        try:
            for name, var in ds.variables.items():
                std = str(getattr(var, 'standard_name', '')).lower()
                units = str(getattr(var, 'units', '')).lower()
                if lat_var is None and (std == 'latitude' or 'degree' in units and 'north' in units):
                    lat_var = var
                if lon_var is None and (std == 'longitude' or 'degree' in units and 'east' in units):
                    lon_var = var
                if lat_var is not None and lon_var is not None:
                    break
        except Exception:
            pass
        if lat_var is None or lon_var is None:
            return None, None, None, None

    lat_vals = lat_var.values
    lon_vals = lon_var.values

    # Adjust 0..360 longitudes to -180..180
    if np.nanmax(lon_vals) > 180:
        lon_vals = np.where(lon_vals > 180, lon_vals - 360, lon_vals)

    # 1D lat/lon â†’ broadcast to 2D
    if lat_vals.ndim == 1 and lon_vals.ndim == 1:
        lat_dim = lat_var.dims[0]
        lon_dim = lon_var.dims[0]
        lat2d, lon2d = np.meshgrid(lat_vals, lon_vals, indexing="ij")
        return lat2d, lon2d, lat_dim, lon_dim

    # 2D lat/lon
    if lat_vals.ndim == 2 and lon_vals.ndim == 2:
        lat_dim, lon_dim = lat_var.dims
        return lat_vals, lon_vals, lat_dim, lon_dim

    return None, None, None, None

def _open_dataset_fallback(path):
    """Open a NetCDF with multiple engine fallbacks. Returns xr.Dataset or None."""
    for eng in ("h5netcdf", "netcdf4", "scipy", None):
        try:
            return xr.open_dataset(path, engine=eng, decode_times=False)
        except Exception:
            continue
    return None


def _discover_obs_path(candidate_names):
    """Return first existing path matching any of candidate file basenames, searching workspace recursively."""
    try:
        for name in candidate_names:
            direct = os.path.join(os.getcwd(), name)
            if os.path.exists(direct):
                return direct
        import glob
        for name in candidate_names:
            matches = glob.glob(os.path.join(os.getcwd(), "**", name), recursive=True)
            if matches:
                return matches[0]
    except Exception:
        pass
    return None


def _extract_all_points_from_file(file_path):
    """Open a NetCDF file and return a list of (lat, lon) for all coordinate points.
    Uses multiple engine fallbacks and supports lat/lon or y/x (1D or 2D)."""
    try:
        ds = _open_dataset_fallback(file_path)
        if ds is None:
            return []
        lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
        if lat2d is None or lon2d is None:
            ds.close()
            return []
        # Build full list of points
        points = []
        try:
            total_i = lat2d.shape[0]
            total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
            for i in range(total_i):
                for j in range(total_j):
                    points.append((float(lat2d[i, j]), float(lon2d[i, j])))
        except Exception:
            pass
        ds.close()
        return points
    except Exception:
        return []


def _extract_nonzero_points_from_file(file_path):
    """Return (lat, lon) points where the data is non-zero for at least one time step.
    Supports time dim named 'time' or 'band'. Falls back to all points if no time dim.
    """
    try:
        ds = _open_dataset_fallback(file_path)
        if ds is None:
            return []
        # Choose primary data variable (exclude spatial_ref)
        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
        if not data_vars:
            ds.close(); return []
        var = data_vars[0]
        lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
        if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
            ds.close(); return []
        dims = ds[var].dims
        time_dim = 'time' if 'time' in dims else ('band' if 'band' in dims else None)
        points = []
        if time_dim and lat_dim in dims and lon_dim in dims:
            data = ds[var]
            data = data.where(np.isfinite(data)).fillna(0)
            mask = (data != 0).any(dim=time_dim)
            n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
            for i in range(n_i):
                for j in range(n_j):
                    if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                        points.append((float(lat2d[i, j]), float(lon2d[i, j])))
        else:
            # No time dim; include all coordinates
            try:
                total_i = lat2d.shape[0]
                total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
                for i in range(total_i):
                    for j in range(total_j):
                        points.append((float(lat2d[i, j]), float(lon2d[i, j])))
            except Exception:
                pass
        ds.close()
        return points
    except Exception:
        return []

def create_grid_geojson(common_valid_grid_indices, ds_for_coords):
    """
    Creates a GeoJSON FeatureCollection of grid cells based on a pre-computed
    set of common valid (lat_idx, lon_idx) pairs.
    """
    if not common_valid_grid_indices or ds_for_coords is None:
        logging.error("Error: No common valid grid indices or coordinate dataset for GeoJSON creation.")
        return None, []

    logging.info(f"Generating GeoJSON features for {len(common_valid_grid_indices)} common valid grid cells...")

    lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds_for_coords)
    if lat2d is None or lon2d is None:
        logging.error("Error: Unable to extract spatial coordinates for GeoJSON creation.")
        return None, []

    # Estimate deltas from medians of neighbor diffs (fallback to small values)
    try:
        # Differences along each axis, ignore NaNs
        lat_diff_rows = np.nanmedian(np.abs(np.diff(lat2d, axis=0))) if lat2d.shape[0] > 1 else np.nan
        lon_diff_cols = np.nanmedian(np.abs(np.diff(lon2d, axis=1))) if lon2d.shape[1] > 1 else np.nan
        # Use the smaller of the two deltas to ensure square cells
        if np.isfinite(lat_diff_rows) and lat_diff_rows > 0 and np.isfinite(lon_diff_cols) and lon_diff_cols > 0:
            # Use the smaller delta to ensure square appearance
            delta_lat = min(lat_diff_rows, lon_diff_cols) / 2.0
            delta_lon = delta_lat  # Make them equal for square cells
        else:
            delta_lat = delta_lon = 0.05  # Fallback to equal values
    except Exception:
        delta_lat, delta_lon = 0.05, 0.05

    features = []
    feature_ids = []

    for lat_idx, lon_idx in common_valid_grid_indices:
        # Ensure indices are within bounds of the coordinate dataset
        if not (0 <= lat_idx < lat2d.shape[0] and 0 <= lon_idx < lon2d.shape[1]):
            logging.warning(f"Warning: Skipping out-of-bounds index ({lat_idx}, {lon_idx}) during GeoJSON creation.")
            continue

        lat = float(lat2d[lat_idx, lon_idx])
        lon = float(lon2d[lat_idx, lon_idx])

        display_lon_for_geojson = lon

        # Add small gap between cells to prevent visual overlap
        gap_factor = 0.95  # 5% gap between cells
        polygon = geojson.Polygon([[(display_lon_for_geojson - delta_lon * gap_factor, lat - delta_lat * gap_factor),
                                    (display_lon_for_geojson + delta_lon * gap_factor, lat - delta_lat * gap_factor),
                                    (display_lon_for_geojson + delta_lon * gap_factor, lat + delta_lat * gap_factor),
                                    (display_lon_for_geojson - delta_lon * gap_factor, lat + delta_lat * gap_factor),
                                    (display_lon_for_geojson - delta_lon * gap_factor, lat - delta_lat * gap_factor)]])
    
        feature_id = f"{lat_idx},{lon_idx}"
        properties = {"lat": lat, "lon": lon, "display_lat": f"{lat:.3f}", "display_lon": f"{display_lon_for_geojson:.3f}"}
        feature = geojson.Feature(geometry=polygon, id=feature_id, properties=properties)

        features.append(feature)
        feature_ids.append(feature_id)

    logging.info(f"Finished GeoJSON generation. Total features created: {len(features)}.")
    return geojson.FeatureCollection(features), feature_ids

def _calculate_gridbox_intersections(polygon_coords, grid_geojson_data):
    """
    Calculate which gridboxes intersect with a drawn polygon and their weighted coverage.
    
    Args:
        polygon_coords: List of [lon, lat] coordinate pairs defining the drawn polygon
        grid_geojson_data: GeoJSON FeatureCollection containing all gridbox features
    
    Returns:
        List of (grid_id, weight) tuples where weight is the percentage of gridbox covered (0.0 to 1.0)
        Returns empty list if no intersections found.
    """
    try:
        # Create shapely Polygon from drawn coordinates
        drawn_polygon = Polygon(polygon_coords)
        
        if not drawn_polygon.is_valid:
            logging.warning("WARNING: Drawn polygon is not valid")
            return []
        
        intersections = []
        
        # Iterate through all gridbox features
        for feature in grid_geojson_data.features:
            try:
                # Get grid ID from feature
                grid_id = feature.id
                
                # Convert GeoJSON geometry to shapely shape
                gridbox_shape = shape(feature.geometry)
                
                # Check if there's any intersection
                if drawn_polygon.intersects(gridbox_shape):
                    # Calculate intersection area
                    intersection = drawn_polygon.intersection(gridbox_shape)
                    intersection_area = intersection.area
                    
                    # Calculate gridbox total area
                    gridbox_area = gridbox_shape.area
                    
                    # Calculate coverage percentage (weight)
                    if gridbox_area > 0:
                        weight = intersection_area / gridbox_area
                        # Clamp weight to [0, 1] range
                        weight = max(0.0, min(1.0, weight))
                        
                        if weight > 0:
                            intersections.append((grid_id, weight))
                            logging.debug(f"DEBUG: Grid {grid_id} intersects with weight {weight:.3f}")
            
            except Exception as e:
                logging.warning(f"WARNING: Error processing grid feature {grid_id if 'grid_id' in locals() else 'unknown'}: {e}")
                continue
        
        logging.debug(f"DEBUG: Found {len(intersections)} gridbox intersections")
        return intersections
    
    except Exception as e:
        logging.error(f"ERROR: Error in _calculate_gridbox_intersections: {e}")
        import traceback
        traceback.print_exc()
        return []

# --- Load CSV Data ---
def load_csv_minimum_temperature_data():
    """
    Loads minimum temperature data from a CSV file.
    """
    try:
        possible_filenames = ['chart.csv', 'chart (1).csv', 'chart1.csv']
        found_path = None
        for root, _, files in os.walk("."):
            for f in files:
                if f in possible_filenames:
                    found_path = os.path.join(root, f)
                    break
            if found_path:
                break
        if not found_path:
            raise FileNotFoundError("No CSV file matching expected names found.")

        try:
            df = pd.read_csv(found_path, skiprows=8, na_values=['', 'nan', 'NaN'])
        except Exception:
            df = pd.read_csv(found_path, na_values=['', 'nan', 'NaN'])

        df.rename(columns=lambda x: x.strip(), inplace=True)
        df["year"] = pd.to_numeric(df.iloc[:, 0], errors="coerce")
        df.dropna(subset=["year"], inplace=True)
        df["year"] = df["year"].astype(int)
    
        column_mapping = {
            "Observed": "Observed",
            "Modeled RCP 8.5 Range Min": "Modeled RCP 8.5 Range Min",
            "Modeled RCP 8.5 Range Max": "Modeled RCP 8.5 Range Max",
            "CanESM2 (Average)": "CanESM2 (Average)",
            "CNRM-CM5 (Cool/Wet)": "CNRM-CM5 (Cool/Wet)",
            "HadGEM2-ES (Warm/Dry)": "HadGEM2-ES (Warm/Dry)",
            "MIROC5 (Complement)": "MIROC5 (Complement)"
        }
    
        df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns}, inplace=True)

        return df
    except Exception as e:
        logging.error(f"Error loading CSV: {e}")
        return None

# --- Utility Functions for NetCDF Data ---

def calculate_mad(trace1_data, trace2_data, trace1_freq, trace2_freq, trace1_name, trace2_name):
    """
    Calculate Mean Absolute Difference (MAD) between two traces.
    
    Args:
        trace1_data: DataFrame with 'Date' and variable columns
        trace2_data: DataFrame with 'Date' and variable columns  
        trace1_freq: Frequency of trace1 ('daily', 'monthly', 'annual')
        trace2_freq: Frequency of trace2 ('daily', 'monthly', 'annual')
        trace1_name: Display name for trace1
        trace2_name: Display name for trace2
    
    Returns:
        dict with 'mad', 'categorization', 'common_years', 'n_points', 'error' keys
    """
    try:
        # Check if frequencies match
        if trace1_freq != trace2_freq:
            return {
                'mad': None,
                'categorization': 'Cannot compare - different frequencies',
                'common_years': None,
                'n_points': 0,
                'error': f'Frequencies do not match: {trace1_freq} vs {trace2_freq}'
            }
        
        # Get variable names from dataframes
        trace1_var = [col for col in trace1_data.columns if col != 'Date'][0]
        trace2_var = [col for col in trace2_data.columns if col != 'Date'][0]
        
        # Ensure we have Year column for all data
        if 'Year' not in trace1_data.columns:
            trace1_data['Year'] = trace1_data['Date'].dt.year
        if 'Year' not in trace2_data.columns:
            trace2_data['Year'] = trace2_data['Date'].dt.year
        
        # For daily and monthly data, match by exact date instead of just year
        if trace1_freq == 'daily' or trace1_freq == 'monthly':
            # Match by exact date for daily/monthly comparisons
            trace1_dates = set(pd.to_datetime(trace1_data['Date']).dt.date)
            trace2_dates = set(pd.to_datetime(trace2_data['Date']).dt.date)
            common_dates = sorted(trace1_dates.intersection(trace2_dates))
            
            logging.debug(f"DEBUG: MAD calculation - Trace1 has {len(trace1_dates)} {trace1_freq} dates")
            logging.debug(f"DEBUG: MAD calculation - Trace2 has {len(trace2_dates)} {trace2_freq} dates")
            logging.debug(f"DEBUG: MAD calculation - Common dates: {len(common_dates)}")
            
            if len(common_dates) == 0:
                return {
                    'mad': None,
                    'categorization': 'Cannot compare - no common time period',
                    'common_years': None,
                    'n_points': 0,
                    'error': 'No overlapping dates found'
                }
            
            # Filter to common dates and merge on date
            trace1_data['Date_normalized'] = pd.to_datetime(trace1_data['Date']).dt.date
            trace2_data['Date_normalized'] = pd.to_datetime(trace2_data['Date']).dt.date
            
            trace1_filtered = trace1_data[trace1_data['Date_normalized'].isin(common_dates)].sort_values('Date')
            trace2_filtered = trace2_data[trace2_data['Date_normalized'].isin(common_dates)].sort_values('Date')
            
            # Get year range for display
            common_years = f"{min(common_dates).year}-{max(common_dates).year}"
            
        else:
            # For annual data, match by year
            trace1_years = set(trace1_data['Year'].dropna())
            trace2_years = set(trace2_data['Year'].dropna())
            common_years_set = sorted(trace1_years.intersection(trace2_years))
            
            # Debug output for time ranges
            logging.debug(f"DEBUG: MAD calculation - Trace1 years: {sorted(trace1_years)}")
            logging.debug(f"DEBUG: MAD calculation - Trace2 years: {sorted(trace2_years)}")
            logging.debug(f"DEBUG: MAD calculation - Common years: {common_years_set}")
            
            if len(common_years_set) == 0:
                return {
                    'mad': None,
                    'categorization': 'Cannot compare - no common time period',
                    'common_years': None,
                    'n_points': 0,
                    'error': 'No overlapping years found'
                }
            
            # Filter to common years and get values
            trace1_filtered = trace1_data[trace1_data['Year'].isin(common_years_set)].sort_values('Year')
            trace2_filtered = trace2_data[trace2_data['Year'].isin(common_years_set)].sort_values('Year')
            common_years = f"{min(common_years_set)}-{max(common_years_set)}"
        # Ensure we have the same number of points
        if len(trace1_filtered) != len(trace2_filtered):
            return {
                'mad': None,
                'categorization': 'Cannot compare - different number of data points',
                'common_years': common_years,
                'n_points': 0,
                'error': f'Different number of points: {len(trace1_filtered)} vs {len(trace2_filtered)}'
            }
        
        # Get values for comparison
        values1 = trace1_filtered[trace1_var].values
        values2 = trace2_filtered[trace2_var].values
        
        # Remove any NaN values
        valid_mask = ~(np.isnan(values1) | np.isnan(values2))
        if not np.any(valid_mask):
            return {
                'mad': None,
                'categorization': 'Cannot compare - no valid data',
                'common_years': common_years,
                'n_points': 0,
                'error': 'No valid (non-NaN) data points found'
            }
        
        values1_valid = values1[valid_mask]
        values2_valid = values2[valid_mask]
        
        # Check if we have any data points for MAD calculation
        if len(values1_valid) < 2:
            return {
                'mad': None,
                'categorization': 'Insufficient data - need at least 2 data points',
                'common_years': f"{min(common_years)}-{max(common_years)}",
                'n_points': len(values1_valid),
                'error': f'Only {len(values1_valid)} data points available (minimum 2 required)'
            }
        
        # Calculate MAD
        mad = np.mean(np.abs(values1_valid - values2_valid))
        
        # Debug output for limited data cases
        if len(values1_valid) <= 5:
            logging.debug(f"DEBUG: MAD Debug - {len(values1_valid)} data points")
            logging.debug(f"DEBUG: Values1: {values1_valid}")
            logging.debug(f"DEBUG: Values2: {values2_valid}")
            logging.debug(f"DEBUG: Differences: {np.abs(values1_valid - values2_valid)}")
            logging.debug(f"DEBUG: Calculated MAD: {mad}")
            logging.debug(f"DEBUG: Common years: {common_years}")
        
        # Add warning for limited data
        warning = ""
        if len(values1_valid) < 5:
            warning = f"âš ï¸ Limited data ({len(values1_valid)} points) - results may be less reliable"
        
        # Calculate p-value using paired t-test
        # Tests if the two traces are significantly different
        p_value = None
        try:
            from scipy.stats import ttest_rel
            if len(values1_valid) >= 3:  # Need at least 3 points for t-test
                t_stat, p_value = ttest_rel(values1_valid, values2_valid)
                logging.debug(f"DEBUG: MAD p-value calculation - t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}")
            else:
                logging.debug(f"DEBUG: MAD p-value - insufficient data for t-test ({len(values1_valid)} points)")
        except Exception as e:
            logging.debug(f"DEBUG: MAD p-value calculation failed: {e}")
            p_value = None
        
        # Categorize based on MAD value and variable type
        # Determine variable type for appropriate thresholds
        if trace1_var in ['tasmin', 'tasmax', 'obs_tmin', 'obs_tmax']:
            # Temperature variables (in Fahrenheit)
            if mad < 2.0:
                categorization = 'Excellent match'
            elif mad < 5.0:
                categorization = 'Good match'
            elif mad < 10.0:
                categorization = 'Fair match'
            else:
                categorization = 'Poor match'
        elif trace1_var in ['pr', 'obs_pr']:
            # Precipitation variables (in inches)
            if mad < 0.5:
                categorization = 'Excellent match'
            elif mad < 1.0:
                categorization = 'Good match'
            elif mad < 2.0:
                categorization = 'Fair match'
            else:
                categorization = 'Poor match'
        else:
            # Other variables - use relative thresholds
            # Calculate relative MAD as percentage of mean
            mean_val = np.mean([np.mean(values1_valid), np.mean(values2_valid)])
            if mean_val != 0:
                relative_mad = (mad / mean_val) * 100
                if relative_mad < 5:
                    categorization = 'Excellent match'
                elif relative_mad < 15:
                    categorization = 'Good match'
                elif relative_mad < 30:
                    categorization = 'Fair match'
                else:
                    categorization = 'Poor match'
            else:
                categorization = 'Cannot categorize'
        
        return {
            'mad': round(mad, 4),
            'categorization': categorization,
            'common_years': common_years,  # Already formatted as string in the logic above
            'n_points': len(values1_valid),
            'p_value': p_value,
            'error': None,
            'warning': warning
        }
        
    except Exception as e:
        return {
            'mad': None,
            'categorization': 'Error in calculation',
            'common_years': None,
            'n_points': 0,
            'error': str(e)
        }

def extract_timeseries(ds, lat_idx, lon_idx, variable_name=None):
    """
    Extracts a time series for a specific grid point from an xarray Dataset.
    If the specified point has no data, tries to find a nearby valid point.
    """
    if ds is None:
        return pd.DataFrame({"Date": [], "No Data": []})
    
    # Adapt datasets that use 'band','x','y' instead of 'time','lat','lon' (e.g., observed Livneh NetCDFs)
    try:
        has_band = ('band' in getattr(ds, 'dims', {})) or ('band' in getattr(ds, 'coords', {}))
        if "time" not in ds.coords and has_band:
            # Synthesize daily time from 1950-01-01 for the number of bands
            num_bands = ds.dims.get('band', ds['band'].size if 'band' in ds.coords else None)
            if num_bands is not None and num_bands > 0:
                start_date = pd.Timestamp('1950-01-01')
                times = pd.date_range(start=start_date, periods=num_bands, freq='D')
                if 'band' in ds.dims:
                    ds = ds.rename({'band': 'time'})
                # Assign synthesized time coordinate
                ds = ds.assign_coords(time=("time", times))
        # Normalize spatial dims
        if 'lat' not in ds.coords and 'y' in getattr(ds, 'dims', {}):
            ds = ds.rename({'y': 'lat'})
        if 'lon' not in ds.coords and 'x' in getattr(ds, 'dims', {}):
            ds = ds.rename({'x': 'lon'})
    except Exception as adapt_err:
        logging.debug(f"DEBUG: extract_timeseries - adaptation error: {adapt_err}")
    
    if "time" not in ds.coords:
        return pd.DataFrame({"Date": [], "No Data": []})

    times = pd.to_datetime(ds["time"].values)

    # Map observed variable names to actual dataset variable names
    if variable_name and variable_name.startswith('obs_'):
        # For observed variables, find the actual data variable (not spatial_ref)
        data_vars = [var for var in ds.data_vars.keys() if var != 'spatial_ref']
        if data_vars:
            var_to_extract = data_vars[0]  # Use the first non-spatial_ref variable
        else:
            var_to_extract = list(ds.data_vars.keys())[0]
    else:
        var_to_extract = variable_name if variable_name and variable_name in ds.data_vars else list(ds.data_vars.keys())[0]

    if var_to_extract not in ds.data_vars:
        return pd.DataFrame({"Date": [], "No Data": []})

    # Try the specified point first
    try:
        values = ds[var_to_extract].isel(time=slice(None), lat=lat_idx, lon=lon_idx).values
        
        # Check if the data is valid (not all NaN)
        if not np.isnan(values).all():
            # Use the original variable_name for the column, not the extracted variable name
            column_name = variable_name if variable_name else var_to_extract
            df = pd.DataFrame({"Date": times, column_name: values})
            return df
    except IndexError as e:
        logging.error(f"Error: Lat/Lon indices {lat_idx},{lon_idx} out of bounds for variable {var_to_extract}.")
    except Exception as e:
        logging.info(f"An unexpected error occurred during time series extraction: {e}")

    # If the specified point has no data, search for a valid point
    try:
        # Get the data array
        data_array = ds[var_to_extract]
        
        # Find valid data points (non-NaN)
        valid_mask = ~data_array.isnull().all(dim='time')
        
        # Get valid indices
        valid_lat_indices = valid_mask.lat.values
        valid_lon_indices = valid_mask.lon.values
        
        # Find the closest valid point to the requested location
        min_distance = float('inf')
        best_lat_idx = None
        best_lon_idx = None
        
        for i in range(len(valid_lat_indices)):
            for j in range(len(valid_lon_indices)):
                if valid_mask.isel(lat=i, lon=j).item():
                    # Calculate distance to requested point
                    distance = abs(i - lat_idx) + abs(j - lon_idx)
                    if distance < min_distance:
                        min_distance = distance
                        best_lat_idx = i
                        best_lon_idx = j
        
        if best_lat_idx is not None and best_lon_idx is not None:
            values = data_array.isel(time=slice(None), lat=best_lat_idx, lon=best_lon_idx).values
            
            if not np.isnan(values).all():
                # Use the original variable_name for the column, not the extracted variable name
                column_name = variable_name if variable_name else var_to_extract
                df = pd.DataFrame({"Date": times, column_name: values})
                return df
            
    except Exception as e:
        logging.debug(f"DEBUG: Error searching for valid data points: {e}")

    # If all else fails, return empty DataFrame
    return pd.DataFrame({"Date": [], "No Data": []})

def _extract_regional_average_timeseries(loop_gridboxes, variable_name, model, frequency, ssp, filepaths_map, cached_datasets, obs_gridbox_mappings=None):
    """
    Extract and compute weighted time series across multiple gridboxes for regional analysis.
    
    For precipitation variables (pr, obs_pr):
    - Monthly/Annual frequencies: Computes weighted TOTAL (sum) across region
      This represents the total precipitation that fell over the entire polygon region.
    - Daily frequency: Computes weighted AVERAGE across region (since it's a rate in inches/day)
    
    For all other variables:
    - All frequencies: Computes weighted AVERAGE across region
    
    Args:
        loop_gridboxes: List of (grid_id, weight) tuples where weight is 0.0-1.0
        variable_name: Variable to extract (e.g., 'pr', 'tasmax', 'obs_pr')
        model: Model name (e.g., 'TaiESM1', 'Observed')
        frequency: Data frequency (e.g., 'annual', 'monthly', 'daily')
        ssp: Scenario (e.g., 'historical', 'ssp245')
        filepaths_map: Dictionary mapping (var, model, ssp) to file paths
        cached_datasets: Cache of loaded datasets
        obs_gridbox_mappings: Optional pre-computed mappings for observational data
    
    Returns:
        pd.DataFrame with 'Date' column and variable column containing weighted values
        Raises exception if gridboxes have inconsistent data availability
    """
    try:
        logging.debug(f"DEBUG: _extract_regional_average_timeseries called for {variable_name} ({model}, {frequency}, {ssp}) with {len(loop_gridboxes)} gridboxes")
        
        # PERFORMANCE: Check cache first to avoid re-extracting same polygon data
        # Create stable cache key using just the grid IDs (ignore weights for caching)
        grid_ids_only = tuple(sorted([grid_id for grid_id, weight in loop_gridboxes]))
        cache_key = (grid_ids_only, variable_name, model, frequency, ssp)
        
        logging.debug(f"DEBUG: Cache check - key: (gridboxes={len(grid_ids_only)}, var={variable_name}, model={model}, freq={frequency}, ssp={ssp})")
        logging.debug(f"DEBUG: Cache status - size: {len(_REGIONAL_TIMESERIES_CACHE)} entries")
        logging.debug(f"DEBUG: Cache key hash: {hash(cache_key) if len(grid_ids_only) < 100 else 'too large to hash safely'}")
        
        # VALIDATION: Clean out any invalid/corrupted cache entries
        # Valid keys must be 5-tuple: (grid_ids_tuple, variable, model, frequency, ssp)
        # AND the first element must be a tuple of grid IDs (strings)
        invalid_keys = []
        for k in _REGIONAL_TIMESERIES_CACHE.keys():
            if not isinstance(k, tuple) or len(k) != 5:
                invalid_keys.append(k)
                logging.debug(f"DEBUG: Invalid key structure: type={type(k)}, len={len(k) if hasattr(k, '__len__') else 'N/A'}, value={k if not isinstance(k, tuple) else f'tuple[{len(k)}]'}")
            elif not isinstance(k[0], tuple):
                invalid_keys.append(k)
                logging.debug(f"DEBUG: Invalid key - first element not a tuple: type={type(k[0])}, value={k[0]}")
        
        if invalid_keys:
            logging.debug(f"DEBUG: âš ï¸  Clearing {len(invalid_keys)} corrupted cache entries")
            for bad_key in invalid_keys:
                del _REGIONAL_TIMESERIES_CACHE[bad_key]
        
        if cache_key in _REGIONAL_TIMESERIES_CACHE:
            logging.debug(f"DEBUG: âš¡ CACHE HIT! Returning cached regional data for {len(loop_gridboxes)} gridboxes")
            return _REGIONAL_TIMESERIES_CACHE[cache_key].copy()  # Return copy to prevent modification
        
        logging.debug(f"DEBUG: âŒ Cache miss - will compute and cache regional data")
        
        # Warn user about large polygons
        if len(loop_gridboxes) > 200:
            logging.info(f"âš ï¸  LARGE REGION DETECTED: Processing {len(loop_gridboxes)} gridboxes - this may take 1-3 minutes...")
        
        # Determine if this is observational data
        is_obs = variable_name.startswith('obs_')
        
        # IMPORTANT: For regional loops, ALWAYS use weighted AVERAGE
        # This computes the mean value across the region, which is what users expect
        # 
        # Previous logic used SUM for precipitation monthly/annual, but this gave
        # unrealistic values for large regions (e.g., 762 gridboxes would sum to huge totals)
        # 
        # Example: A region with 762 gridboxes, each receiving 10 inches/month
        #   - OLD (SUM): 7,620 inches/month (unrealistic!)
        #   - NEW (AVERAGE): 10 inches/month (realistic - average rainfall across region)
        use_sum = False  # Always use weighted average for regional data
        
        logging.debug(f"DEBUG: Using weighted AVERAGE for {variable_name} ({frequency}) - regional average across {len(loop_gridboxes)} gridboxes")
        
        # Get the dataset - OPTIMIZED: Load once for all gridboxes
        if is_obs:
            # For observational data, load the appropriate dataset
            obs_ds = load_observed_dataset(variable_name)
            if obs_ds is None:
                raise Exception(f"Could not load observational dataset for {variable_name}")
            ds = obs_ds
        else:
            # For model data, get from filepaths_map
            file_path = filepaths_map.get((variable_name, model, ssp))
            if not file_path:
                raise Exception(f"No file found for {variable_name}, {model}, {ssp}")
            
            # Try to get from cache first
            if (variable_name, model, ssp) in cached_datasets:
                ds = cached_datasets[(variable_name, model, ssp)]
                logging.debug(f"DEBUG: Using cached dataset for {variable_name}")
            else:
                logging.debug(f"DEBUG: Loading dataset for {variable_name} from {file_path}")
                ds = xr.open_dataset(file_path)
                cached_datasets[(variable_name, model, ssp)] = ds
        
        # HIGHLY OPTIMIZED: Batch map all gridboxes at once to avoid 762 individual mapping calls
        gridbox_indices = []  # List of (lat_idx, lon_idx, weight, grid_id)
        
        if is_obs:
            # OPTIMIZATION: Batch process all obs mappings to avoid slow individual calls
            logging.debug(f"DEBUG: Batch mapping {len(loop_gridboxes)} projection gridboxes to observational grid...")
            import time
            map_start = time.time()
            
            # Initialize obs_gridbox_mappings if not provided
            if obs_gridbox_mappings is None:
                obs_gridbox_mappings = {}
            
            # Step 1: Collect all unique projection coordinates that need mapping
            coords_to_map = {}  # {cache_key: (proj_lat, proj_lon, grid_ids)}
            grid_to_cache_key = {}  # {grid_id: cache_key}
            
            for grid_id, weight in loop_gridboxes:
                from_coords = _get_coordinates_from_grid_id(grid_id, filepaths_map)
                if from_coords:
                    proj_lat, proj_lon = from_coords
                    coord_cache_key = (round(proj_lat, 4), round(proj_lon, 4))  # RENAMED to avoid collision with regional cache_key
                    grid_to_cache_key[grid_id] = coord_cache_key
                    
                    # Only add to mapping queue if not already in cache
                    if not (obs_gridbox_mappings and coord_cache_key in obs_gridbox_mappings):
                        if coord_cache_key not in coords_to_map:
                            coords_to_map[coord_cache_key] = (proj_lat, proj_lon, [])
                        coords_to_map[coord_cache_key][2].append(grid_id)
            
            # Step 2: Batch map all unique uncached coordinates
            if coords_to_map:
                logging.debug(f"DEBUG: Mapping {len(coords_to_map)} unique uncached projection coordinates...")
                
                # Get obs grid info once
                lat2d, lon2d, _, _ = _extract_spatial_coords(ds)
                common_obs_gridboxes = _find_common_observational_gridboxes()
                
                if lat2d is not None and lon2d is not None and common_obs_gridboxes:
                    # FULLY VECTORIZED mapping: process all coordinates at once
                    # Convert to arrays for vectorized operations
                    proj_coords = np.array([(proj_lat, proj_lon) for _, (proj_lat, proj_lon, _) in coords_to_map.items()])
                    obs_coords = np.array([(lat, lon) for lat, lon in common_obs_gridboxes if not (np.isnan(lat) or np.isnan(lon))])
                    
                    if len(obs_coords) > 0:
                        # Compute all pairwise distances at once using broadcasting
                        # Shape: (num_proj_coords, num_obs_coords)
                        proj_lats = proj_coords[:, 0:1]  # Shape (n, 1) for broadcasting
                        proj_lons = proj_coords[:, 1:2]
                        obs_lats = obs_coords[:, 0]  # Shape (m,)
                        obs_lons = obs_coords[:, 1]
                        
                        # Euclidean distance: sqrt((lat1-lat2)^2 + (lon1-lon2)^2)
                        distances = np.sqrt((proj_lats - obs_lats)**2 + (proj_lons - obs_lons)**2)
                        
                        # Find nearest obs point for each projection point
                        nearest_obs_indices = np.argmin(distances, axis=1)
                        
                        # Map each projection point to its nearest obs gridbox
                        for idx, (coord_cache_key, (proj_lat, proj_lon, grid_ids)) in enumerate(coords_to_map.items()):
                            nearest_obs_lat, nearest_obs_lon = obs_coords[nearest_obs_indices[idx]]
                            
                            # Find grid indices for this obs point
                            lat_distances = np.abs(lat2d - nearest_obs_lat)
                            lon_distances = np.abs(lon2d - nearest_obs_lon)
                            min_lat_idx, min_lon_idx = np.unravel_index(
                                np.argmin(lat_distances + lon_distances), lat2d.shape
                            )
                            
                            if 0 <= min_lat_idx < lat2d.shape[0] and 0 <= min_lon_idx < lat2d.shape[1]:
                                obs_gridbox_mappings[coord_cache_key] = (min_lat_idx, min_lon_idx)
                
                logging.debug(f"DEBUG: Batch mapping completed in {time.time() - map_start:.2f}s")
            else:
                logging.debug(f"DEBUG: All {len(loop_gridboxes)} gridboxes already cached!")
            
            # Step 3: Build gridbox_indices using cached mappings
            for grid_id, weight in loop_gridboxes:
                lat_idx, lon_idx = map(int, grid_id.split(','))
                coord_cache_key = grid_to_cache_key.get(grid_id)
                
                if coord_cache_key and obs_gridbox_mappings and coord_cache_key in obs_gridbox_mappings:
                    lat_idx, lon_idx = obs_gridbox_mappings[coord_cache_key]
                
                gridbox_indices.append((lat_idx, lon_idx, weight, grid_id))
        else:
            # Model data - simple index extraction
            for grid_id, weight in loop_gridboxes:
                lat_idx, lon_idx = map(int, grid_id.split(','))
                gridbox_indices.append((lat_idx, lon_idx, weight, grid_id))
        
        logging.debug(f"DEBUG: Mapped {len(gridbox_indices)} gridboxes, extracting data...")
        
        # OPTIMIZED: Extract all gridboxes at once using vectorized operations
        try:
            # Get variable name for extraction
            if is_obs:
                data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                extract_var = data_vars[0] if data_vars else None
                if not extract_var:
                    raise Exception(f"No data variable found in observed dataset for {variable_name}")
            else:
                extract_var = variable_name
            
            # Extract data for all gridboxes at once using proper fancy indexing
            lat_indices = np.array([idx[0] for idx in gridbox_indices])
            lon_indices = np.array([idx[1] for idx in gridbox_indices])
            
            # Get time coordinate - handle both model and obs data structures
            if 'time' in ds.coords:
                time_var = ds['time']
                time_values = pd.to_datetime(time_var.values)
            elif 'Time' in ds.coords:
                time_var = ds['Time']
                time_values = pd.to_datetime(time_var.values)
            elif 'band' in ds.coords:
                # Observational data uses 'band' as time dimension
                # Need to construct date range based on file metadata
                band_count = len(ds['band'])
                # Livneh data is daily 1950-2013
                time_values = pd.date_range('1950-01-01', periods=band_count, freq='D')
            else:
                raise Exception(f"No time coordinate found. Available coords: {list(ds.coords.keys())}")
            
            # Vectorized extraction: select all gridboxes' time series at once
            # This extracts data with shape (time, num_gridboxes)
            data_array = ds[extract_var].values[:, lat_indices, lon_indices]
            
            # Build gridbox_data list from vectorized extraction
            gridbox_data = []
            total_weight = 0.0
            
            for i, (lat_idx, lon_idx, weight, grid_id) in enumerate(gridbox_indices):
                # Extract this gridbox's time series (column i)
                gridbox_series = data_array[:, i]
                
                # Create DataFrame
                df = pd.DataFrame({
                    'Date': time_values,
                    variable_name if is_obs else extract_var: gridbox_series
                })
                
                # Rename column for consistency if needed
                if is_obs and extract_var != variable_name:
                    df.rename(columns={extract_var: variable_name}, inplace=True)
                
                # Check for valid data - skip gridboxes with no data (common for obs data at edges)
                if df.empty or df[variable_name].isna().all():
                    logging.debug(f"DEBUG: Skipping gridbox {grid_id} - no valid data for {variable_name}")
                    continue
                
                gridbox_data.append((df, weight, grid_id))
                total_weight += weight
            
            logging.debug(f"DEBUG: Vectorized extraction complete for {len(gridbox_data)} gridboxes")
            
        except Exception as e:
            # Fallback to sequential extraction if vectorized fails
            logging.debug(f"DEBUG: Vectorized extraction failed ({e}), falling back to sequential extraction")
            gridbox_data = []
            total_weight = 0.0
            
            for lat_idx, lon_idx, weight, grid_id in gridbox_indices:
                try:
                    if is_obs:
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        extract_var = data_vars[0] if data_vars else None
                        if not extract_var:
                            raise Exception(f"No data variable found in observed dataset for {variable_name}")
                        df = extract_timeseries(ds, lat_idx, lon_idx, variable_name=extract_var)
                        if extract_var in df.columns:
                            df.rename(columns={extract_var: variable_name}, inplace=True)
                    else:
                        df = extract_timeseries(ds, lat_idx, lon_idx, variable_name)
                    
                    if df.empty or 'No Data' in df.columns:
                        raise Exception(f"Gridbox {grid_id} has no valid data for {variable_name}")
                    
                    gridbox_data.append((df, weight, grid_id))
                    total_weight += weight
                
                except Exception as e:
                    raise Exception(f"Error extracting data for gridbox {grid_id}: {str(e)}")
        
        if len(gridbox_data) == 0:
            raise Exception("No valid data extracted from any gridbox")
        
        # Normalize weights
        if total_weight == 0:
            raise Exception("Total weight is zero")
        
        # Optimized data consistency check for large polygons
        # For large regions, skip detailed date comparison to improve performance
        base_df = gridbox_data[0][0]
        base_len = len(base_df)
        
        # Quick validation: check lengths match (much faster than set comparison)
        for df, weight, grid_id in gridbox_data[1:]:
            if len(df) != base_len:
                raise Exception(f"Data length mismatch: gridbox {grid_id} has {len(df)} timestamps but expected {base_len}")
        
        logging.debug(f"DEBUG: Date consistency check passed for {len(gridbox_data)} gridboxes (quick validation)")
        
        # Compute weighted sum or average based on variable type
        # Start with the first gridbox's dates
        result_df = pd.DataFrame({'Date': gridbox_data[0][0]['Date'].values})
        
        # Get the data column name (should be the variable name)
        data_col = [col for col in gridbox_data[0][0].columns if col != 'Date'][0]
        
        num_gridboxes = len(gridbox_data)
        num_timesteps = len(result_df)
        
        logging.debug(f"DEBUG: Computing weighted average for {num_gridboxes} gridboxes using vectorized operations...")
        
        # FULLY VECTORIZED APPROACH: Stack all data into a 2D array and compute in one operation
        # Create arrays to hold all data and weights
        all_data = np.zeros((num_timesteps, num_gridboxes), dtype=np.float32)  # Use float32 for memory efficiency
        all_weights = np.zeros(num_gridboxes, dtype=np.float32)
        
        # Fill arrays with data and weights
        for idx, (df, weight, grid_id) in enumerate(gridbox_data):
            all_data[:, idx] = df[data_col].values
            all_weights[idx] = weight / total_weight if not use_sum else weight
        
        # Single matrix-vector multiplication: (timesteps x gridboxes) @ (gridboxes,) = (timesteps,)
        weighted_values = np.dot(all_data, all_weights)
        
        logging.debug(f"DEBUG: âœ… Vectorized computation complete for all {num_gridboxes} gridboxes")
        
        # Create result dataframe
        result_df[variable_name] = weighted_values
        
        # NOTE: Do NOT apply unit conversions here
        # The plotting functions (selected_gridbox_plot, get_trace_data_for_mad) handle
        # all unit conversions uniformly for both single-gridbox and loop traces
        # Converting here would cause double-conversion (e.g., 20Â°C â†’ 68Â°F â†’ 154Â°F)
        
        logging.debug(f"DEBUG: Regional average computed successfully with {len(result_df)} time steps")
        logging.debug(f"DEBUG: Regional sample values (raw): {result_df[variable_name].head(5).tolist()}")
        
        # PERFORMANCE: Cache the result for future use (trendline toggles, slider changes, etc.)
        logging.debug(f"DEBUG: ðŸ’¾ About to cache - cache_key type: {type(cache_key)}, len: {len(cache_key)}")
        logging.debug(f"DEBUG: ðŸ’¾ cache_key elements: grid_ids_len={len(grid_ids_only)}, var={variable_name}, model={model}, freq={frequency}, ssp={ssp}")
        logging.debug(f"DEBUG: ðŸ’¾ cache_key[0] type: {type(cache_key[0])}, is tuple: {isinstance(cache_key[0], tuple)}")
        _REGIONAL_TIMESERIES_CACHE[cache_key] = result_df.copy()
        logging.debug(f"DEBUG: ðŸ’¾ Cached regional data successfully")
        logging.debug(f"DEBUG: ðŸ’¾ Cache now has {len(_REGIONAL_TIMESERIES_CACHE)} entries")
        
        # Verify what was actually stored
        if cache_key in _REGIONAL_TIMESERIES_CACHE:
            logging.debug(f"DEBUG: âœ… Verification: Key exists in cache")
        else:
            logging.debug(f"DEBUG: âŒ ERROR: Key NOT found in cache immediately after storage!")
        
        # Check all keys in cache
        for stored_key in list(_REGIONAL_TIMESERIES_CACHE.keys())[:3]:
            logging.debug(f"DEBUG: ðŸ’¾ Stored key: type={type(stored_key)}, len={len(stored_key) if hasattr(stored_key, '__len__') else 'N/A'}")
        
        return result_df
    
    except Exception as e:
        logging.error(f"ERROR: _extract_regional_average_timeseries failed: {e}")
        import traceback
        traceback.print_exc()
        raise

# --- UI Definition ---
app_ui = ui.page_fluid(
    ui.tags.head(
        ui.tags.meta(name="cache-control", content="no-cache, no-store, must-revalidate"),
        ui.tags.meta(name="pragma", content="no-cache"),
        ui.tags.meta(name="expires", content="0"),
        ui.tags.style("""
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap');
            body { font-family: 'Inter', sans-serif; background-color: #f5f5f5; color: #212529; overflow-x: auto; overflow-y: auto; }
            .custom-card { background: #ffffff; color: #212529; border-radius: 12px; padding: 25px; margin: 15px 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); border: 1px solid #dee2e6; }
            .metric-card { background: white; border-radius: 10px; padding: 15px; margin: 10px 0; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); border-left: 4px solid #343a40; }
            .text-center { text-align: center; }
            .text-muted { color: #6c757d; }
            .container { width: 100% !important; max-width: none !important; margin: 0 auto; padding: 25px; background-color: #fff; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.08); border: 1px solid #e9ecef; margin-top: 20px; margin-bottom: 20px; }
            .row { width: 100% !important; margin: 0 !important; }
            .col-12, .col-lg-3, .col-lg-9 { padding-left: 10px; padding-right: 10px; }
            /* Override Bootstrap container constraints */
            .container-fluid { width: 100% !important; max-width: none !important; }
            /* Ensure full width for all main content */
            .main-content { width: 100% !important; max-width: none !important; }
            /* Force full width layout */
            html, body { width: 100% !important; margin: 0 !important; padding: 0 !important; }
            .shiny-app-container { width: 100% !important; max-width: none !important; }
            /* Override any remaining Bootstrap constraints */
            .container, .container-fluid, .container-sm, .container-md, .container-lg, .container-xl, .container-xxl {
                width: 100% !important;
                max-width: none !important;
            }
            /* Make Plotly widgets responsive */
            .js-plotly-plot, .plotly, .plotly-graph-div {
                width: 100% !important;
                max-width: 100% !important;
                height: auto !important;
                min-height: 500px !important;
            }
            /* Ensure map and plot containers are responsive */
            .map-container, .plot-container {
                width: 100% !important;
                max-width: 100% !important;
                overflow: visible !important;
            }
            /* MAD table responsive styling */
            .mad-table-container {
                width: 100% !important;
                max-width: none !important;
                overflow-x: auto !important;
                overflow-y: auto !important;
            }
            .mad-table-container .js-plotly-plot {
                width: 100% !important;
                max-width: none !important;
            }
            /* Ensure all UI components are responsive */
            .shiny-input-container, .form-group, .form-control {
                width: 100% !important;
                max-width: none !important;
            }
            /* Progressive dropdown responsive behavior */
            .progressive-dropdown-container {
                width: 100% !important;
                max-width: none !important;
            }
            /* Ensure all containers use full width */
            .container-fluid, .container {
                width: 100% !important;
                max-width: none !important;
            }
            h2 { font-size: 3.75rem; font-weight: 700; margin-bottom: 25px; color: #212529; letter-spacing: -0.02em; }
            h3 { font-size: 2rem; font-weight: 600; margin-top: 25px; margin-bottom: 15px; color: #212529; }
            h4 { font-size: 1.5rem; font-weight: 600; margin-top: 20px; margin-bottom: 12px; color: #495057; }
            h5 { font-size: 1.25rem; font-weight: 600; margin-top: 15px; margin-bottom: 10px; color: #495057; }
            ul { list-style-type: disc; margin-left: 20px; line-height: 1.7; }
            li { margin-bottom: 8px; color: #495057; }
            p { line-height: 1.7; color: #495057; margin-bottom: 15px; }
            hr { border-top: 2px solid #dee2e6; margin: 30px 0; }
            .shiny-input-container { margin-bottom: 15px; }
            .js-plotly-plot { margin-top: 20px; border: 1px solid #dee2e6; border-radius: 10px; overflow: hidden; box-shadow: 0 2px 6px rgba(0,0,0,0.08); }
            .loading-message {
                text-align: center;
                font-size: 1.2em;
                color: #495057;
                margin-top: 50px;
                padding: 25px;
                border: 2px solid #dee2e6;
                border-radius: 10px;
                background-color: #ffffff;
                box-shadow: 0 2px 8px rgba(0,0,0,0.06);
            }
            #multi_plot_options {
                display: none;
                background-color: #f8f9fa;
                border: 1px solid #dee2e6;
                border-radius: 10px;
                padding: 20px;
                margin-top: 15px;
            }
            .multi-plot-section {
                border-left: 4px solid #343a40;
                padding-left: 20px;
                margin: 15px 0;
            }
            /* Navigation tabs styling - 1.5x larger */
            .nav-link {
                font-size: 1.35rem !important;
                padding: 15px 24px !important;
                font-weight: 500 !important;
                color: #495057 !important;
                border: none !important;
                background-color: transparent !important;
                transition: all 0.2s ease !important;
            }
            .nav-link:hover {
                color: #212529 !important;
                background-color: #e9ecef !important;
            }
            .nav-link.active {
                color: #212529 !important;
                background-color: #ffffff !important;
                border-bottom: 3px solid #343a40 !important;
                font-weight: 600 !important;
            }
            .nav-tabs {
                border-bottom: 2px solid #dee2e6 !important;
                margin-bottom: 30px !important;
            }
            /* Button styling for theme - EXCLUDE map and plot controls */
            .btn:not(.leaflet-control-zoom-in):not(.leaflet-control-zoom-out):not(.modebar-btn), 
            button:not(.leaflet-control-zoom-in):not(.leaflet-control-zoom-out):not(.modebar-btn):not([class*='leaflet']):not([class*='modebar']), 
            .shiny-input-actionbutton {
                background-color: #343a40 !important;
                color: #ffffff !important;
                border: 1px solid #343a40 !important;
                border-radius: 6px !important;
                padding: 10px 20px !important;
                font-weight: 500 !important;
                transition: all 0.2s ease !important;
            }
            .btn:not(.leaflet-control-zoom-in):not(.leaflet-control-zoom-out):not(.modebar-btn):hover, 
            button:not(.leaflet-control-zoom-in):not(.leaflet-control-zoom-out):not(.modebar-btn):not([class*='leaflet']):not([class*='modebar']):hover, 
            .shiny-input-actionbutton:hover {
                background-color: #23272b !important;
                border-color: #1d2124 !important;
                box-shadow: 0 2px 6px rgba(0,0,0,0.15) !important;
            }
            /* Ensure Leaflet controls maintain default styling */
            .leaflet-control-zoom a, 
            .leaflet-control-layers-toggle,
            .leaflet-bar a {
                background-color: #ffffff !important;
                color: #333333 !important;
                border: 2px solid rgba(0,0,0,0.2) !important;
            }
            .leaflet-control-zoom a:hover,
            .leaflet-bar a:hover {
                background-color: #f4f4f4 !important;
            }
            /* Ensure Plotly modebar buttons maintain default styling */
            .modebar-btn,
            .modebar-btn svg {
                background-color: transparent !important;
                fill: #444444 !important;
            }
            .modebar-btn:hover {
                background-color: rgba(0,0,0,0.05) !important;
            }
            /* Fix for ipyleaflet container to prevent gray map - TARGETED */
            .leaflet-container {
                background-color: #ddd !important;
                height: 600px !important;
                min-height: 600px !important;
            }
            /* Only force visibility on tile pane, not overlay pane (where gridboxes are) */
            .leaflet-tile-pane {
                z-index: 200 !important;
                opacity: 1 !important;
            }
            .leaflet-tile-pane .leaflet-layer {
                opacity: 1 !important;
                visibility: visible !important;
            }
            .leaflet-tile-container {
                opacity: 1 !important;
                visibility: visible !important;
                display: block !important;
            }
            .leaflet-tile {
                opacity: 1 !important;
                visibility: visible !important;
                display: block !important;
            }
            /* Ensure map widget renders properly */
            .jupyter-widgets-view {
                width: 100% !important;
                height: 600px !important;
                min-height: 600px !important;
            }
            .widget-leaflet {
                height: 600px !important;
                min-height: 600px !important;
            }
            /* Force tile loading */
            .leaflet-container img.leaflet-tile {
                max-width: none !important;
                border: 0 !important;
                opacity: 1 !important;
            }
            /* Feature cards for welcome page */
            .feature-card {
                background: #ffffff;
                border: 1px solid #dee2e6;
                border-radius: 10px;
                padding: 20px;
                margin: 15px 0;
                box-shadow: 0 2px 6px rgba(0,0,0,0.06);
                transition: all 0.3s ease;
                cursor: pointer;
            }
            .feature-card:hover {
                box-shadow: 0 4px 12px rgba(0,0,0,0.12);
                transform: translateY(-2px);
            }
            .feature-card h4 {
                color: #212529;
                margin-top: 10px;
            }
            .feature-card p {
                color: #6c757d;
                font-size: 0.95rem;
            }
            /* Team member cards */
            .team-card {
                background: #ffffff;
                border: 1px solid #dee2e6;
                border-radius: 10px;
                padding: 25px;
                margin: 20px 0;
                box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            }
            .team-card h4 {
                color: #212529;
                margin-bottom: 8px;
                font-size: 1.4rem;
            }
            .team-card .role {
                color: #6c757d;
                font-size: 0.9rem;
                font-style: italic;
                margin-bottom: 15px;
            }
            /* Hero section */
            .hero-section {
                background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
                border-radius: 12px;
                padding: 40px 30px;
                margin-bottom: 30px;
                border: 1px solid #dee2e6;
            }
            .hero-section h3 {
                color: #212529;
                font-size: 2.5rem;
                margin-bottom: 15px;
            }
            .hero-section p {
                color: #495057;
                font-size: 1.15rem;
            }
            /* Hide Shiny client errors that are non-critical */
            .shiny-output-error-validation {
                display: none !important;
            }
            .shiny-output-error:has(.shiny-error-silent) {
                display: none !important;
            }
            /* Aggressively hide error modals and notifications */
            .modal:has(.shiny-notification-error),
            .shiny-modal:has(.shiny-notification-error),
            [role="dialog"]:has(.shiny-notification-error) {
                display: none !important;
                opacity: 0 !important;
                pointer-events: none !important;
            }
            .modal-backdrop {
                display: none !important;
            }
            #shiny-notification-panel .shiny-notification-error {
                display: none !important;
            }
            /* Hide specific error messages */
            .shiny-notification-error:has([class*="Cannot set properties"]),
            .shiny-notification:has([class*="Cannot set properties"]),
            .shiny-notification-error:has([class*="Widget is not attached"]),
            .shiny-notification:has([class*="Widget is not attached"]),
            .shiny-notification-error:has([class*="model cleanup"]),
            .shiny-notification:has([class*="model cleanup"]) {
                display: none !important;
            }
            /* Hide all widget-related client errors */
            div[role="status"]:has([class*="Shiny Client Errors"]),
            div:has(> div:has([class*="Shiny Client Errors"])),
            .shiny-notification-error,
            .shiny-notification:has([class*="Shiny Client Errors"]),
            div[class*="shiny-notification"]:has([class*="error"]),
            .modal-content:has([class*="error"]) {
                display: none !important;
            }
            
            /* Hide any error dialogs that might appear */
            .modal:has([class*="error"]),
            .modal:has([class*="Shiny Client Errors"]) {
                display: none !important;
            }
        """),
        ui.tags.script("""
            // ===== SESSION KEEP-ALIVE AND RECONNECTION HANDLER =====
            (function() {
                console.log('ðŸ”„ Keep-alive system initialized');
                
                let keepAliveInterval = null;
                let connectionCheckInterval = null;
                let lastKeepAlive = Date.now();
                let reconnectBannerShown = false;
                
                // Create reconnection banner
                function createReconnectBanner() {
                    if (reconnectBannerShown) return;
                    
                    const banner = document.createElement('div');
                    banner.id = 'reconnect-banner';
                    banner.style.cssText = `
                        position: fixed;
                        top: 0;
                        left: 0;
                        right: 0;
                        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                        color: white;
                        padding: 15px 20px;
                        text-align: center;
                        font-size: 16px;
                        font-weight: 600;
                        z-index: 999999;
                        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
                        display: flex;
                        align-items: center;
                        justify-content: center;
                        gap: 15px;
                    `;
                    
                    banner.innerHTML = `
                        <span style="font-size: 24px; animation: spin 2s linear infinite;">ðŸ”„</span>
                        <span>Connection lost. Attempting to reconnect...</span>
                        <button onclick="location.reload();" style="
                            background: white;
                            color: #667eea;
                            border: none;
                            padding: 8px 20px;
                            border-radius: 6px;
                            font-weight: 600;
                            cursor: pointer;
                            transition: transform 0.2s;
                        " onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1)'">
                            ðŸ”„ Reload Now
                        </button>
                    `;
                    
                    // Add spin animation
                    const style = document.createElement('style');
                    style.textContent = `
                        @keyframes spin {
                            0% { transform: rotate(0deg); }
                            100% { transform: rotate(360deg); }
                        }
                    `;
                    document.head.appendChild(style);
                    
                    document.body.insertBefore(banner, document.body.firstChild);
                    reconnectBannerShown = true;
                    console.warn('âš ï¸ Connection lost - reconnect banner shown');
                }
                
                function removeReconnectBanner() {
                    const banner = document.getElementById('reconnect-banner');
                    if (banner) {
                        banner.remove();
                        reconnectBannerShown = false;
                        console.log('âœ… Connection restored - banner removed');
                    }
                }
                
                // Keep-alive ping function
                function sendKeepAlive() {
                    try {
                        // Check if Shiny is connected
                        if (typeof Shiny !== 'undefined' && Shiny.shinyapp && Shiny.shinyapp.isConnected()) {
                            // Send a lightweight custom message to keep session alive
                            Shiny.setInputValue('keep_alive_ping', Date.now(), {priority: 'event'});
                            lastKeepAlive = Date.now();
                            removeReconnectBanner(); // Remove banner if connection is restored
                            console.log('ðŸ’“ Keep-alive ping sent');
                        } else {
                            console.warn('âš ï¸ Shiny not connected, skipping keep-alive ping');
                            const timeSinceLastPing = Date.now() - lastKeepAlive;
                            // If more than 2 minutes since last successful ping, show banner
                            if (timeSinceLastPing > 120000) {
                                createReconnectBanner();
                            }
                        }
                    } catch (error) {
                        console.warn('âš ï¸ Keep-alive ping failed:', error.message);
                    }
                }
                
                // Connection health check
                function checkConnectionHealth() {
                    try {
                        if (typeof Shiny !== 'undefined' && Shiny.shinyapp) {
                            const isConnected = Shiny.shinyapp.isConnected();
                            
                            if (!isConnected) {
                                console.warn('âš ï¸ Connection health check: DISCONNECTED');
                                createReconnectBanner();
                            } else {
                                // Connection is good
                                const timeSinceLastPing = Date.now() - lastKeepAlive;
                                if (timeSinceLastPing > 90000) { // 1.5 minutes
                                    console.warn('âš ï¸ No keep-alive for 1.5 min, sending emergency ping');
                                    sendKeepAlive();
                                }
                            }
                        }
                    } catch (error) {
                        console.warn('âš ï¸ Connection health check failed:', error.message);
                    }
                }
                
                // Start keep-alive system when Shiny is ready
                function startKeepAlive() {
                    if (keepAliveInterval) {
                        clearInterval(keepAliveInterval);
                    }
                    if (connectionCheckInterval) {
                        clearInterval(connectionCheckInterval);
                    }
                    
                    // Send keep-alive ping every 30 seconds
                    keepAliveInterval = setInterval(sendKeepAlive, 10000);
                    
                    // Check connection health every 15 seconds
                    connectionCheckInterval = setInterval(checkConnectionHealth, 5000);
                    
                    // Send initial ping
                    setTimeout(sendKeepAlive, 2000);
                    
                    console.log('âœ… Keep-alive system started (ping every 10s, check every 5s)');
                }
                
                // Start when document is ready
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', function() {
                        setTimeout(startKeepAlive, 3000);
                    });
                } else {
                    setTimeout(startKeepAlive, 3000);
                }
                
                // Also hook into Shiny's connection events if available
                if (typeof Shiny !== 'undefined') {
                    Shiny.addCustomMessageHandler('keep-alive-init', startKeepAlive);
                    
                    // Listen for disconnection events
                    $(document).on('shiny:disconnected', function() {
                        console.error('âŒ Shiny disconnected event detected');
                        createReconnectBanner();
                    });
                    
                    $(document).on('shiny:connected', function() {
                        console.log('âœ… Shiny connected/reconnected');
                        removeReconnectBanner();
                        lastKeepAlive = Date.now();
                    });
                }
            })();
        """),
        ui.tags.script("""
            // Aggressive cache-busting for ipyleaflet map tiles and widgets
            (function() {
                'use strict';
                
                // Add cache-busting timestamp to all ipyleaflet widget requests
                const cacheBustTimestamp = Date.now();
                
                // Force no-cache for all fetch requests (including widget data)
                const originalFetch = window.fetch;
                window.fetch = function(...args) {
                    let [resource, config] = args;
                    
                    // Add no-cache headers
                    config = config || {};
                    config.headers = config.headers || {};
                    config.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate';
                    config.headers['Pragma'] = 'no-cache';
                    config.headers['Expires'] = '0';
                    
                    // Add timestamp to URL if it's a widget or tile request
                    if (typeof resource === 'string' && 
                        (resource.includes('widget') || resource.includes('tile') || resource.includes('.png'))) {
                        const separator = resource.includes('?') ? '&' : '?';
                        resource = resource + separator + '_cb=' + cacheBustTimestamp;
                    }
                    
                    return originalFetch.call(this, resource, config);
                };
                
                // Force reload of Leaflet map tiles when they load
                function forceMapTileReload() {
                    // Wait for leaflet maps to be available
                    setTimeout(function() {
                        try {
                            // Find all leaflet tile layers and force reload
                            document.querySelectorAll('.leaflet-tile-pane').forEach(function(pane) {
                                const tiles = pane.querySelectorAll('img.leaflet-tile');
                                tiles.forEach(function(tile) {
                                    // Force reload by adding timestamp to src
                                    if (tile.src && !tile.src.includes('_cb=')) {
                                        const separator = tile.src.includes('?') ? '&' : '?';
                                        tile.src = tile.src + separator + '_cb=' + cacheBustTimestamp;
                                    }
                                });
                            });
                            
                            // Force invalidateSize on all leaflet maps
                            if (typeof L !== 'undefined' && L.Map) {
                                document.querySelectorAll('.leaflet-container').forEach(function(container) {
                                    const map = container._leaflet_map;
                                    if (map) {
                                        map.invalidateSize(true);
                                    }
                                });
                            }
                        } catch (e) {
                            console.debug('Cache-busting tile reload: ' + e.message);
                        }
                    }, 1000);
                }
                
                // Run on page load
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', forceMapTileReload);
                } else {
                    forceMapTileReload();
                }
                
                // Also run when navigating to the map tab
                document.addEventListener('click', function(e) {
                    if (e.target && e.target.textContent && 
                        (e.target.textContent.includes('Gridbox Map') || e.target.textContent.includes('Variable Map'))) {
                        setTimeout(forceMapTileReload, 500);
                    }
                });
                
                console.log('ðŸš« Cache-busting enabled for map tiles (timestamp: ' + cacheBustTimestamp + ')');
            })();
        """),
        ui.tags.script("""
            // Restore active tab from URL hash (for Reload Map button)
            (function() {
                'use strict';
                
                function restoreActiveTab() {
                    const hash = window.location.hash.replace('#', '');
                    if (hash === 'gridbox-map' || hash.startsWith('gridbox-map')) {
                        console.log('ðŸ”„ Restoring Gridbox Map tab from URL hash');
                        
                        // Find the Gridbox Map tab button
                        const tabButtons = document.querySelectorAll('[role="tab"]');
                        let gridboxButton = null;
                        
                        for (let btn of tabButtons) {
                            if (btn.textContent && btn.textContent.includes('Gridbox Map')) {
                                gridboxButton = btn;
                                break;
                            }
                        }
                        
                        if (gridboxButton && typeof bootstrap !== 'undefined') {
                            // Use Bootstrap's tab API to show the tab
                            const tab = new bootstrap.Tab(gridboxButton);
                            tab.show();
                            
                            console.log('âœ… Gridbox Map tab restored successfully');
                            
                            // Clean up URL (remove query params and hash)
                            setTimeout(function() {
                                history.replaceState(null, '', window.location.pathname);
                            }, 500);
                        } else {
                            console.log('âš ï¸ Gridbox Map tab button not found or Bootstrap not loaded');
                        }
                    }
                }
                
                // Run after page loads
                if (document.readyState === 'loading') {
                    document.addEventListener('DOMContentLoaded', function() {
                        setTimeout(restoreActiveTab, 300);
                    });
                } else {
                    setTimeout(restoreActiveTab, 300);
                }
            })();
        """),
        ui.tags.script("""
            // Comprehensive widget error suppression
            // Override window.onerror to catch errors at the earliest stage
            (function() {
                const originalOnError = window.onerror;
                
                window.onerror = function(message, source, lineno, colno, error) {
                    const msgStr = String(message);
                    if (msgStr.includes('Widget is not attached') ||
                        msgStr.includes('model cleanup') ||
                        msgStr.includes('Cannot set properties of undefined') ||
                        msgStr.includes('Cannot read properties of undefined') ||
                        msgStr.includes('setting') ||
                        msgStr.includes('[anywidget]') ||
                        msgStr.includes('Runtime not found') ||
                        msgStr.includes('Could not create a view for model')) {
                        console.log('[Suppressed] Non-critical widget error:', msgStr);
                        return true; // Prevent default error handling
                    }
                    if (originalOnError) {
                        return originalOnError(message, source, lineno, colno, error);
                    }
                    return false;
                };
            })();
            
            // Also use addEventListener as backup
            window.addEventListener('error', function(event) {
                if (event.message && (
                    event.message.includes('Widget is not attached') ||
                    event.message.includes('model cleanup') ||
                    event.message.includes('Cannot set properties of undefined') ||
                    event.message.includes('Cannot read properties of undefined') ||
                    event.message.includes('setting') ||
                    event.message.includes('[anywidget]') ||
                    event.message.includes('Runtime not found') ||
                    event.message.includes('Could not create a view for model')
                )) {
                    console.log('[Suppressed] Non-critical widget error (listener):', event.message);
                    event.stopImmediatePropagation();
                    event.stopPropagation();
                    event.preventDefault();
                    return false;
                }
            }, true);
            
            // Override Shiny's error handling to prevent error dialogs
            // Wait for Shiny to be available, then override
            function overrideShinyErrors() {
                if (window.Shiny) {
                    var originalError = window.Shiny.onError;
                    window.Shiny.onError = function(error) {
                        // Convert error to string for checking
                        var errorStr = error ? (error.message || String(error)) : '';
                        
                        if (errorStr.includes('Cannot set properties of undefined') ||
                            errorStr.includes('Cannot read properties of undefined') ||
                            errorStr.includes('setting') ||
                            errorStr.includes('Widget is not attached') ||
                            errorStr.includes('model cleanup') ||
                            errorStr.includes('[anywidget]') ||
                            errorStr.includes('Runtime not found') ||
                            errorStr.includes('Could not create a view for model')) {
                            console.log('[Suppressed] Shiny error:', errorStr);
                            return false; // Suppress the error
                        }
                        if (originalError) {
                            return originalError(error);
                        }
                    };
                    console.log('[Info] Shiny error handler overridden');
                } else {
                    // Retry after a short delay if Shiny isn't loaded yet
                    setTimeout(overrideShinyErrors, 100);
                }
            }
            overrideShinyErrors();
            
            // Intercept and suppress error dialogs at the DOM level
            var observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    if (mutation.type === 'childList') {
                        mutation.addedNodes.forEach(function(node) {
                            if (node.nodeType === 1) { // Element node
                                // FIXED: Convert className to string to handle SVG elements
                                var className = String(node.className || '');
                                if (className && (
                                    className.includes('shiny-notification') ||
                                    className.includes('modal') ||
                                    node.textContent && (
                                        node.textContent.includes('Shiny Client Errors') ||
                                        node.textContent.includes('Cannot set properties of undefined') ||
                                        node.textContent.includes('Cannot read properties of undefined')
                                    )
                                )) {
                                    console.log('Suppressing error dialog:', node);
                                    node.style.display = 'none';
                                    if (node.parentNode) {
                                        node.parentNode.removeChild(node);
                                    }
                                }
                            }
                        });
                    }
                });
            });
            
            // Start observing
            observer.observe(document.body, {
                childList: true,
                subtree: true
            });
            
            // Also suppress promise rejection errors related to widgets
            window.addEventListener('unhandledrejection', function(event) {
                if (event.reason && event.reason.message && (
                    event.reason.message.includes('Widget is not attached') ||
                    event.reason.message.includes('model cleanup') ||
                    event.reason.message.includes('[anywidget]')
                )) {
                    console.log('Suppressed non-critical widget promise rejection:', event.reason.message);
                    event.preventDefault();
                    return false;
                }
            });
        """),
        ui.tags.style("""
            /* Legend scrolling styles for Plotly plots - positioned below plot */
            /* FIXED: Legend at bottom of plot, scrollable horizontally, fixed height */
            .js-plotly-plot .legend {
                max-height: 60px !important;
                max-width: 90% !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                position: relative !important;
                z-index: 1000 !important;
                margin: 10px auto !important;
                padding: 5px 10px !important;
                /* Scrollbar styling */
                scrollbar-width: thin !important;
                scrollbar-color: rgba(0,0,0,0.3) rgba(0,0,0,0.1) !important;
            }

            /* Webkit scrollbar styling for legend */
            .js-plotly-plot .legend::-webkit-scrollbar {
                height: 6px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-track {
                background: rgba(0,0,0,0.1) !important;
                border-radius: 3px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-thumb {
                background: rgba(0,0,0,0.3) !important;
                border-radius: 3px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-thumb:hover {
                background: rgba(0,0,0,0.5) !important;
            }

            .js-plotly-plot .legend .legendscrollbox {
                max-height: 55px !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                width: 100% !important;
            }

            /* Ensure legend items are properly sized and scrollable horizontally */
            .js-plotly-plot .legend .traces {
                max-height: 55px !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                white-space: nowrap !important;
                display: flex !important;
                flex-direction: row !important;
                gap: 15px !important;
            }

            /* Make legend items more compact for horizontal layout */
            .js-plotly-plot .legend .legendtext {
                font-size: 9px !important;
                line-height: 1.2 !important;
                white-space: nowrap !important;
            }
            
            /* Ensure legend items are horizontally aligned */
            .js-plotly-plot .legend .legendtoggle {
                display: inline-flex !important;
                align-items: center !important;
                margin-right: 10px !important;
            }
            
            /* Plot container dimensions - now scrollable */
            .plot-container {
                width: 100% !important;
                min-height: 500px !important;
                overflow: visible !important;
                position: relative !important;
            }
            
            /* Plotly plots - allow larger sizes */
            .plot-container .js-plotly-plot {
                width: 100% !important;
                min-height: 500px !important;
                overflow: visible !important;
            }
        """),
        ui.tags.style("""
            /* Progressive dropdown styles */
            .dropdown-step {
                background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
                border: 2px solid #dee2e6;
                border-radius: 10px;
                padding: 20px;
                margin: 15px 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: all 0.3s ease;
                opacity: 0.3;
                pointer-events: none;
            }
            .dropdown-step.active {
                border-color: #667eea;
                background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
                box-shadow: 0 4px 8px rgba(102, 126, 234, 0.2);
                opacity: 1;
                pointer-events: auto;
            }
            .dropdown-step.completed {
                border-color: #28a745;
                background: linear-gradient(135deg, #f8fff9 0%, #e8f5e8 100%);
                opacity: 1;
                pointer-events: auto;
            }
            .step-explanation {
                font-size: 0.9em;
                color: #6c757d;
                margin-top: 8px;
                font-style: italic;
            }
            .step-title {
                font-weight: 600;
                color: #495057;
                margin-bottom: 10px;
            }
            .progressive-dropdown-container {
                width: 100%;
                margin: 0 auto;
            }
            /* Plotly plot width fixes */
            .plotly-graph-div {
                width: 100% !important;
                max-width: none !important;
                overflow: visible !important;
            }
            .plotly .main-svg {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxis {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxislayer-above {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxislayer-below {
                width: 100% !important;
                max-width: none !important;
            }
            .js-plotly-plot {
                width: 100% !important;
                max-width: none !important;
                overflow: visible !important;
            }
            /* Animation for completion */
            .pulse-animation {
                animation: pulse 1s ease-in-out;
            }
            @keyframes pulse {
                0% { transform: scale(1); }
                50% { transform: scale(1.02); }
                100% { transform: scale(1); }
            }
            .animation-done {
                transition: all 0.3s ease;
            }
            /* Ensure button centering is maintained */
            .text-center {
                text-align: center !important;
            }
            /* Specific styling for the reset selection button container */
            .dropdown-step .text-center {
                display: flex !important;
                justify-content: center !important;
                align-items: center !important;
                width: 100% !important;
            }
            /* Ensure the button itself is centered */
            .dropdown-step .btn {
                margin: 0 auto !important;
                display: block !important;
            }
        """),
        ui.tags.script("""
            // AGGRESSIVE error suppression for Shiny dynamic UI updates
            (function() {
                // Override console.error to suppress Shiny-related errors
                var originalError = console.error;
                console.error = function() {
                    var args = Array.from(arguments);
                    var message = args.join(' ');
                    // Suppress specific Shiny errors
                    if (message.includes('Cannot set properties of undefined') ||
                        message.includes('Cannot read properties of undefined') ||
                        message.includes('SilentException') ||
                        message.includes('setting \'')) {
                        console.log('Suppressed Shiny error:', message);
                        return;
                    }
                    originalError.apply(console, arguments);
                };
                
                // Suppress global errors
                window.addEventListener('error', function(event) {
                    if (event.error && event.error.message) {
                        var msg = event.error.message;
                        if (msg.includes('Cannot set properties of undefined') ||
                            msg.includes('Cannot read properties of undefined') ||
                            msg.includes('setting \'') ||
                            msg.includes('SilentException')) {
                            event.preventDefault();
                            event.stopPropagation();
                            event.stopImmediatePropagation();
                            console.log('Suppressed error:', msg);
                            return false;
                        }
                    }
                }, true);
                
                // Suppress promise rejections
                window.addEventListener('unhandledrejection', function(event) {
                    if (event.reason) {
                        var msg = (event.reason.message || event.reason || '').toString();
                        if (msg.includes('Cannot set properties of undefined') ||
                            msg.includes('Cannot read properties of undefined') ||
                            msg.includes('setting \'') ||
                            msg.includes('SilentException')) {
                            event.preventDefault();
                            event.stopPropagation();
                            console.log('Suppressed promise rejection:', msg);
                            return false;
                        }
                    }
                });
                
                // Aggressively hide Shiny error notifications and modal dialogs
                function hideErrorNotifications() {
                    // Hide error notifications
                    var notifs = document.querySelectorAll(
                        '.shiny-notification-error, ' +
                        '.shiny-output-error, ' +
                        '.shiny-output-error-validation, ' +
                        '[id*="shiny-notification"]'
                    );
                    notifs.forEach(function(el) {
                        var text = (el.textContent || el.innerText || '').toLowerCase();
                        if (text.includes('cannot set properties') ||
                            text.includes('cannot read properties') ||
                            text.includes('undefined') ||
                            text.includes('setting') ||
                            text.includes('silentexception')) {
                            el.remove();
                        }
                    });
                    
                    // Also check for modals/dialogs
                    var modals = document.querySelectorAll('.modal, .shiny-modal, [role="dialog"]');
                    modals.forEach(function(modal) {
                        var text = (modal.textContent || modal.innerText || '').toLowerCase();
                        if (text.includes('shiny client errors') ||
                            text.includes('cannot set properties') ||
                            text.includes('error on client')) {
                            modal.remove();
                            // Also remove backdrop if present
                            var backdrop = document.querySelector('.modal-backdrop');
                            if (backdrop) backdrop.remove();
                        }
                    });
                }
                
                // Run immediately and then every 50ms
                hideErrorNotifications();
                setInterval(hideErrorNotifications, 50);
                
                // Also run on DOM changes (catches newly added error elements)
                if (typeof MutationObserver !== 'undefined') {
                    var observer = new MutationObserver(hideErrorNotifications);
                    observer.observe(document.body, {
                        childList: true,
                        subtree: true
                    });
                }
            })();
        """),
        ui.tags.script("""
            // Fix for ipyleaflet map tiles not loading after page reload
            (function() {
                console.log('ipyleaflet fix script loaded');
                
                // More aggressive fix for ipyleaflet/jupyter widgets
                // Exposed globally so it can be called from reload button
                window.fixIPyLeafletMap = function() {
                    console.log('Running ipyleaflet fix...');
                    
                    // Find all jupyter widget views (ipyleaflet widgets)
                    var widgetViews = document.querySelectorAll('.jupyter-widgets-view, .widget-leaflet');
                    console.log('Found', widgetViews.length, 'jupyter widget views');
                    
                    // Find all leaflet containers
                    var containers = document.querySelectorAll('.leaflet-container');
                    console.log('Found', containers.length, 'leaflet containers');
                    
                    if (containers.length > 0) {
                        containers.forEach(function(container, index) {
                            console.log('Processing container', index);
                            
                            // Method 1: Try using Leaflet's internal map registry
                            try {
                                if (typeof L !== 'undefined' && L.DomUtil) {
                                    // Get the map ID from the container
                                    var mapId = container._leaflet_id;
                                    console.log('Container has leaflet ID:', mapId);
                                    
                                    // Try multiple ways to find the map instance
                                    var map = null;
                                    
                                    // Try L._maps registry
                                    if (L._maps && L._maps[mapId]) {
                                        map = L._maps[mapId];
                                        console.log('Found map via L._maps');
                                    }
                                    
                                    // If found, invalidate and force redraw
                                    if (map) {
                                        console.log('Invalidating map size...');
                                        map.invalidateSize({pan: false});
                                        
                                        // Force a redraw by slightly adjusting zoom
                                        setTimeout(function() {
                                            var currentZoom = map.getZoom();
                                            console.log('Forcing redraw at zoom:', currentZoom);
                                            map.setZoom(currentZoom);
                                        }, 100);
                                    }
                                }
                            } catch (e) {
                                console.log('Error with Leaflet map fix:', e);
                            }
                            
                            // Method 2: Force tile pane visibility
                            var tilePanes = container.querySelectorAll('.leaflet-tile-pane');
                            tilePanes.forEach(function(pane) {
                                pane.style.opacity = '1';
                                pane.style.visibility = 'visible';
                            });
                            
                            // Method 3: Force tile container visibility
                            var tileContainers = container.querySelectorAll('.leaflet-tile-container');
                            tileContainers.forEach(function(tc) {
                                tc.style.opacity = '1';
                                tc.style.visibility = 'visible';
                            });
                            
                            // Method 4: Ensure all tiles are visible
                            var tiles = container.querySelectorAll('.leaflet-tile');
                            console.log('Found', tiles.length, 'tiles');
                            tiles.forEach(function(tile) {
                                tile.style.opacity = '1';
                                tile.style.visibility = 'visible';
                            });
                        });
                    }
                }
                
                // ULTRA AGGRESSIVE startup - check every 25ms for the first 15 seconds
                var startupCheckCount = 0;
                var startupInterval = setInterval(function() {
                    startupCheckCount++;
                    console.log('Startup check #' + startupCheckCount);
                    
                    // Check if leaflet containers exist
                    var containers = document.querySelectorAll('.leaflet-container');
                    if (containers.length > 0) {
                        console.log('Leaflet container found on startup check #' + startupCheckCount);
                        fixIPyLeafletMap();
                    }
                    
                    // Stop after 10 seconds (100 checks)
                    if (startupCheckCount >= 100) {
                        clearInterval(startupInterval);
                        console.log('Startup checks complete');
                    }
                }, 25);
                
                // Also run at specific times as backup
                setTimeout(fixIPyLeafletMap, 1);
                setTimeout(fixIPyLeafletMap, 5);
                setTimeout(fixIPyLeafletMap, 10);
                setTimeout(fixIPyLeafletMap, 25);
                setTimeout(fixIPyLeafletMap, 50);
                setTimeout(fixIPyLeafletMap, 200);
                setTimeout(fixIPyLeafletMap, 500);
                setTimeout(fixIPyLeafletMap, 1000);
                setTimeout(fixIPyLeafletMap, 2000);
                setTimeout(fixIPyLeafletMap, 3000);
                
                // Watch for tab navigation (Gridbox Map page load)
                document.addEventListener('click', function(e) {
                    var target = e.target;
                    // Check if any nav link was clicked
                    if (target && (target.textContent.includes('Gridbox Map') || 
                                   target.closest('a[data-toggle="tab"]') ||
                                   target.closest('.nav-link'))) {
                        console.log('Tab navigation detected, scheduling aggressive fix');
                        // Run fix very aggressively after tab click
                        setTimeout(fixIPyLeafletMap, 10);
                        setTimeout(fixIPyLeafletMap, 50);
                        setTimeout(fixIPyLeafletMap, 100);
                        setTimeout(fixIPyLeafletMap, 200);
                        setTimeout(fixIPyLeafletMap, 300);
                        setTimeout(fixIPyLeafletMap, 500);
                        setTimeout(fixIPyLeafletMap, 700);
                        setTimeout(fixIPyLeafletMap, 1000);
                        setTimeout(fixIPyLeafletMap, 1500);
                        setTimeout(fixIPyLeafletMap, 2000);
                    }
                });
                
                // Watch for visibility changes (tab switching)
                document.addEventListener('visibilitychange', function() {
                    if (!document.hidden) {
                        console.log('Page became visible, running fix');
                        setTimeout(fixIPyLeafletMap, 50);
                        setTimeout(fixIPyLeafletMap, 200);
                        setTimeout(fixIPyLeafletMap, 500);
                    }
                });
                
                // Silent background check for deployment - runs every 3 seconds
                setInterval(function() {
                    var containers = document.querySelectorAll('.leaflet-container');
                    if (containers.length > 0) {
                        containers.forEach(function(container) {
                            var tiles = container.querySelectorAll('.leaflet-tile');
                            var tilePanes = container.querySelectorAll('.leaflet-tile-pane');
                            
                            // Check multiple conditions for gray map
                            var isGray = false;
                            
                            // Condition 1: Very few tiles
                            if (tiles.length < 10) {
                                isGray = true;
                                // Silent fix
                            }
                            
                            // Condition 2: Tile pane has 0 opacity
                            tilePanes.forEach(function(pane) {
                                if (pane.style.opacity === '0' || pane.style.visibility === 'hidden') {
                                    isGray = true;
                                    // Silent fix
                                }
                            });
                            
                            // Condition 3: Tiles are invisible
                            var visibleTiles = 0;
                            tiles.forEach(function(tile) {
                                if (tile.style.opacity !== '0' && tile.style.visibility !== 'hidden') {
                                    visibleTiles++;
                                }
                            });
                            if (visibleTiles < 5 && tiles.length > 0) {
                                isGray = true;
                                // Silent fix
                            }
                            
                            if (isGray) {
                                // Silent fix
                                fixIPyLeafletMap();
                            }
                        });
                    }
                }, 3000);
                
                // Watch for new DOM elements (map loading)
                if (typeof MutationObserver !== 'undefined') {
                    var observer = new MutationObserver(function(mutations) {
                        var hasNewLeaflet = false;
                        mutations.forEach(function(mutation) {
                            mutation.addedNodes.forEach(function(node) {
                                if (node.nodeType === 1) {
                                    if (node.classList && (node.classList.contains('leaflet-container') || 
                                        node.classList.contains('jupyter-widgets-view') ||
                                        node.classList.contains('widget-leaflet'))) {
                                        hasNewLeaflet = true;
                                    } else if (node.querySelector) {
                                        if (node.querySelector('.leaflet-container') || 
                                            node.querySelector('.jupyter-widgets-view')) {
                                            hasNewLeaflet = true;
                                        }
                                    }
                                }
                            });
                        });
                        
                        if (hasNewLeaflet) {
                            console.log('New leaflet/widget detected via MutationObserver');
                            setTimeout(fixIPyLeafletMap, 100);
                            setTimeout(fixIPyLeafletMap, 500);
                        }
                    });
                    
                    observer.observe(document.body, {
                        childList: true,
                        subtree: true
                    });
                }
            })();
        """),
        ui.tags.link(rel="stylesheet", href="https://api.mapbox.com/mapbox-gl-js/v2.15.0/mapbox-gl.css"),
    ),
    ui.h2("ðŸŒŠ Santa Barbara Climate Data Dashboard", class_="text-center"),
    ui.div(ui.p("ðŸ“Š Data Source: Cal-Adapt CSV & NetCDF", class_="text-center text-muted"), class_="data-source"),
    ui.navset_tab(
        ui.nav_panel("ðŸ  Welcome",
            ui.div(
                # Hero Section
                ui.div(
                    ui.h3("Welcome to the Santa Barbara Climate Dashboard", class_="text-center"),
                    ui.p("An interactive tool for exploring climate projections and observational data for Santa Barbara County", class_="text-center"),
                    class_="hero-section"
                ),
                
                # Goals & Objectives Section
                ui.h4("About This Dashboard"),
                ui.p("""Created with support from the Bren Environmental Leadership (BEL) Program, this interactive Shiny for Python 
                dashboard helps users explore, compare, and understand climate projections for Santa Barbara County. It uses the 
                LOCA2-Hybrid datasetâ€”the most recent hybrid-statistical downscaling product developed for California's Fifth Climate 
                Change Assessment. LOCA2 statistically downscales CMIP6 global climate model (GCM) outputs by calibrating them against 
                regional observations, providing more accurate results at the local scale."""),
                ui.p("""The dashboard transforms complex climate data into clear, interactive visuals tailored for scientists, students, 
                community members, and decision-makers. It features location-specific climate visualizations, integrated spatial analysis 
                tools, and is fully open-source, with all code available on GitHub to support transparency, reproducibility, and collaboration."""),
                
                # Getting Started Section
                ui.h4("Getting Started"),
                ui.p("Follow these steps to begin exploring climate data:"),
                ui.tags.ol([
                    ui.tags.li("Navigate to the Gridbox Map page to explore location-specific time series data"),
                    ui.tags.li("Click any grid cell on the map or use the dropdown to select a location"),
                    ui.tags.li("Use the progressive dropdowns to select your data: Data Type â†’ Model â†’ Variable â†’ Scenario â†’ Frequency"),
                    ui.tags.li("View time series plots with trend analysis and compare multiple datasets using Additional Traces mode"),
                    ui.tags.li("Try the interactive shapefile drawing tool to analyze data across custom regions"),
                    ui.tags.li("Visit the Variable Maps page to see spatial patterns of climate change across Santa Barbara County"),
                ]),
                
                # Quick Navigation Cards
                ui.h4("Explore Dashboard Features", style="margin-top: 30px;"),
                ui.div(
                    ui.div(
                        ui.div(
                            ui.h5("ðŸ“ Gridbox Map"),
                            ui.p("Explore location-specific climate time series data. Select gridboxes, compare multiple variables and models, and analyze trends from 1950-2100."),
                            class_="feature-card"
                        ),
                        class_="col-12 col-md-6"
                    ),
                    ui.div(
                        ui.div(
                            ui.h5("ðŸ—ºï¸ Variable Maps"),
                            ui.p("View spatial patterns of climate change across the region. See projected changes in temperature, precipitation, and more across Santa Barbara County."),
                            class_="feature-card"
                        ),
                        class_="col-12 col-md-6"
                    ),
                    ui.div(
                        ui.div(
                            ui.h5("ðŸ“– User Guide"),
                            ui.p("Detailed instructions on how to use each feature, interpret visualizations, and understand the data processing methods."),
                            class_="feature-card"
                        ),
                        class_="col-12 col-md-6"
                    ),
                    ui.div(
                        ui.div(
                            ui.h5("ðŸ“š Glossary"),
                            ui.p("Comprehensive definitions of climate science terms, variables, models, scenarios, and analysis methods used in this dashboard."),
                            class_="feature-card"
                        ),
                        class_="col-12 col-md-6"
                    ),
                    class_="row"
                ),
                
                # Data Sources Section
                ui.h4("Data Sources", style="margin-top: 30px;"),
                ui.div(
                    ui.h5("Projection Data"),
                    ui.p("""LOCA2-Hybrid downscaled CMIP6 climate model projections from Cal-Adapt. Includes 15 global climate models 
                    under multiple emission scenarios (SSP2-4.5, SSP3-7.0, SSP5-8.5) from 1950-2100."""),
                    ui.h5("Observational Data"),
                    ui.p("""Livneh gridded observational dataset providing daily temperature and precipitation data from 1950-2013. 
                    This historical data is used to validate model performance and understand past climate trends."""),
                    ui.h5("Variables Available"),
                ui.tags.ul([
                        ui.tags.li("Temperature: Daily minimum and maximum (tasmin/tasmax)"),
                        ui.tags.li("Precipitation: Daily rainfall (pr)"),
                        ui.tags.li("Humidity: Relative humidity max/min, specific humidity (hursmax/hursmin/huss)"),
                        ui.tags.li("Solar Radiation: Surface downwelling shortwave (rsds)"),
                        ui.tags.li("Wind: Wind speed (wspeed)"),
                    ]),
                    class_="custom-card"
                ),
                
                # Key Features
                ui.h4("Key Features", style="margin-top: 30px;"),
                ui.tags.ul([
                    ui.tags.li("Location-specific climate visualizations with 762 gridboxes covering Santa Barbara County"),
                    ui.tags.li("Compare projection data from 15 CMIP6 global climate models"),
                    ui.tags.li("Analyze observational data (1950-2013) alongside future projections (2014-2100)"),
                    ui.tags.li("Interactive shapefile drawing tool for custom region analysis"),
                    ui.tags.li("Variable spatial maps showing projected changes across the county"),
                    ui.tags.li("Mean Absolute Difference (MAD) analysis for model comparison"),
                    ui.tags.li("Multiple time aggregation options: daily, monthly, and annual data"),
                    ui.tags.li("Automatic unit conversions (temperatures in Â°F, precipitation in inches)"),
                    ui.tags.li("Download capabilities for plots and data"),
                    ui.tags.li("Open-source code available for transparency and reproducibility"),
                ]),
                
                class_="container"
            )
        ),
        ui.nav_panel("ðŸ“– User Guide",
            ui.div(
                ui.h3("Comprehensive User Guide"),
                
                # Introduction
                ui.h4("Introduction"),
                ui.p("""This dashboard provides interactive access to climate projections and observational data for Santa Barbara County. 
                You can explore location-specific time series, compare different climate models and scenarios, analyze spatial patterns, 
                and draw custom regions for aggregated analysis."""),
                
                # Page-by-Page Guide
                ui.h3("Page-by-Page Guide", style="margin-top: 35px;"),
                
                # GRIDBOX MAP PAGE
                ui.h4("ðŸ“ Gridbox Map Page", style="border-left: 4px solid #343a40; padding-left: 15px;"),
                
                ui.h5("Selecting a Gridbox"),
                ui.p("There are two ways to select a gridbox:"),
                ui.tags.ul([
                    ui.tags.li("Click any grid cell directly on the map (the selected cell will be highlighted with a red border)"),
                    ui.tags.li("Use the 'Select Gridbox' dropdown menu to choose from 762 available locations"),
                ]),
                
                ui.h5("Progressive Dropdown Sequence"),
                ui.p("Once a gridbox is selected, use the progressive dropdowns to specify your data:"),
                ui.tags.ol([
                    ui.tags.li(ui.HTML("<strong>Data Type:</strong> Choose 'Projection' for future climate projections (1950-2100) or 'Observed' for historical data (1950-2013 from Livneh dataset)")),
                    ui.tags.li(ui.HTML("<strong>Model:</strong> For projections, select from 15 CMIP6 global climate models (e.g., ACCESS-CM2, CESM2-LENS, GFDL-ESM4). For observed data, this is automatically set to 'Observed'")),
                    ui.tags.li(ui.HTML("<strong>Variable:</strong> Select the climate variable (temperature, precipitation, humidity, radiation, wind speed)")),
                    ui.tags.li(ui.HTML("<strong>Scenario:</strong> Choose emission scenario (SSP2-4.5, SSP3-7.0, or SSP5-8.5). Note: Not all models have all scenarios available")),
                    ui.tags.li(ui.HTML("<strong>Frequency:</strong> Choose data aggregation: Daily (raw daily values), Monthly (monthly averages/totals), or Annual (yearly averages/totals)")),
                ]),
                
                ui.h5("Understanding the Time Series Plot"),
                ui.tags.ul([
                    ui.tags.li("X-axis shows years from 1950 to 2100 (or 1950-2013 for observational data)"),
                    ui.tags.li("Y-axis shows values in converted units (Â°F for temperature, inches or inches/day for precipitation)"),
                    ui.tags.li("Hover over any point to see the exact value and year"),
                    ui.tags.li("A trend line is automatically calculated and displayed"),
                    ui.tags.li("The plot title shows the selected gridbox coordinates (latitude, longitude)"),
                ]),
                
                ui.h5("Additional Traces Mode"),
                ui.p("Compare multiple datasets on the same plot:"),
                ui.tags.ol([
                    ui.tags.li("Click 'Enable Additional Traces' button to activate multi-trace mode"),
                    ui.tags.li("Use the additional progressive dropdowns to select more data combinations"),
                    ui.tags.li("Each trace appears in a different color with its own legend entry"),
                    ui.tags.li("Click legend entries to toggle traces on/off"),
                    ui.tags.li("Double-click a legend entry to isolate that trace"),
                    ui.tags.li("Up to 6 additional traces can be added for comprehensive comparison"),
                ]),
                
                ui.h5("Interactive Shapefile Drawing Tool (Loop Traces)"),
                ui.p("Analyze climate data across custom regions:"),
                ui.tags.ol([
                    ui.tags.li("Click the 'Draw Polygon' button to activate drawing mode"),
                    ui.tags.li("Click multiple points on the map to define your region of interest"),
                    ui.tags.li("Double-click to close the polygon (or click near the first point)"),
                    ui.tags.li("The dashboard automatically identifies all gridboxes within your polygon"),
                    ui.tags.li("Climate data is extracted for each enclosed gridbox and averaged"),
                    ui.tags.li("A new trace appears on the plot showing the regional average"),
                    ui.tags.li("Name your custom region using the 'Loop Name' input field"),
                    ui.tags.li("Draw multiple regions to compare different areas of Santa Barbara County"),
                ]),
                ui.p(ui.HTML("<em>How it works:</em> The tool uses point-in-polygon algorithms to determine which gridboxes fall within your drawn shape. For each gridbox, it extracts the full time series, then computes the spatial average across all enclosed gridboxes. This provides a representative climate signal for your custom region.")),
                
                ui.h5("Mean Absolute Difference (MAD) Analysis"),
                ui.p("Compare how different climate models agree or disagree:"),
                ui.tags.ul([
                    ui.tags.li("MAD analysis appears automatically when comparing models"),
                    ui.tags.li("Shows the average absolute difference between two model time series"),
                    ui.tags.li("Lower MAD values indicate better model agreement"),
                    ui.tags.li("Useful for assessing model uncertainty and identifying reliable projections"),
                ]),
                
                # VARIABLE MAPS PAGE
                ui.h4("ðŸ—ºï¸ Variable Maps Page", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 35px;"),
                
                ui.h5("What Variable Maps Show"),
                ui.p("""Variable maps display the projected change in climate variables across Santa Barbara County. Each gridbox 
                shows the difference between a future time period and a historical baseline (1981-2010), allowing you to see 
                spatial patterns of climate change."""),
                
                ui.h5("Selecting Map Data"),
                ui.tags.ol([
                    ui.tags.li(ui.HTML("<strong>Data Type:</strong> Choose 'Projection' for model data or 'Observed' for historical trends")),
                    ui.tags.li(ui.HTML("<strong>Model:</strong> Select which climate model to display (for projection data only)")),
                    ui.tags.li(ui.HTML("<strong>Variable:</strong> Choose the climate variable to map")),
                    ui.tags.li(ui.HTML("<strong>Scenario:</strong> Select the emission scenario (only scenarios available for your chosen model/variable combination will appear)")),
                ]),
                
                ui.h5("Time Range Selection"),
                ui.tags.ul([
                    ui.tags.li("Use the 'Start Year' and 'End Year' sliders to define your analysis period"),
                    ui.tags.li("End Year automatically adjusts to be at least one year after Start Year"),
                    ui.tags.li("The map shows the average change during this period compared to 1981-2010 baseline"),
                ]),
                
                ui.h5("Change Type Options"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>Absolute Change:</strong> Shows the raw difference in units (e.g., +3.5Â°F warming, +2 inches more rain)")),
                    ui.tags.li(ui.HTML("<strong>Percentage Change:</strong> Shows the relative change (e.g., +15% more precipitation). Most useful for precipitation and humidity variables")),
                ]),
                
                ui.h5("Understanding the Color Scale"),
                ui.tags.ul([
                    ui.tags.li("For temperature and most variables: Red indicates increase, blue indicates decrease"),
                    ui.tags.li("For precipitation: Blue indicates increase (more rain), red indicates decrease (less rain)"),
                    ui.tags.li("Darker colors indicate larger magnitude changes"),
                    ui.tags.li("Hover over any gridbox to see the exact change value and location"),
                ]),
                
                ui.h5("How Variable Maps Are Created"),
                ui.p(ui.HTML("<strong>Data Processing Steps:</strong>")),
                ui.tags.ol([
                    ui.tags.li("The dashboard loads climate data for all 762 gridboxes covering Santa Barbara County"),
                    ui.tags.li("For each gridbox, daily data is aggregated to annual values (averages for temperature, totals for precipitation)"),
                    ui.tags.li("A baseline climatology is calculated by averaging values from 1981-2010"),
                    ui.tags.li("For your selected time period, annual values are averaged"),
                    ui.tags.li("The change is calculated: Future Average - Historical Baseline"),
                    ui.tags.li("For percentage change: ((Future - Baseline) / Baseline) Ã— 100"),
                    ui.tags.li("All gridboxes are plotted on the map with colors representing the magnitude of change"),
                ]),
                
                # Data Calculations & Conversions
                ui.h3("Data Calculations & Unit Conversions", style="margin-top: 35px;"),
                
                ui.h4("Temperature Conversions"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>Projection Data:</strong> Original units are Kelvin. Converted to Fahrenheit: (K - 273.15) Ã— 9/5 + 32")),
                    ui.tags.li(ui.HTML("<strong>Observational Data:</strong> Original units are Celsius. Converted to Fahrenheit: C Ã— 9/5 + 32")),
                    ui.tags.li("All temperature plots and maps display values in Â°F for user convenience"),
                ]),
                
                ui.h4("Precipitation Conversions"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>Projection Data:</strong> Original units are kg mâ»Â² sâ»Â¹ (mass flux). Converted to inches/day: Ã— 86400 / 25.4")),
                    ui.tags.li(ui.HTML("<strong>Observational Data:</strong> Original units are mm. Converted to inches: / 25.4")),
                    ui.tags.li("For monthly/annual aggregations, daily values are summed to give total inches"),
                    ui.tags.li("Daily precipitation plots show inches/day; monthly and annual show total inches"),
                ]),
                
                ui.h4("Time Aggregations"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>Daily:</strong> Raw daily values from the dataset, no aggregation performed")),
                    ui.tags.li(ui.HTML("<strong>Monthly:</strong> Daily values averaged (temperature) or summed (precipitation) for each month")),
                    ui.tags.li(ui.HTML("<strong>Annual:</strong> Daily values averaged (temperature) or summed (precipitation) for each year")),
                    ui.tags.li("For spatial maps, only annual aggregation is used to capture long-term trends"),
                ]),
                
                ui.h4("Trend Calculations"),
                ui.p("Trend lines on time series plots are calculated using linear regression:"),
                ui.tags.ul([
                    ui.tags.li("The slope represents the rate of change per year"),
                    ui.tags.li("For temperature: slope in Â°F/year shows warming or cooling rate"),
                    ui.tags.li("For precipitation: slope in inches/year shows increasing or decreasing precipitation"),
                    ui.tags.li("RÂ² values (when displayed) indicate how well the trend fits the data"),
                ]),
                
                # Tips & Best Practices
                ui.h3("Tips and Best Practices", style="margin-top: 35px;"),
                ui.tags.ul([
                    ui.tags.li("Compare multiple models to assess uncertaintyâ€”different models may show different magnitudes of change"),
                    ui.tags.li("Use observational data (1950-2013) to validate model performance during the historical period"),
                    ui.tags.li("For precipitation, annual or monthly aggregation provides more reliable trends than daily data"),
                    ui.tags.li("SSP2-4.5 represents moderate emissions, SSP3-7.0 represents medium-high, SSP5-8.5 represents very high emissions"),
                    ui.tags.li("The interactive shapefile drawing tool is ideal for analyzing specific watersheds, cities, or regions of interest"),
                    ui.tags.li("Download plot images using the camera icon in the top-right of any plot"),
                    ui.tags.li("When comparing models, pay attention to both the mean change and the spread across models"),
                ]),
                
                # Troubleshooting
                ui.h3("Troubleshooting", style="margin-top: 35px;"),
                ui.h4("No data appears after selection"),
                ui.tags.ul([
                    ui.tags.li("Some model/scenario combinations may not be available. Try a different scenario or model"),
                    ui.tags.li("Ensure you've completed all required dropdown selections"),
                    ui.tags.li("Check that your selected time range is valid (Start Year < End Year)"),
                ]),
                
                ui.h4("Map takes a long time to load"),
                ui.tags.ul([
                    ui.tags.li("Variable maps process data for all 762 gridboxes, which can take 15-30 seconds"),
                    ui.tags.li("Wait for the loading indicator to complete before adjusting settings"),
                    ui.tags.li("Avoid rapid changes to the time sliders; wait for previous calculations to finish"),
                ]),
                
                ui.h4("Plot appears cluttered with many traces"),
                ui.tags.ul([
                    ui.tags.li("Click legend entries to temporarily hide traces"),
                    ui.tags.li("Double-click a single legend entry to isolate that trace"),
                    ui.tags.li("Consider using separate analysis sessions for different comparison groups"),
                ]),
                
                class_="container"
            )
        ),
        ui.nav_panel("ðŸ“š Glossary",
            ui.div(
                ui.h3("Comprehensive Climate Data Glossary"),
                ui.p("This glossary defines key terms, datasets, models, and methods used throughout the dashboard."),
                
                # Data Sources Section
                ui.h4("Data Sources & Datasets", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Cal-Adapt"),
                ui.p("""Cal-Adapt encompasses a range of research and development efforts designed to provide access to California climate data. 
                Each component of Cal-Adapt serves a specific purpose within the broader mission. As Cal-Adapt evolves and expands, it aims to 
                support California's Climate Change Assessments and offer a more comprehensive and powerful solution for technical and data-intensive needs."""),
                
                ui.h5("LOCA2-Hybrid"),
                ui.p("""The most recent hybrid-statistical downscaling product developed for California's Fifth Climate Change Assessment. 
                LOCA2 statistically downscales CMIP6 global climate model outputs by calibrating them against regional observations, providing 
                more accurate results at the local scale. This dashboard uses LOCA2-Hybrid downscaled data for all projection scenarios."""),
                
                ui.h5("Livneh Dataset"),
                ui.p("""Gridded observational dataset providing daily temperature and precipitation data from 1950-2013. This historical 
                reconstruction combines weather station observations with gridded interpolation to provide spatially continuous observational 
                data for model validation."""),
                
                ui.h5("Reanalysis Datasets (Historical Reconstruction)"),
                ui.p("""Reconstructions of the historical weather record that combine model data with historical observational data. Reanalysis 
                products synthesize disparate sources of observational data and use an atmospheric model to produce a spatiotemporally continuous, 
                self-consistent dataset, avoiding the gaps and data collection inconsistencies in weather station data. Like a climate model, 
                reanalysis products have a complete set of atmospheric and surface weather variables on a full spatial grid."""),
                
                ui.h5("NetCDF"),
                ui.p("Network Common Data Formâ€”a set of interfaces for array-oriented scientific data. NetCDF files are the standard format for climate model output and gridded observational datasets."),
                
                # Climate Models & Methods Section
                ui.h4("Climate Models & Methods", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Global Climate Model (GCM)"),
                ui.p("""A mathematical model that represents the physical processes in the atmosphere, ocean, cryosphere, and land surface. 
                GCMs are used to simulate the climate system and predict future climate conditions based on different scenarios of greenhouse 
                gas emissions and other factors. GCMs produce physically consistent simulations of climate over historical and future periods."""),
                
                ui.h5("CMIP6 (Coupled Model Intercomparison Project, Phase 6)"),
                ui.p("""A coordinated international effort to produce climate model output with consistent standards in areas such as variable 
                naming and experimental design. This consistency allows different models to be compared to each other more easily. CMIP6 is the 
                latest generation of global climate models (ca. 2020), used in the Intergovernmental Panel on Climate Change Sixth Assessment 
                Report (IPCC AR6), and in California's Fifth Climate Change Assessment. This dashboard includes 15 CMIP6 models."""),
                
                ui.h5("Historical Runs from Global Climate Models"),
                ui.p("""Simulations that use the historical record of greenhouse gas concentrations as inputs. Using combined simulations of 
                the atmosphere, ocean, and land surface, GCMs produce a physically consistent timeline of plausible climate conditions over the 
                historical period. Unlike reanalysis data, these simulations do not reproduce specific events from the historical record, but 
                instead represent the general conditions during that time period."""),
                
                ui.h5("Downscaled Simulations"),
                ui.p("""Regional simulations over the Western United States and California, produced by downscaling Global Climate Models from 
                CMIP6. These downscaled simulations were created in support of California's Fifth Climate Change Assessment."""),
                
                ui.h5("Statistical Downscaling"),
                ui.p("""A method to generate fine scale spatial resolution data outputs from coarser scale global climate models using statistical 
                relationships between the coarse global climate model output and observed climatological conditions at fine scale spatial resolutions. 
                LOCA2-Hybrid uses statistical downscaling to produce high-resolution climate projections."""),
                
                ui.h5("Dynamical Downscaling"),
                ui.p("""A method to generate fine scale spatial resolution data from coarse scale global climate models using a dynamical regional 
                weather model. The regional model uses the global climate model output as the inputs to generate additional projections. The Analytics 
                Engine hosts dynamically downscaled model output created using the Weather Research and Forecasting (WRF) model."""),
                
                ui.h5("Climate Sensitivity"),
                ui.p("""A measurement that describes the equilibrium change in global mean surface temperature resulting from a doubling of 
                atmospheric carbon dioxide (COâ‚‚) concentrations compared to pre-industrial levels (1850-1900). Different climate models have 
                different climate sensitivities, contributing to projection uncertainty."""),
                
                ui.h5("Climatology"),
                ui.p("""The long-term average or baseline conditions for a given climate variable (such as temperature or precipitation) over a 
                specified historical period. These values are typically calculated from multi-year datasets and are used as a reference point to 
                assess changes, anomalies, or trends in future climate projections. Best scientific practice is to use a 30-year period to ensure 
                the reference period captures a range of variability. This dashboard uses 1981-2010 as the baseline climatology."""),
                
                # Simulations & Uncertainty Section
                ui.h4("Simulations & Uncertainty", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Simulation"),
                ui.p("""A computational process used to model and analyze the behavior of complex systems by representing them through mathematical 
                models. Simulations involve running climate models on different parameters to predict future climate conditions based on various 
                scenarios. In the context of this dashboard, a simulation is a specific ensemble member from a GCM."""),
                
                ui.h5("Ensemble Member"),
                ui.p("""A single simulation or GCM run within a larger set of simulations (referred to as an ensemble) that are generated to assess 
                the uncertainty and variability in climate model projections. Each ensemble member is typically produced by slightly varying the 
                initial conditions, model parameters, or by using different models altogether. Using multiple ensemble members helps account for the 
                natural variability of the climate system and the uncertainties inherent within model predictions."""),
                
                ui.h5("Model Run"),
                ui.p("""Refers to data from different initial-condition ensemble runs of a GCM. Climate models are often run multiple times with 
                slightly different initial conditions (ensemble members), and each of these runs is termed as a 'model run'. For instance, the 
                dashboard contains LOCA2-Hybrid data for different model runs. ACCESS-CM2 r1i1p1f1 refers to one model run while ACCESS-CM2 r2i1p1f1 
                is another model run."""),
                
                ui.h5("Model Uncertainty"),
                ui.p("""Uncertainty in global climate model output that arises from design differences between models. Global climate models are 
                developed by different research institutions and differ in how they represent the global climate system. In some approaches, it may 
                be appropriate to average responses from different models to obtain a consensus model estimate (referred to as a multi-model mean). 
                Alternatively, it may be informative to look at the spread of a particular response across multiple models."""),
                
                ui.h5("Scenario Uncertainty"),
                ui.p("""Uncertainty that arises from not knowing how people, policies, the economy, and technology will evolve in the future to 
                address the issue of climate change. Projecting levels of future emissions requires inherent assumptions about economic production, 
                land use, technological advancements, and energy use that create differences in the timing and strength of climate response between 
                scenarios."""),
                
                # Scenarios & Projections Section
                ui.h4("Scenarios & Projections", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Shared Socioeconomic Pathways (SSPs)"),
                ui.p("""Inputs into the latest generation of IPCC reports which describe potential pathways the world could take in terms of 
                features such as political, economic, and other societal dynamics and choices which impact greenhouse gas emissions, anthropogenic 
                aerosol generation, and land use changes. SSPs are an update to the "Representative Concentration Pathways" (RCPs) used in an earlier 
                CMIP phase. This dashboard includes three SSP scenarios:"""),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>SSP2-4.5:</strong> Intermediate emissions scenario representing moderate climate policy")),
                    ui.tags.li(ui.HTML("<strong>SSP3-7.0:</strong> Medium-high emissions scenario with limited climate policy")),
                    ui.tags.li(ui.HTML("<strong>SSP5-8.5:</strong> Very high emissions scenario representing fossil fuel-intensive development")),
                ]),
                
                ui.h5("Projection"),
                ui.p("""A potential future evolution of a quantity or set of quantities, often associated with climate variables such as temperature 
                or sea level. Projections are typically based on simulations produced by climate models, assuming specific scenarios of greenhouse gas 
                emissions, land use, and other factors. Unlike predictions, projections do not imply certainty but rather illustrate a range of possible 
                outcomes based on different assumptions and conditions. Projections are used to explore the potential impacts of climate change under 
                various future pathways."""),
                
                # Variables & Units Section
                ui.h4("Climate Variables & Units", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Temperature Variables"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>tasmin:</strong> Daily minimum near-surface air temperature (2m above ground). Projection data in Kelvin, converted to Â°F for display")),
                    ui.tags.li(ui.HTML("<strong>tasmax:</strong> Daily maximum near-surface air temperature (2m above ground). Projection data in Kelvin, converted to Â°F for display")),
                    ui.tags.li(ui.HTML("<strong>obs_tmin / obs_tmax:</strong> Observed minimum/maximum temperature from Livneh dataset. Original units in Celsius, converted to Â°F")),
                ]),
                
                ui.h5("Precipitation Variables"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>pr:</strong> Precipitation rate (projection data). Original units: kg mâ»Â² sâ»Â¹. Converted to inches/day for daily data, total inches for monthly/annual")),
                    ui.tags.li(ui.HTML("<strong>obs_pr:</strong> Observed precipitation from Livneh dataset. Original units: mm. Converted to inches (Ã· 25.4)")),
                ]),
                
                ui.h5("Humidity Variables"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>hursmax / hursmin:</strong> Maximum and minimum near-surface relative humidity (%)")),
                    ui.tags.li(ui.HTML("<strong>huss:</strong> Near-surface specific humidity (kg/kg) - the mass ratio of water vapor to total air mass")),
                ]),
                
                ui.h5("Other Variables"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>rsds:</strong> Surface Downwelling Shortwave Radiation (W/mÂ²) - solar energy reaching Earth's surface")),
                    ui.tags.li(ui.HTML("<strong>wspeed:</strong> Near-surface wind speed (m/s) - can be converted to mph by multiplying by 2.237")),
                ]),
                
                # Analysis Methods Section
                ui.h4("Analysis Methods", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Mean Absolute Difference (MAD)"),
                ui.p("""A statistical measure comparing the differences between two datasets, commonly used to assess model agreement. MAD is 
                calculated as the average of the absolute differences between corresponding values in two time series. Lower MAD values indicate 
                better agreement between models or between a model and observations. This dashboard displays MAD when comparing different models 
                or scenarios."""),
                
                ui.h5("Anomaly Calculations"),
                ui.p("""The difference between an observed or projected value and a baseline climatology. Anomalies are useful for identifying 
                extreme events and long-term trends. Calculated as: Anomaly = Observed Value - Climatological Mean. Positive anomalies indicate 
                above-normal conditions, negative anomalies indicate below-normal conditions."""),
                
                ui.h5("Trend Analysis"),
                ui.p("""Linear regression applied to time series data to identify long-term changes. The slope of the trend line represents the 
                rate of change per year (e.g., Â°F/year for temperature, inches/year for precipitation). RÂ² values indicate how well the linear 
                trend explains the data variability (values closer to 1 indicate stronger trends)."""),
                
                ui.h5("P-values and Statistical Significance"),
                ui.p("""Statistical measures used to determine whether observed trends or differences are statistically significant (unlikely to 
                occur by chance). P-values less than 0.05 are typically considered statistically significant, meaning there is less than 5% 
                probability the trend occurred by random chance. Future implementations may include significance testing for spatial patterns."""),
                
                # Frequency Options Section
                ui.h4("Time Aggregation Options", style="border-left: 4px solid #343a40; padding-left: 15px; margin-top: 30px;"),
                
                ui.h5("Daily Data"),
                ui.p("Raw daily values from the climate model or observational dataset. No temporal aggregation is performed. Best for studying short-term variability and extreme events."),
                
                ui.h5("Monthly Aggregation"),
                ui.p("""Daily values aggregated to monthly resolution. For temperature variables, monthly values represent the average of all 
                daily values in that month. For precipitation, monthly values represent the sum of all daily precipitation in that month. Monthly 
                aggregation reduces noise while preserving seasonal patterns."""),
                
                ui.h5("Annual Aggregation"),
                ui.p("""Daily values aggregated to annual resolution. For temperature variables, annual values represent the average of all daily 
                values in that year. For precipitation, annual values represent the total annual precipitation. Annual aggregation is best for 
                identifying long-term trends and is the only aggregation used for Variable Maps."""),
                
                class_="container"
            )
        ),
        ui.nav_panel("â„¹ï¸ About",
            ui.div(
                ui.h3("About the Santa Barbara Climate Dashboard"),
                
                # Dashboard Overview
                ui.h4("Project Overview"),
                ui.p("""Created with support from the Bren Environmental Leadership (BEL) Program, this interactive Shiny for Python 
                dashboard helps users explore, compare, and understand climate projections for Santa Barbara County. It uses the 
                LOCA2-Hybrid datasetâ€”the most recent hybrid-statistical downscaling product developed for California's Fifth Climate 
                Change Assessment. LOCA2 statistically downscales CMIP6 global climate model (GCM) outputs by calibrating them against 
                regional observations, providing more accurate results at the local scale."""),
                
                ui.p("""The dashboard transforms complex climate data into clear, interactive visuals tailored for scientists, students, 
                community members, and decision-makers. It features location-specific climate visualizations, integrated spatial analysis 
                tools, and is fully open-source, with all code available on GitHub to support transparency, reproducibility, and 
                collaboration. The project promotes open data science by providing transparent, up-to-date climate information and supports 
                public education on Santa Barbara's future climate risks."""),
                
                ui.p("""Through the Santa Barbara Climate Dashboard, users can compare model outputs, examine how historical projections 
                align with observed data, and create interactive maps that show projected changes across different regions and scenarios 
                in the county."""),
                
                # Meet the Team Section
                ui.h3("Meet the Santa Barbara Climate Dashboard Team", style="margin-top: 40px;"),
                
                ui.div(
                    ui.h4("Ozair Usmani"),
                    ui.p("Dashboard Developer & BEL Fellow", class_="role"),
                    ui.p("""Ozair is interested in pursuing a career in climate science, specifically with improving climate models to 
                    better improve future projections. He began his interest by working with Professor Loaiciga on a saltwater intrusion 
                    groundwater modeling research project. The goal of this project was to show that saltwater intrusion in the Oxnard 
                    plain is worse with two pumping wells near the coast and one pumping well further inland, compared to only one pumping 
                    well further inland. He also showed that barrier wells help mitigate saltwater intrusion. Through his Bren Environmental 
                    Leadership Fellowship in the summer of 2025, he worked with Ph.D. student Cali Pfleger in the Stevenson Lab on 
                    developing a full climate dashboard for Santa Barbara County, based on Cal-Adapt data."""),
                    class_="team-card"
                ),
                
                ui.div(
                    ui.h4("Cali Pfleger"),
                    ui.p("Research Collaborator & Ph.D. Student", class_="role"),
                    ui.p("""Cali Pfleger earned her B.A. in Geology and Environmental Studies from Cornell College, where she developed a 
                    strong foundation in climate systems, environmental change, and sustainability engagement. At the Bren School, her 
                    research explores hydroclimatic variability and the ways climate systems respond to both natural events and human-driven 
                    influences, using climate models to better understand large-scale patterns and their regional impacts."""),
                    ui.p("""Alongside her research, Cali is dedicated to bridging the gap between climate science and public understanding. 
                    Developing strategies to communicate complex climate information in clear, accessible ways. Her academic journey has 
                    included mentoring students, contributing to collaborative research efforts, and participating in sustainability 
                    initiatives within and beyond the university. Looking ahead, she aims to continue advancing climate research while 
                    fostering stronger connections between scientific knowledge, policy, and community decision-making."""),
                    class_="team-card"
                ),
                
                ui.div(
                    ui.h4("Samantha Stevenson-Karl"),
                    ui.p("Principal Investigator & Climate DataLab Director", class_="role"),
                    ui.p("""Sam is the PI of the Climate DataLab effort, and has done a lot of the Web development work as well as building 
                    tutorials and generally making sure everything is on track!"""),
                    class_="team-card"
                ),
                
                # Climate Data Lab
                ui.h3("Climate Data Lab", style="margin-top: 40px;"),
                ui.p("""This dashboard is part of the broader Climate Data Lab initiative at UC Santa Barbara. The Climate Data Lab 
                develops tools, resources, and educational materials to make climate data more accessible to researchers, educators, 
                and the public."""),
                ui.p(ui.HTML('Learn more at: <a href="https://climate-datalab.org/" target="_blank" style="color: #343a40; font-weight: 600;">climate-datalab.org</a>')),
                
                # Current Implementations
                ui.h3("Current Dashboard Features", style="margin-top: 40px;"),
                ui.h4("Interactive Capabilities"),
                ui.tags.ul([
                    ui.tags.li(ui.HTML("<strong>Interactive Shapefile Drawing Tool:</strong> Draw custom polygons on the Gridbox Map to analyze climate data across user-defined regions. The tool automatically identifies enclosed gridboxes and computes spatial averages.")),
                    ui.tags.li(ui.HTML("<strong>Variable Spatial Maps:</strong> View projected climate changes across Santa Barbara County with interactive choropleth maps showing gridbox-level changes for any time period and scenario.")),
                    ui.tags.li(ui.HTML("<strong>Multi-Trace Comparison:</strong> Compare up to 6 additional datasets simultaneously on a single plot. Mix and match different models, scenarios, variables, and frequencies.")),
                    ui.tags.li(ui.HTML("<strong>Mean Absolute Difference (MAD) Analysis:</strong> Automatically calculated when comparing models to quantify agreement and assess uncertainty.")),
                ]),
                
                ui.h4("Data Processing & Analysis"),
                ui.tags.ul([
                    ui.tags.li("762 gridboxes covering Santa Barbara County at high spatial resolution"),
                    ui.tags.li("15 CMIP6 global climate models with multiple emission scenarios"),
                    ui.tags.li("Observational data validation (Livneh 1950-2013)"),
                    ui.tags.li("Automatic unit conversions for user convenience (Â°F, inches)"),
                    ui.tags.li("Multiple temporal aggregations (daily, monthly, annual)"),
                    ui.tags.li("Trend analysis with linear regression"),
                    ui.tags.li("Climate anomaly calculations relative to 1981-2010 baseline"),
                ]),
                
                # Technical Details
                ui.h3("Technical Details", style="margin-top: 40px;"),
                ui.div(
                    ui.tags.ul([
                        ui.tags.li(ui.HTML("<strong>Framework:</strong> Built with Shiny for Python for reactive, interactive web applications")),
                        ui.tags.li(ui.HTML("<strong>Visualization:</strong> Plotly for interactive, publication-quality plots and maps")),
                        ui.tags.li(ui.HTML("<strong>Data Processing:</strong> xarray, pandas, and numpy for efficient climate data manipulation")),
                        ui.tags.li(ui.HTML("<strong>Data Source:</strong> Cal-Adapt LOCA2-Hybrid downscaled CMIP6 projections and Livneh observational data")),
                        ui.tags.li(ui.HTML("<strong>Geospatial:</strong> Shapely and point-in-polygon algorithms for custom region analysis")),
                        ui.tags.li(ui.HTML("<strong>Open Source:</strong> All code available on GitHub for transparency and reproducibility")),
                    ]),
                    class_="custom-card"
                ),
                
                # Data Coverage
                ui.h4("Data Coverage"),
                ui.div(
                ui.tags.ul([
                        ui.tags.li(ui.HTML("<strong>Spatial Coverage:</strong> Santa Barbara County with 762 gridboxes at ~6km resolution")),
                        ui.tags.li(ui.HTML("<strong>Temporal Coverage:</strong> 1950-2100 (projections), 1950-2013 (observations)")),
                        ui.tags.li(ui.HTML("<strong>Climate Variables:</strong> Temperature, precipitation, humidity, solar radiation, wind speed")),
                        ui.tags.li(ui.HTML("<strong>Scenarios:</strong> SSP2-4.5 (moderate), SSP3-7.0 (medium-high), SSP5-8.5 (very high emissions)")),
                        ui.tags.li(ui.HTML("<strong>Models:</strong> 15 CMIP6 GCMs including ACCESS-CM2, CESM2-LENS, GFDL-ESM4, and more")),
                    ]),
                    class_="custom-card"
                ),
                
                # Acknowledgments
                ui.h4("Acknowledgments", style="margin-top: 30px;"),
                ui.p("""This project was developed with support from the Bren Environmental Leadership (BEL) Program at UC Santa Barbara. 
                Climate data provided by Cal-Adapt and California's Fifth Climate Change Assessment. Observational data from the Livneh 
                gridded dataset. We thank the climate modeling groups participating in CMIP6 for making their model output publicly available."""),
                
                class_="container"
            )
        ),
        ui.nav_panel("ðŸ“ Gridbox Map",
            ui.div(
                # FIXED: Layout optimized for better space utilization with scrollable container
                ui.div(
                    ui.div(
                        ui.output_ui("progressive_dropdown_ui"),
                        ui.output_ui("polygon_loading_indicator"),
                        ui.output_ui("loop_naming_ui"),
                        class_="col-12 col-xl-2 col-lg-3"
                    ),
                    ui.div(
                        ui.output_ui("map_and_data_output"),
                        class_="col-12 col-xl-10 col-lg-9",
                        style="overflow-y: auto; max-height: 90vh;"
                    ),
                    class_="row"
                ),
                class_="container-fluid px-3",
                style="max-width: 100%;"
            )
        ),
        ui.nav_panel("ðŸ—ºï¸ Variable Maps",
            ui.div(
                ui.h3("Climate Variable Spatial Maps"),
                ui.p("Explore spatial patterns of climate change across the region. All variables show annual averages/totals.", class_="text-muted"),
                ui.output_ui("variable_map_controls"),
                ui.output_ui("variable_map_time_controls"),
                ui.output_ui("variable_spatial_map"),
                class_="container"
            )
        )
    )
)
def create_empty_map(message="No data available"):
    """Create an empty map with a message."""
    fig = go.Figure()
    fig.add_annotation(
        text=message,
        x=0.5, y=0.5,
        showarrow=False,
        font=dict(size=16, color="gray")
    )
    fig.update_layout(
        mapbox=dict(
            style="open-street-map",
            center=dict(lat=34.4, lon=-119.7),
            zoom=8
        ),
        margin=dict(l=0, r=0, t=0, b=0),
        height=400
    )
    return fig
# --- Server Logic ---
def server(input, output, session: Session):
    # Debug: Print all available inputs
    logging.debug("DEBUG: Available inputs:", [attr for attr in dir(input) if not attr.startswith('_')])
    
    # reactive_netcdf_filepaths: Stores (variable, model, ssp) -> file_path
    reactive_netcdf_filepaths = reactive.Value({})
    
    # _cached_datasets: Stores (variable, model, ssp) -> xr.Dataset
    _cached_datasets = reactive.Value({})

    reactive_grid_geojson = reactive.Value(None)
    reactive_grid_feature_ids = reactive.Value([])

    # Map reload trigger - increment this to force map re-render
    # Per-file observed points (prec, tmax, tmin)
    obs_points_prec = reactive.Value([])  # list of (lat, lon)
    obs_points_tmax = reactive.Value([])
    obs_points_tmin = reactive.Value([])
    # Store discovered file paths for fallback rendering
    obs_prec_path = reactive.Value(None)
    obs_tmax_path = reactive.Value(None)
    obs_tmin_path = reactive.Value(None)

    # Reactive value for CSV data
    csv_data = reactive.Value(None)

    # Reactive value for selected grid ID (from map click or dropdown), now holds "lat_idx,lon_idx"
    selected_grid_id = reactive.Value(None)
    hovered_grid_id = reactive.Value(None)  # Track which gridbox is being hovered over
    
    # Explicitly clear any previous selection on startup
    selected_grid_id.set(None)
    logging.debug("DEBUG: Cleared grid selection on session start")
    
    # Note: Grid selection clearing is handled in the multi-plot mode toggle handler

    # Reactive values for NetCDF plot selection
    selected_netcdf_variable = reactive.Value(None)
    selected_netcdf_model = reactive.Value(None)
    selected_netcdf_frequency = reactive.Value(None)  # NEW: Add frequency selection
    selected_netcdf_ssp = reactive.Value(None)
    
    # Reactive trigger for plot updates
    plot_update_trigger = reactive.Value(0)
    
    # Reactive trigger for startup
    startup_trigger = reactive.Value(0)
    
    # Reactive values for progressive dropdown system
    dropdown_step_1_completed = reactive.Value(False)
    dropdown_step_2_completed = reactive.Value(False)
    dropdown_step_3_completed = reactive.Value(False)
    
    # Reactive flag to track when variable filtering is complete for a specific grid
    variable_filtering_complete = reactive.Value(False)
    filtered_grid_id = reactive.Value(None)
    dropdown_step_4_completed = reactive.Value(False)
    dropdown_step_5_completed = reactive.Value(False)
    dropdown_step_6_completed = reactive.Value(False)
    
    # Reactive values for multi-plot mode
    multi_plot_step_1_completed = reactive.Value(False)
    multi_plot_step_2_completed = reactive.Value(False)
    multi_plot_step_3_completed = reactive.Value(False)
    multi_plot_step_4_completed = reactive.Value(False)
    multi_plot_step_5_completed = reactive.Value(False)
    multi_plot_step_6_completed = reactive.Value(False)
    
    # Reactive flags to track when variable filtering is complete for multi-plot mode
    multi_plot_variable_filtering_complete = reactive.Value(False)
    multi_plot_filtered_grid_id = reactive.Value(None)
    
    # Reactive values for multi-plot data
    multi_plot_grid_id = reactive.Value(None)
    multi_plot_variable = reactive.Value(None)
    multi_plot_model = reactive.Value(None)
    multi_plot_frequency = reactive.Value(None)  # NEW: Add frequency for multi-plot
    multi_plot_ssp = reactive.Value(None)
    # Reactive value to control multi-plot mode checkbox
    multi_plot_mode_enabled = reactive.Value(False)
    multi_plot_action = reactive.Value(None)  # 'add_to_existing' or 'new_plot'
    
    # Reactive values for loop-based regional averaging (Start New Selection mode)
    selected_loop_polygon = reactive.Value(None)  # Stores polygon coordinates
    selected_loop_gridboxes = reactive.Value(None)  # Stores list of (grid_id, weight) tuples
    selected_loop_name = reactive.Value(None)  # Stores user-provided loop name
    selected_loop_available_variables = reactive.Value(None)  # Stores set of variables available in all loop gridboxes
    
    # Reactive values for loop-based regional averaging (Additional Traces mode)
    multi_plot_loop_polygon = reactive.Value(None)
    multi_plot_loop_gridboxes = reactive.Value(None)
    multi_plot_loop_name = reactive.Value(None)
    multi_plot_loop_available_variables = reactive.Value(None)  # Stores set of variables available in all loop gridboxes
    
    # Reactive value to track polygon processing status (for loading indicator)
    polygon_processing = reactive.Value(False)  # True when processing a drawn polygon, False when done
    
    # Reactive value to force map re-render ONLY when "Start New Selection" is clicked
    # This prevents unnecessary map re-renders on mode toggles or step changes
    map_reset_trigger = reactive.Value(0)  # Increment to force re-render
    
    # Reactive value to store all active loops for map display
    active_loops = reactive.Value([])  # List of dicts: {'polygon': [...], 'name': '...', 'color': '...', 'trace_id': '...'}
    
    # System for unlimited multi-plots
    multi_plots_list = reactive.Value([])  # List of plot configurations
    current_multi_plot_id = reactive.Value(0)  # Counter for unique plot IDs
    multi_plot_time_ranges = reactive.Value({})  # Store time ranges for each plot ID
    multi_plot_update_trigger = reactive.Value(0)  # Trigger for multi-plot updates
    multi_plot_status_message = reactive.Value("")  # Status messages for user feedback
    
    # Additional traces for main plot when user chooses to combine on main
    main_plot_additional_traces = reactive.Value([])
    main_plot_update_trigger = reactive.Value(0)
    
    # FIXED: Add persistent storage for main plot additional traces
    # This prevents traces from being lost when plots are regenerated
    main_plot_persistent_traces = reactive.Value([])
    
    # FIXED: Add flag to prevent clearing traces during plot updates
    preserve_additional_traces = reactive.Value(True)
    
    # Variable Map page reactive values
    var_map_data_type = reactive.Value(None)  # "Projection" or "Observed"
    var_map_model = reactive.Value(None)
    var_map_variable = reactive.Value(None)
    var_map_scenario = reactive.Value(None)
    var_map_start_year = reactive.Value(1950)
    var_map_end_year = reactive.Value(2014)
    var_map_change_type = reactive.Value("absolute")  # "absolute" or "percentage"
    var_map_step_1_completed = reactive.Value(False)
    var_map_step_2_completed = reactive.Value(False)
    var_map_step_3_completed = reactive.Value(False)
    
    # Initialize reactive values safely to prevent undefined errors
    try:
        # Ensure all reactive values have safe initial states
        if main_plot_additional_traces.get() is None:
            main_plot_additional_traces.set([])
        if main_plot_persistent_traces.get() is None:
            main_plot_persistent_traces.set([])
        if main_plot_update_trigger.get() is None:
            main_plot_update_trigger.set(0)
    except Exception as init_error:
        logging.debug(f"DEBUG: Error initializing reactive values: {init_error}")
        # Force set safe defaults
        try:
            main_plot_additional_traces.set([])
            main_plot_persistent_traces.set([])
            main_plot_update_trigger.set(0)
        except:
            pass
    
    # Execute startup data loading immediately when server function is called
    logging.info("STARTUP: Loading data synchronously...")
    logging.debug("DEBUG: Startup data loading effect called")
    
    # 1. Build the complete filepath map and collect all unique options
    filepaths_map = {}
    variables = set()
    models = set()
    scenarios = set()
    
    files_analyzed = 0
    logging.info("Checking file paths:")
    for filename in NETCDF_FILE_LIST:
        exists = os.path.exists(filename)
        logging.info(f"- {filename}: {'Exists' if exists else 'MISSING'}")
        if not exists:
            continue
            
        var, model, ssp = parse_filename(filename)
        if var and model and ssp:
            filepaths_map[(var, model, ssp)] = filename
            variables.add(var)
            models.add(model)
            scenarios.add(ssp)
            files_analyzed += 1
    
    logging.info(f"Analyzed {files_analyzed} NetCDF files")
    logging.info(f"Found variables: {sorted(variables)}")
    logging.info(f"Found models: {sorted(models)}")
    logging.info(f"Found scenarios: {sorted(scenarios)}")
    
    # Set reactive values
    reactive_netcdf_filepaths.set(filepaths_map)
    logging.info(f"STARTUP: Set reactive_netcdf_filepaths with {len(filepaths_map)} entries")
    
    def get_trace_data_for_mad(trace_config, time_range=None):
        """
        Extract trace data for MAD calculation, respecting current time range settings.
        This function applies the same data processing as the plotting functions.
        
        Args:
            trace_config: Dictionary with trace configuration (variable, model, ssp, frequency, lat_idx, lon_idx, or loop_gridboxes, loop_name)
            time_range: Optional tuple of (start_year, end_year) to filter data
        
        Returns:
            DataFrame with 'Date' and variable columns, or None if extraction fails
        """
        try:
            # Check if this is a loop-based trace
            loop_gridboxes = trace_config.get('loop_gridboxes')
            loop_name = trace_config.get('loop_name')
            is_loop = loop_gridboxes is not None and len(loop_gridboxes) > 0
            
            # Extract trace configuration variables (needed for both loop and non-loop traces)
            variable = trace_config['variable']
            model = trace_config['model']
            ssp = trace_config['ssp']
            frequency = trace_config['frequency']
            lat_idx = trace_config.get('lat_idx')
            lon_idx = trace_config.get('lon_idx')
            
            # Get file path
            filepaths_map = reactive_netcdf_filepaths.get()
            if not filepaths_map:
                return None
            
            # Create cache key (include loop data if present)
            if is_loop:
                # Use loop name and gridbox count for cache key
                cache_key = (
                    variable,
                    model,
                    ssp,
                    frequency,
                    f"loop_{loop_name}_{len(loop_gridboxes)}",
                    None,  # lon_idx not used for loops
                    time_range
                )
            else:
                cache_key = (
                    variable,
                    model,
                    ssp,
                    frequency,
                    lat_idx,
                    lon_idx,
                    time_range
                )
                
            # Check cache first (for both loop and non-loop traces)
            if cache_key in _time_series_cache:
                logging.debug(f"DEBUG: MAD - Using cached data for {variable}")
                return _time_series_cache[cache_key].copy()
            
            # Handle loop-based regional average
            if is_loop:
                logging.debug(f"DEBUG: MAD - Extracting loop-based data for {loop_name} with {len(loop_gridboxes)} gridboxes")
                try:
                    df_raw = _extract_regional_average_timeseries(
                        loop_gridboxes,
                        variable,
                        model,
                        frequency,
                        ssp,
                        filepaths_map,
                        _cached_datasets.get(),
                        obs_gridbox_mappings=_obs_gridbox_mapping_cache  # Pass cache for performance
                    )
                    if df_raw is None or df_raw.empty:
                        logging.debug(f"DEBUG: MAD - Loop extraction returned no data")
                        return None
                    df = df_raw.copy()
                    
                    # Load dataset for unit conversion checks (needed below)
                    if variable.startswith('obs_'):
                        ds = load_observed_dataset(variable)
                        if ds is None:
                            logging.debug(f"DEBUG: MAD - Could not load observed dataset for loop")
                            return None
                    else:
                        # Load model dataset
                        file_key = (variable, model, ssp)
                        if file_key not in filepaths_map:
                            logging.debug(f"DEBUG: MAD - File key not found for loop trace")
                            return None
                        file_path = filepaths_map[file_key]
                        try:
                            with xr.open_dataset(file_path, engine="netcdf4") as temp_ds:
                                ds = temp_ds.load()
                        except Exception as e:
                            logging.debug(f"DEBUG: MAD - Error loading dataset for loop: {e}")
                            return None
                except Exception as e:
                    logging.debug(f"DEBUG: MAD - Error extracting loop data: {e}")
                    import traceback
                    traceback.print_exc()
                    return None
            else:
                # Handle single gridbox extraction
                # Handle observed data
                if variable.startswith('obs_'):
                    ds = load_observed_dataset(variable)
                    if ds is None:
                        return None
                    
                    # For observed data, use coordinate mapping like plotting functions
                    # Get the target coordinates for the grid indices
                    try:
                        trace_grid_id = f"{lat_idx},{lon_idx}"
                        target_lat, target_lon = _get_coordinates_from_grid_id(trace_grid_id, filepaths_map)
                        if target_lat is not None and target_lon is not None:
                            # Find the most encompassing observed grid cell
                            obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(target_lat, target_lon, ds)
                            if obs_lat_idx is not None and obs_lon_idx is not None:
                                # Update lat/lon indices to use mapped coordinates
                                lat_idx, lon_idx = obs_lat_idx, obs_lon_idx
                                logging.debug(f"DEBUG: MAD - Mapped observed grid coordinates ({lat_idx}, {lon_idx}) for variable {variable}")
                            else:
                                logging.debug(f"DEBUG: MAD - Using fallback observed grid coordinates for {variable}")
                        else:
                            logging.debug(f"DEBUG: MAD - Could not get target coordinates for grid {trace_grid_id}")
                    except Exception as e:
                        logging.debug(f"DEBUG: MAD - Error mapping observed coordinates: {e}")
                        # Continue with original indices
                
                    # Extract the time series data for observed gridbox
                    df_raw = extract_timeseries(ds, lat_idx, lon_idx, variable)
                else:
                    # Handle model data
                    file_key = (variable, model, ssp)
                    if file_key not in filepaths_map:
                        return None
                    
                    file_path = filepaths_map[file_key]
                    try:
                        with xr.open_dataset(file_path, engine="netcdf4") as temp_ds:
                            ds = temp_ds.load()
                    except Exception as e:
                        logging.debug(f"DEBUG: Error loading dataset for MAD: {e}")
                        return None
                
                # Extract time series
                df_raw = extract_timeseries(ds, lat_idx, lon_idx, variable)
            
            # For loops, df is already set above; for single gridbox, set it here
            if not is_loop:
                if df_raw is None or df_raw.empty:
                    return None
                df = df_raw.copy()
            # Note: Loop df was already set from _extract_regional_average_timeseries
                
            # Apply unit conversions for ALL traces (both single-gridbox and loop)
            # Loop data from _extract_regional_average_timeseries is in RAW units
            # and needs the same conversions as single-gridbox data
            # Get the actual variable name used in the dataset for unit conversion
            actual_var_name = None
            if variable.startswith('obs_'):
                # For observed data, find the actual data variable name
                data_vars = [var for var in ds.data_vars.keys() if var != 'spatial_ref']
                if data_vars:
                    actual_var_name = data_vars[0]
                else:
                    actual_var_name = variable
            else:
                actual_var_name = variable
            
            # Apply unit conversions (same as plotting functions)
            if variable in ["tasmin", "tasmax"]:
                original_units = ds[actual_var_name].attrs.get('units', '').lower()
                if original_units in ['k', 'kelvin']:
                    df[variable] = (df[variable] - 273.15) * 9/5 + 32
                    logging.debug(f"DEBUG: MAD - Converted projected temperature {variable} from Kelvin to Fahrenheit")
            elif variable in ["obs_tmin", "obs_tmax"]:
                # Observed temperature conversion - Livneh data is always in Celsius
                logging.debug(f"DEBUG: MAD - Converting observed temperature {variable} from Celsius to Fahrenheit")
                df[variable] = (df[variable] * 9/5) + 32
            elif variable == "pr":
                original_units = ds[actual_var_name].attrs.get('units', '').lower()
                if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                    df[variable] = df[variable] * 86400 / 25.4
            elif variable == "obs_pr":
                # Observed precipitation conversion - Livneh data is in mm
                df[variable] = df[variable] / 25.4  # Convert mm to inches
            
            # Apply frequency conversion (same as plotting functions)
            # Loop traces return daily data that needs to be resampled just like single-gridbox traces
            if frequency == 'daily':
                # For daily data, keep it as daily for accurate MAD calculations
                df_freq = df.copy()
                df_freq["Year"] = df_freq["Date"].dt.year
            elif frequency == 'monthly':
                # For monthly data, resample to monthly
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            elif frequency == 'annual':
                # For annual data, resample to annual
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            else:
                # Default to annual if frequency not specified or invalid
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            
            # Apply time range filtering if provided
            if time_range and len(time_range) == 2:
                try:
                    start_year, end_year = time_range
                    if frequency == 'daily':
                        # For daily data, filter by date range
                        start_date = pd.Timestamp(f"{start_year}-01-01")
                        end_date = pd.Timestamp(f"{end_year}-12-31")
                        df_freq = df_freq[(df_freq["Date"] >= start_date) & (df_freq["Date"] <= end_date)]
                    else:
                        # For monthly and annual data, filter by year
                        df_freq = df_freq[(df_freq['Year'] >= start_year) & (df_freq['Year'] <= end_year)]
                except Exception as e:
                    logging.debug(f"DEBUG: MAD - Error applying time range filter: {e}")
                    # Continue without filtering if there's an error
            
            # Store in cache before returning
            _time_series_cache[cache_key] = df_freq.copy()
            
            return df_freq
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in get_trace_data_for_mad: {e}")
            return None
    
    # Load grid cells - find common cells across all files
    logging.info("Computing common grid cells across all NetCDF files...")
    try:
        if filepaths_map:
            filepaths_list = list(filepaths_map.values())
            logging.info(f"Computing common valid grid cells across {len(filepaths_list)} files...")
            
            # Use representative files since all 398 files have identical grid structure
            sample_files = filepaths_list[:5]  # Use first 5 files - all files have identical grids
            logging.info(f"Using {len(sample_files)} representative files for grid cell computation (all files have identical grid structure)...")
            
            import signal
            import time
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Grid cell computation timed out")
            
            # Set timeout of 10 seconds for processing 5 representative files
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(10)
            
            try:
                common_cells = find_common_valid_gridcells(sample_files)
            except TimeoutError:
                logging.warning("WARNING: Grid cell computation timed out, using fallback")
                common_cells = None
            except Exception as e:
                logging.warning(f"WARNING: Grid cell computation failed: {e}, using fallback")
                common_cells = None
            finally:
                signal.alarm(0)  # Cancel the alarm
            
            if common_cells:
                grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
                logging.info(f"Found {len(common_cells)} common valid grid cells")
                
                # Set reactive values
                reactive_grid_feature_ids.set(grid_feature_ids)
                logging.info(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
                    
                # Create GeoJSON for map using the first file for coordinate reference
                sample_file = next(iter(filepaths_map.values()))
                with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                    geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                    reactive_grid_geojson.set(geojson_data)
                    logging.info("STARTUP: Set reactive_grid_geojson")
                    
                    # Don't auto-select first grid cell - let user choose explicitly
                    # first_grid_id = grid_feature_ids[0]
                    # selected_grid_id.set(first_grid_id)
                    # print(f"STARTUP: Auto-selected first grid cell: {first_grid_id}")
                    logging.info("STARTUP: Grid cells loaded - user must select explicitly")
            else:
                logging.warning("WARNING: No common grid cells found, using fallback")
                # Fallback: use all cells from first file within Santa Barbara bounds
                sample_file = next(iter(filepaths_map.values()))
                with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                    fallback_cells = get_grid_in_bounds(ds)
                    grid_feature_ids = [f"{lat},{lon}" for lat, lon in fallback_cells]
                    reactive_grid_feature_ids.set(grid_feature_ids)
                    logging.info(f"STARTUP: Using fallback with {len(grid_feature_ids)} grid cells within Santa Barbara bounds")
                    
                    geojson_data, feature_ids = create_grid_geojson(fallback_cells, ds)
                    reactive_grid_geojson.set(geojson_data)
                    logging.info("STARTUP: Set reactive_grid_geojson (fallback)")
                    
                    # Don't auto-select first grid cell - let user choose explicitly
                    # first_grid_id = grid_feature_ids[0]
                    # selected_grid_id.set(first_grid_id)
                    # print(f"STARTUP: Auto-selected first grid cell (fallback): {first_grid_id}")
                    logging.info("STARTUP: Grid cells loaded (fallback) - user must select explicitly")
        else:
            logging.info("No filepaths available for grid computation!")
    except Exception as e:
        logging.error(f"Error computing common grid cells: {e}")
        logging.info(f"Full traceback: {traceback.format_exc()}")
        # Create fallback grid cells
        fallback_cells = [(i, j) for i in range(10, 30) for j in range(10, 30)]
        grid_feature_ids = [f"{lat},{lon}" for lat, lon in fallback_cells]
        reactive_grid_feature_ids.set(grid_feature_ids)
        logging.info(f"STARTUP: Set fallback grid_feature_ids with {len(grid_feature_ids)} entries")
    
    # Build observations (Livneh) common cells
    try:
        # Try both original temporal filenames and converted .nc names
        obs_prec = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_prec_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_prec:
            obs_prec = _discover_obs_path(["livneh_day_Prec_clip_15Oct2014.1950-2013.nc"])
        obs_tmax = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_tmax_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_tmax:
            obs_tmax = _discover_obs_path(["livneh_day_Tmax_clip_15Oct2014.1950-2013.nc"])
        obs_tmin = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_tmin_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_tmin:
            obs_tmin = _discover_obs_path(["livneh_day_Tmin_clip_15Oct2014.1950-2013.nc"])
        if not obs_prec:
            obs_prec = _discover_obs_path(["livneh_day_Prec_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_prec
        if not obs_tmax:
            obs_tmax = _discover_obs_path(["livneh_day_Tmax_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_tmax
        if not obs_tmin:
            obs_tmin = _discover_obs_path(["livneh_day_Tmin_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_tmin
        logging.info(f"OBS STARTUP: Discovered files -> prec={obs_prec} tmax={obs_tmax} tmin={obs_tmin}")
        obs_files = [p for p in [obs_prec, obs_tmax, obs_tmin] if p and os.path.exists(p)]
        # Save discovered paths for later use in render fallbacks
        if obs_prec and os.path.exists(obs_prec):
            obs_prec_path.set(obs_prec)
        if obs_tmax and os.path.exists(obs_tmax):
            obs_tmax_path.set(obs_tmax)
        if obs_tmin and os.path.exists(obs_tmin):
            obs_tmin_path.set(obs_tmin)
        # Remove intersection logic; we no longer compute common cells
            # Also compute per-file valid points (no intersection) for display
            for f, holder in [
                (obs_prec, obs_points_prec),
                (obs_tmax, obs_points_tmax),
                (obs_tmin, obs_points_tmin),
            ]:
                try:
                    ds = _open_dataset_fallback(f)
                    if ds is None:
                        holder.set([])
                        continue
                    var = list(ds.data_vars.keys())[0]
                    lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
                    if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
                        holder.set([])
                        ds.close(); continue
                    dims = ds[var].dims
                    if 'time' not in dims or lat_dim not in dims or lon_dim not in dims:
                        holder.set([])
                        ds.close(); continue
                    reduce_dims = tuple(d for d in dims if d not in (lat_dim, lon_dim))
                    mask = ~ds[var].isnull().all(dim=reduce_dims)
                    n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
                    pts = []
                    for i in range(n_i):
                        for j in range(n_j):
                            if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                    # Fallback: if no valid-mask points were detected, still plot all coordinate points
                    if not pts and lat2d is not None and lon2d is not None:
                        try:
                            total_i, total_j = lat2d.shape[0], lat2d.shape[1] if lat2d.ndim==2 else 0
                            if total_j == 0 and hasattr(lon2d, 'shape'):
                                total_j = lon2d.shape[1] if lon2d.ndim==2 else 0
                            if total_i and total_j:
                                for i in range(total_i):
                                    for j in range(total_j):
                                        lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                        pts.append((lat, lon))
                        except Exception:
                            pass
                    holder.set(pts)
                    ds.close()
                except Exception as _e:
                    holder.set([])
        else:
            logging.info("OBS STARTUP: One or more Livneh files missing")

        # Always compute per-file points (regardless of intersection success)
        for f, holder in [
            (obs_prec, obs_points_prec),
            (obs_tmax, obs_points_tmax),
            (obs_tmin, obs_points_tmin),
        ]:
            try:
                if not (f and os.path.exists(f)):
                    holder.set([])
                    continue
                ds = _open_dataset_fallback(f)
                if ds is None:
                    holder.set([])
                    continue
                var = list(ds.data_vars.keys())[0]
                lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
                if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
                    holder.set([])
                    ds.close(); continue
                pts = []
                dims = ds[var].dims
                if 'time' in dims and lat_dim in dims and lon_dim in dims:
                    reduce_dims = tuple(d for d in dims if d not in (lat_dim, lon_dim))
                    mask = ~ds[var].isnull().all(dim=reduce_dims)
                    n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
                    for i in range(n_i):
                        for j in range(n_j):
                            if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                if not pts and hasattr(lat2d, 'shape') and hasattr(lon2d, 'shape'):
                    try:
                        total_i = lat2d.shape[0]
                        total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
                        for i in range(total_i):
                            for j in range(total_j):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                    except Exception:
                        pass
                holder.set(pts)
                logging.info(f"OBS STARTUP: {os.path.basename(f)} -> points stored: {len(pts)}")
                ds.close()
            except Exception as _e:
                holder.set([])
    except Exception as e:
        logging.info(f"OBS STARTUP: Error computing observed cells: {e}")
    
    logging.info("STARTUP: Data loading complete")
    
    @reactive.Effect
    def _trigger_startup():
        """Trigger startup effect"""
        logging.info("STARTUP: _trigger_startup called")
        startup_trigger.set(1)
    @reactive.Effect
    @reactive.event(startup_trigger)
    def _load_data_on_startup():
        """Load data when the server starts"""
        logging.info("STARTUP: _load_data_on_startup called")
        
        # 1. Build the complete filepath map and collect all unique options
        filepaths_map = {}
        variables = set()
        models = set()
        scenarios = set()
        
        files_analyzed = 0
        logging.info("Checking file paths (optimized for faster startup):")
        # Only check first 50 files for faster startup - all files follow same pattern
        files_to_check = NETCDF_FILE_LIST[:50] if len(NETCDF_FILE_LIST) > 50 else NETCDF_FILE_LIST
        for filename in files_to_check:
            exists = os.path.exists(filename)
            if files_analyzed < 10:  # Only print first 10 for faster startup
                logging.info(f"- {filename}: {'Exists' if exists else 'MISSING'}")
            if not exists:
                continue
                
            var, model, ssp = parse_filename(filename)
            if var and model and ssp:
                filepaths_map[(var, model, ssp)] = filename
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
                files_analyzed += 1
        
        # Fill in remaining files without checking existence (for faster startup)
        if len(NETCDF_FILE_LIST) > 50:
            logging.info(f"Fast-scanning remaining {len(NETCDF_FILE_LIST) - 50} files...")
            for filename in NETCDF_FILE_LIST[50:]:
                var, model, ssp = parse_filename(filename)
                if var and model and ssp:
                    filepaths_map[(var, model, ssp)] = filename
                    variables.add(var)
                    models.add(model)
                    scenarios.add(ssp)
                    files_analyzed += 1
        
        logging.info(f"Analyzed {files_analyzed} NetCDF files")
        logging.info(f"Found variables: {sorted(variables)}")
        logging.info(f"Found models: {sorted(models)}")
        logging.info(f"Found scenarios: {sorted(scenarios)}")
        
        # Set reactive values
        reactive_netcdf_filepaths.set(filepaths_map)
        logging.info(f"STARTUP: Set reactive_netcdf_filepaths with {len(filepaths_map)} entries")
        
        # Use pre-computed map data for INSTANT loading
        map_data = _precompute_map_data()
        if map_data:
            # Set reactive values from pre-computed data - INSTANT LOADING
            reactive_grid_feature_ids.set(map_data['grid_feature_ids'])
            reactive_grid_geojson.set(map_data['geojson_data'])
            logging.info(f"STARTUP: âœ… INSTANT MAP LOADING - Set reactive values from pre-computed data - {len(map_data['grid_feature_ids'])} grid cells")
        else:
            logging.warning("WARNING: Could not pre-compute map data, falling back to standard loading")
            # Fallback to original method if pre-computation fails
        common_cells, cached_hash = load_grid_cells_cache()
        current_hash = compute_file_list_hash()
        
        if common_cells is None or current_hash != cached_hash:
            logging.info("Computing common grid cells...")
            filepaths_list = list(filepaths_map.values())
            logging.info(f"Computing common valid grid cells across {len(filepaths_list)} files...")
            
            try:
                # Since all files have the same grid structure, use just a few representative files
                sample_files = filepaths_list[:3]  # Use first 3 files - all files have identical grids
                logging.info(f"Using {len(sample_files)} representative files for grid cell computation (all files have identical grid structure)...")
                common_cells = find_common_valid_gridcells(sample_files)
                
                if common_cells:
                    grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
                    logging.info(f"Found {len(common_cells)} common valid grid cells")
                    
                    # Save to cache
                    save_grid_cells_cache(common_cells, current_hash)
                    
                    # Set reactive values
                    reactive_grid_feature_ids.set(grid_feature_ids)
                    logging.info(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
                    
                    # Create GeoJSON for map
                    if filepaths_map:
                        sample_file = next(iter(filepaths_map.values()))
                        try:
                            with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                                geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                                reactive_grid_geojson.set(geojson_data)
                                logging.info("STARTUP: Set reactive_grid_geojson")
                        except Exception as e:
                            logging.error(f"Error creating GeoJSON: {e}")
                else:
                    logging.info("No common valid grid cells found")
            except Exception as e:
                logging.error(f"Error computing common grid cells: {e}")
        else:
            logging.info("Using cached grid cells")
            grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
            reactive_grid_feature_ids.set(grid_feature_ids)
            logging.info(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
            
            # Create GeoJSON for map
            if filepaths_map:
                sample_file = next(iter(filepaths_map.values()))
                try:
                    with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                        geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                        reactive_grid_geojson.set(geojson_data)
                        logging.info("STARTUP: Set reactive_grid_geojson")
                except Exception as e:
                    logging.error(f"Error creating GeoJSON from cached cells: {e}")
        
        logging.info("STARTUP: Data loading complete")
    


    # Gridbox selection is now handled by map clicks, not dropdowns

    @reactive.Effect
    @reactive.event(selected_grid_id)
    def _handle_variable_filtering():
        """Handle variable filtering asynchronously when grid changes - OPTIMIZED"""
        try:
            current_grid_id = selected_grid_id.get()
            if not current_grid_id:
                return
                
            logging.debug(f"DEBUG: _handle_variable_filtering - Processing grid {current_grid_id}")
            
            # OPTIMIZED: Use isolation to read cached values without triggering reactivity
            with reactive.isolate():
                is_filtering_complete = variable_filtering_complete.get()
                cached_grid_id = filtered_grid_id.get()
            
            # Skip if already cached for this grid
            if is_filtering_complete and cached_grid_id == current_grid_id:
                logging.debug(f"DEBUG: _handle_variable_filtering - Already cached for grid {current_grid_id}")
                return
            
            # OPTIMIZED: Read filepaths once with isolation to avoid triggering during filtering
            with reactive.isolate():
                filepaths_map = reactive_netcdf_filepaths.get()
            
            if not filepaths_map:
                return
                
            # Get coordinates for the grid ID
            lat, lon = _get_coordinates_from_grid_id(current_grid_id, filepaths_map)
            if lat is None or lon is None:
                logging.debug(f"DEBUG: _handle_variable_filtering - Could not convert grid {current_grid_id} to coordinates")
                return
                
            logging.debug(f"DEBUG: _handle_variable_filtering - Converted grid {current_grid_id} to coordinates ({lat:.4f}, {lon:.4f})")
            
            # Check observational data availability
            observed_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
            valid_obs_vars = []
            
            for obs_var in observed_variables:
                if _check_observational_data_availability(current_grid_id, obs_var, lat, lon):
                    valid_obs_vars.append(obs_var)
                    logging.debug(f"DEBUG: _handle_variable_filtering - Added {obs_var} to available variables for grid {current_grid_id}")
                else:
                    logging.debug(f"DEBUG: _handle_variable_filtering - Excluded {obs_var} from available variables for grid {current_grid_id} - no valid data")
            
            # OPTIMIZED: Batch reactive updates together
            variable_filtering_complete.set(True)
            filtered_grid_id.set(current_grid_id)
            logging.debug(f"DEBUG: _handle_variable_filtering - âœ… FILTERING COMPLETE for grid {current_grid_id}")
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in _handle_variable_filtering: {e}")

    @reactive.Effect
    @reactive.event(multi_plot_grid_id)
    def _handle_multi_plot_variable_filtering():
        """Handle variable filtering asynchronously when multi-plot grid changes - OPTIMIZED"""
        try:
            current_grid_id = multi_plot_grid_id.get()
            if not current_grid_id:
                return
                
            logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Processing grid {current_grid_id}")
            
            # OPTIMIZED: Use isolation to read cached values without triggering reactivity
            with reactive.isolate():
                is_filtering_complete = multi_plot_variable_filtering_complete.get()
                cached_grid_id = multi_plot_filtered_grid_id.get()
            
            # Skip if already cached for this grid
            if is_filtering_complete and cached_grid_id == current_grid_id:
                logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Already cached for grid {current_grid_id}")
                return
            
            # OPTIMIZED: Read filepaths once with isolation to avoid triggering during filtering
            with reactive.isolate():
                filepaths_map = reactive_netcdf_filepaths.get()
            
            if not filepaths_map:
                return
                
            # Get coordinates for the grid ID
            lat, lon = _get_coordinates_from_grid_id(current_grid_id, filepaths_map)
            if lat is None or lon is None:
                logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Could not convert grid {current_grid_id} to coordinates")
                return
                
            logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Converted grid {current_grid_id} to coordinates ({lat:.4f}, {lon:.4f})")
            
            # Check observational data availability
            observed_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
            valid_obs_vars = []
            
            for obs_var in observed_variables:
                if _check_observational_data_availability(current_grid_id, obs_var, lat, lon):
                    valid_obs_vars.append(obs_var)
                    logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Added {obs_var} to available variables for grid {current_grid_id}")
                else:
                    logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Excluded {obs_var} from available variables for grid {current_grid_id} - no valid data")
            
            # OPTIMIZED: Batch all reactive updates together
            multi_plot_variable_filtering_complete.set(True)
            multi_plot_filtered_grid_id.set(current_grid_id)
            logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - âœ… FILTERING COMPLETE for grid {current_grid_id}")
            
            # OPTIMIZED: Trigger UI update efficiently
            try:
                with reactive.isolate():
                    current_trigger = multi_plot_update_trigger.get()
                multi_plot_update_trigger.set(current_trigger + 1)
                logging.debug(f"DEBUG: _handle_multi_plot_variable_filtering - Triggered UI update")
            except Exception as trigger_error:
                logging.debug(f"DEBUG: Error triggering UI update: {trigger_error}")
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in _handle_multi_plot_variable_filtering: {e}")

    @reactive.Effect
    @reactive.event(input.netcdf_model)
    def _update_selected_model():
        model_selection = input.netcdf_model()
        if model_selection and model_selection != "Select a model...":
            logging.debug(f"DEBUG: Model selected: {model_selection}")
            selected_netcdf_model.set(model_selection)
            # FIXED ORDER: Model is now Step 2 in the flow
            dropdown_step_2_completed.set(True)
            logging.debug(f"DEBUG: Step 2 completed")
            # Reset subsequent steps when model changes
            dropdown_step_3_completed.set(False)
            dropdown_step_4_completed.set(False)
            dropdown_step_5_completed.set(False)
            # Trigger plot update when model changes
            plot_update_trigger.set(plot_update_trigger.get() + 1)
        else:
            selected_netcdf_model.set("")
            # Reset Step 2 when model cleared
            dropdown_step_2_completed.set(False)
            logging.debug(f"DEBUG: Step 2 reset")

    @reactive.Effect
    @reactive.event(input.netcdf_variable)
    def _update_selected_variable():
        try:
            # The dropdown returns the VALUE from choices dict, which is the technical variable name
            selected_value = input.netcdf_variable()
            logging.debug(f"DEBUG: _update_selected_variable - Selected value from dropdown: {selected_value}")
            
            if selected_value and selected_value != "" and selected_value != "Select a variable...":
                # The value from the dropdown is already the technical variable name
                # No conversion needed since our choices dict has display names as keys and technical names as values
                technical_var = selected_value
                logging.debug(f"DEBUG: _update_selected_variable - Using technical name: {technical_var}")
                
                # Store the technical variable name
                selected_netcdf_variable.set(technical_var)
                # FIXED ORDER: Variable is now Step 3 in the flow
                dropdown_step_3_completed.set(True)
                logging.debug(f"DEBUG: _update_selected_variable - Step 3 completed, variable: {technical_var}")
                # Reset subsequent steps when variable changes
                dropdown_step_4_completed.set(False)
                dropdown_step_5_completed.set(False)
                # Trigger plot update when variable changes
                plot_update_trigger.set(plot_update_trigger.get() + 1)
            else:
                selected_netcdf_variable.set("")
                dropdown_step_3_completed.set(False)
                logging.debug(f"DEBUG: _update_selected_variable - Step 3 reset")
        except Exception as e:
            logging.debug(f"DEBUG: _update_selected_variable - Error: {e}")
            dropdown_step_3_completed.set(False)

    @output
    @render.ui
    def loop_naming_ui():
        """Show UI for naming a drawn loop in regular mode"""
        try:
            gridboxes = selected_loop_gridboxes.get()
            loop_name = selected_loop_name.get()
            
            # Check if we're NOT in multi-plot mode
            try:
                multi_mode = multi_plot_mode_enabled.get()
            except:
                multi_mode = False
            
            # Only show if MULTIPLE gridboxes are selected (loop) AND name is not yet set
            # Single gridbox selections should not trigger naming UI
            if gridboxes and len(gridboxes) > 1 and not multi_mode and not loop_name:
                logging.debug(f"DEBUG: Showing loop naming UI for {len(gridboxes)} gridboxes (loop detected)")
                return ui.div(
                    ui.div(
                        ui.h4("Name Your Region", style="margin-top: 20px;"),
                        ui.p(f"You've selected {len(gridboxes)} gridbox(es). Please provide a name for this region:"),
                        ui.input_text("loop_name_input", "Region Name:", placeholder="e.g., Downtown, Mountains, Coastal Area"),
                        ui.div(
                            ui.input_action_button("confirm_loop_name", "Confirm", class_="btn btn-primary", style="margin-right: 10px;"),
                            style="margin-top: 15px;"
                        ),
                        ui.div(
                            ui.input_action_button("cancel_loop", "Cancel", class_="btn btn-secondary"),
                            style="margin-top: 10px; margin-bottom: 20px;"
                        ),
                        style="padding: 20px; background-color: #f8f9fa; border: 2px solid #dee2e6; border-radius: 5px;"
                    )
                )
            return ui.div()  # Return empty div if no loop drawn or already named
        except Exception as e:
            logging.error(f"ERROR: Exception in loop_naming_ui: {e}")
            return ui.div()
    
    @output
    @render.ui
    def multi_loop_naming_ui():
        """Show UI for naming a drawn loop in multi-plot mode"""
        try:
            gridboxes = multi_plot_loop_gridboxes.get()
            loop_name = multi_plot_loop_name.get()
            
            # Check if we ARE in multi-plot mode
            try:
                multi_mode = multi_plot_mode_enabled.get()
            except:
                multi_mode = False
            
            logging.debug(f"DEBUG: multi_loop_naming_ui - gridboxes: {len(gridboxes) if gridboxes else 0}, loop_name: {loop_name}, multi_mode: {multi_mode}")
            
            # Only show if MULTIPLE gridboxes are selected (loop) AND name is not yet set
            # Single gridbox selections should not trigger naming UI
            if gridboxes and len(gridboxes) > 1 and multi_mode and not loop_name:
                logging.debug(f"DEBUG: âœ… Rendering multi-plot loop naming UI for {len(gridboxes)} gridboxes")
                return ui.div(
                    ui.div(
                        ui.h4("Name Your Region", style="margin-top: 20px; color: #0056b3;"),
                        ui.p(f"You've selected {len(gridboxes)} gridbox(es). Please provide a name for this region:", style="font-weight: 500;"),
                        ui.input_text("multi_loop_name_input", "Region Name:", value="", placeholder="e.g., Downtown, Mountains, Coastal Area"),
                        ui.div(
                            ui.input_action_button("confirm_multi_loop_name", "Confirm", class_="btn btn-primary", style="margin-right: 10px;"),
                            style="margin-top: 15px;"
                        ),
                        ui.div(
                            ui.input_action_button("cancel_multi_loop", "Cancel", class_="btn btn-secondary"),
                            style="margin-top: 10px; margin-bottom: 20px;"
                        ),
                        style="padding: 25px; background-color: #fff3cd; border: 3px solid #ffc107; border-radius: 8px; margin-bottom: 20px;"
                    ),
                    style="margin-top: 20px;"
                    )
            else:
                logging.debug(f"DEBUG: âŒ NOT showing multi-plot loop naming UI (gridboxes={len(gridboxes) if gridboxes else 0}, loop_name={loop_name}, multi_mode={multi_mode})")
            return ui.div()  # Return empty div if no loop drawn or already named
        except Exception as e:
            logging.error(f"ERROR: Exception in multi_loop_naming_ui: {e}")
            import traceback
            traceback.print_exc()
            return ui.div()
    
    @output
    @render.ui
    def polygon_loading_indicator():
        """Show loading indicator when processing a drawn polygon (regular mode)"""
        try:
            is_processing = polygon_processing.get()
            
            if is_processing:
                logging.debug("DEBUG: ðŸ”„ Showing polygon processing loading indicator (regular mode)")
                return ui.div(
                    ui.div(
                        ui.h5("Processing Polygon...", style="margin-top: 20px; color: #0056b3;"),
                        ui.div(
                            ui.div(
                                class_="spinner-border text-primary",
                                role="status",
                                style="width: 2rem; height: 2rem; margin-right: 15px;"
                            ),
                            ui.p("Analyzing gridboxes and checking data availability. This may take a few seconds for large regions.", 
                                 style="margin: 0; display: inline-block; vertical-align: middle;"),
                            style="display: flex; align-items: center;"
                        ),
                        style="padding: 20px; background-color: #e7f3ff; border: 2px solid #2196F3; border-radius: 8px; margin-top: 15px; margin-bottom: 15px;"
                    )
                )
            else:
                return ui.div()  # Return empty div when not processing
        except Exception as e:
            logging.error(f"ERROR: Exception in polygon_loading_indicator: {e}")
            return ui.div()
    
    @output
    @render.ui
    def polygon_loading_indicator_multi():
        """Show loading indicator when processing a drawn polygon (multi-plot mode)"""
        try:
            is_processing = polygon_processing.get()
            
            if is_processing:
                logging.debug("DEBUG: ðŸ”„ Showing polygon processing loading indicator (multi-plot mode)")
                return ui.div(
                    ui.div(
                        ui.h5("Processing Polygon...", style="margin-top: 20px; color: #0056b3;"),
                        ui.div(
                            ui.div(
                                class_="spinner-border text-primary",
                                role="status",
                                style="width: 2rem; height: 2rem; margin-right: 15px;"
                            ),
                            ui.p("Analyzing gridboxes and checking data availability. This may take a few seconds for large regions.", 
                                 style="margin: 0; display: inline-block; vertical-align: middle;"),
                            style="display: flex; align-items: center;"
                        ),
                        style="padding: 20px; background-color: #e7f3ff; border: 2px solid #2196F3; border-radius: 8px; margin-top: 15px; margin-bottom: 15px;"
                    )
                )
            else:
                return ui.div()  # Return empty div when not processing
        except Exception as e:
            logging.error(f"ERROR: Exception in polygon_loading_indicator_multi: {e}")
            return ui.div()
    
    @reactive.Effect
    @reactive.event(input.confirm_loop_name)
    def _handle_loop_name_confirmation():
        """Handle loop name confirmation for regular mode"""
        loop_name = input.loop_name_input()
        if loop_name and loop_name.strip():
            selected_loop_name.set(loop_name.strip())
            logging.debug(f"DEBUG: Loop named: {loop_name.strip()}")
            # Complete step 1
            dropdown_step_1_completed.set(True)
            logging.debug("DEBUG: Step 1 completed via loop drawing")
            # Close modal
            ui.modal_remove()
        else:
            # Show error - name is required
            logging.debug("DEBUG: Loop name is required")
            multi_plot_status_message.set("âš ï¸ Please provide a name for the region.")
    
    @reactive.Effect
    @reactive.event(input.cancel_loop)
    def _handle_loop_cancel():
        """Handle loop drawing cancellation for regular mode"""
        selected_loop_polygon.set(None)
        selected_loop_gridboxes.set(None)
        selected_loop_name.set(None)
        logging.debug("DEBUG: Loop drawing cancelled")
        ui.modal_remove()
    
    @reactive.Effect
    @reactive.event(input.confirm_multi_loop_name)
    def _handle_multi_loop_name_confirmation():
        """Handle loop name confirmation for multi-plot mode"""
        loop_name = input.multi_loop_name_input()
        if loop_name and loop_name.strip():
            multi_plot_loop_name.set(loop_name.strip())
            logging.debug(f"DEBUG: Multi-plot loop named: {loop_name.strip()}")
            # Complete step 1 for multi-plot
            multi_plot_step_1_completed.set(True)
            logging.debug("DEBUG: Multi-plot Step 1 completed via loop drawing")
            # Close modal
            ui.modal_remove()
        else:
            # Show error - name is required
            logging.debug("DEBUG: Multi-plot loop name is required")
            multi_plot_status_message.set("âš ï¸ Please provide a name for the region.")
    
    @reactive.Effect
    @reactive.event(input.cancel_multi_loop)
    def _handle_multi_loop_cancel():
        """Handle loop drawing cancellation for multi-plot mode"""
        multi_plot_loop_polygon.set(None)
        multi_plot_loop_gridboxes.set(None)
        multi_plot_loop_name.set(None)
        logging.debug("DEBUG: Multi-plot loop drawing cancelled")
        ui.modal_remove()

    @reactive.Effect
    @reactive.event(input.netcdf_frequency)
    def _update_selected_frequency():
        frequency_selection = input.netcdf_frequency()
        if frequency_selection and frequency_selection != "Select frequency...":
            logging.debug(f"DEBUG: Frequency selected: {frequency_selection}")
            selected_netcdf_frequency.set(frequency_selection)
            dropdown_step_4_completed.set(True)
            logging.debug(f"DEBUG: Step 4 completed")
            # If observational variable, skip scenario and complete steps 5 and 6; also set slider years
            try:
                sel_var = selected_netcdf_variable.get()
            except Exception:
                sel_var = None
            if sel_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                dropdown_step_5_completed.set(True)
                dropdown_step_6_completed.set(True)
                # Observational range 1950â€“2013
                ui.update_slider("time_series_range_main", min=1950, max=2013, value=(1950, 2013))
            else:
                # Reset subsequent steps when frequency changes
                dropdown_step_5_completed.set(False)
            # Trigger plot update when frequency changes
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            # Ensure main slider reflects observed range if an observed variable is selected
            try:
                sel_var = selected_netcdf_variable.get()
                if sel_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                    ui.update_slider("time_series_range_main", min=1950, max=2013, value=(1950, 2013))
            except Exception:
                pass

    @reactive.Effect
    @reactive.event(input.netcdf_ssp)
    def _update_selected_scenario():
        ssp_selection = input.netcdf_ssp()
        if ssp_selection and ssp_selection != "Select scenario...":
            logging.debug(f"DEBUG: SSP selected: {ssp_selection}")
            selected_netcdf_ssp.set(ssp_selection)
            dropdown_step_5_completed.set(True)
            # Automatically complete step 6 when step 5 is done
            dropdown_step_6_completed.set(True)
            logging.debug(f"DEBUG: Steps 4 and 5 completed")
            
            # Don't auto-select grid cell - let user choose explicitly
            current_grid_id = selected_grid_id.get()
            logging.debug(f"DEBUG: Current grid ID: {current_grid_id}")
            if current_grid_id is None:
                logging.debug(f"DEBUG: No grid selected - user must select explicitly")
            else:
                logging.debug(f"DEBUG: Grid ID already selected: {current_grid_id}")
            
            # Update time series range slider based on SSP
            ssp_value = input.netcdf_ssp()
            if ssp_value == "historical":
                ui.update_slider("time_series_range_main", min=1950, max=2014, value=(1950, 2014))
            else:
                ui.update_slider("time_series_range_main", min=2015, max=2100, value=(2015, 2100))
            logging.debug(f"DEBUG: Updated time series range slider for SSP: {ssp_value}")
            
            # Trigger plot update
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            logging.debug(f"DEBUG: Plot update trigger incremented")

    # Debounced time series range update
    _last_time_series_update = reactive.Value(time.time())
    
    @reactive.Effect
    @reactive.event(input.time_series_range)
    def _update_time_series_range():
        if input.time_series_range():
            # Update the last update time
            _last_time_series_update.set(time.time())
    
    @reactive.Effect
    @reactive.event(_last_time_series_update)
    def _debounced_time_series_update():
        # OPTIMIZED: Reduced wait time from 0.05 to 0.01 seconds for faster responsiveness
        time.sleep(0.01)
        # Check if this is still the most recent update
        if time.time() - _last_time_series_update.get() >= 0.01:
            plot_update_trigger.set(plot_update_trigger.get() + 1)
    
    # Cache for processed time series data to avoid reprocessing
    _time_series_cache = {}
    
    # Cache for observational gridbox mappings to avoid expensive recomputation
    # Key: (proj_lat, proj_lon) rounded to 4 decimals
    # Value: (obs_lat_idx, obs_lon_idx)
    _obs_gridbox_mapping_cache = {}
    
    # Note: Regional timeseries cache is now global (_REGIONAL_TIMESERIES_CACHE)
    # to be accessible by _extract_regional_average_timeseries function
    
    # Note: Slider and checkbox reactivity is handled automatically by Shiny
    # When the plot function reads slider/checkbox values, it establishes reactive dependencies
    # No separate monitoring needed - this prevents cascading updates and errors





    # Trendline checkboxes are handled directly in the plot rendering logic
    # The plot function reads checkbox values when it renders, establishing natural reactivity
    # No separate monitoring needed - Shiny's reactive system handles this automatically

    @reactive.Effect
    @reactive.event(input.reset_selection)
    def _reset_progressive_selection():
        # Reset all step completion flags
        dropdown_step_1_completed.set(False)
        dropdown_step_2_completed.set(False)
        dropdown_step_3_completed.set(False)
        dropdown_step_4_completed.set(False)
        dropdown_step_5_completed.set(False)
        dropdown_step_6_completed.set(False)
        # Clear selected values
        logging.debug(f"DEBUG: Clearing selected_grid_id (was: {selected_grid_id.get()})")
        selected_grid_id.set(None)
        selected_netcdf_variable.set(None)
        selected_netcdf_model.set(None)
        selected_netcdf_frequency.set(None)
        selected_netcdf_ssp.set(None)
        
        # Clear loop data for main selection
        selected_loop_polygon.set(None)
        selected_loop_gridboxes.set(None)
        selected_loop_name.set(None)
        selected_loop_available_variables.set(None)
        logging.debug("DEBUG: Cleared main selection loop data")
        
        # Also reset multi-plot selections when starting new selection
        multi_plot_step_1_completed.set(False)
        multi_plot_step_2_completed.set(False)
        multi_plot_step_3_completed.set(False)
        multi_plot_step_4_completed.set(False)
        multi_plot_step_5_completed.set(False)
        multi_plot_step_6_completed.set(False)
        # Clear multi-plot selected values
        multi_plot_grid_id.set(None)
        multi_plot_variable.set(None)
        multi_plot_model.set(None)
        multi_plot_frequency.set(None)
        multi_plot_ssp.set(None)
        multi_plot_action.set(None)
        
        # Clear loop data for multi-plot selection
        multi_plot_loop_polygon.set(None)
        multi_plot_loop_gridboxes.set(None)
        multi_plot_loop_name.set(None)
        multi_plot_loop_available_variables.set(None)
        logging.debug("DEBUG: Cleared multi-plot loop data")
        
        # Disable multi-plot mode to ensure we're in Start New Selection mode
        multi_plot_mode_enabled.set(False)
        
        # CRITICAL FIX: Also uncheck the UI checkbox so map clicks work correctly
        ui.update_checkbox("multi_plot_mode", value=False)
        logging.debug("DEBUG: Unchecked Additional Traces checkbox - ensuring main mode")
        
        # NOTE: Map no longer needs to be reset - it stays visible and interactive permanently
        logging.debug("DEBUG: Reset complete - map remains interactive")
        
        # IMPORTANT: Reset all UI dropdown inputs to blank/placeholder values
        # Main plot dropdowns
        ui.update_select("netcdf_model", selected="")
        ui.update_select("netcdf_variable", selected="")
        ui.update_select("netcdf_frequency", selected="")
        ui.update_select("netcdf_ssp", selected="")
        ui.update_select("gridbox_selector", selected="")
        
        # Multi-plot dropdowns
        ui.update_select("multi_plot_gridbox_selector", selected="")
        ui.update_select("multi_plot_model", selected="")
        ui.update_select("multi_plot_variable", selected="")
        ui.update_select("multi_plot_frequency", selected="")
        ui.update_select("multi_plot_ssp", selected="")
        
        # Force UI update by triggering updates
        try:
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
            logging.debug("DEBUG: Triggered UI updates and reset all dropdown selections to blank")
        except Exception as e:
            logging.debug(f"DEBUG: Warning - Could not trigger UI updates: {e}")
        
        # Clear additional traces when resetting selection
        main_plot_additional_traces.set([])
        main_plot_persistent_traces.set([])
        
        # Clear multi-plot list
        multi_plots_list.set([])
        
        # Reset multi-plot update trigger
        multi_plot_update_trigger.set(0)
        
        # Reset plot update trigger to force re-render
        plot_update_trigger.set(plot_update_trigger.get() + 1)
        
        logging.debug("DEBUG: All selections reset (main and multi-plot), additional traces cleared")
    


    @reactive.Effect
    @reactive.event(plot_update_trigger)
    def _trigger_plot_update():
        """Trigger plot re-render when data is loaded"""
        logging.debug(f"DEBUG: _trigger_plot_update called - trigger value: {plot_update_trigger.get()}")
        pass

    @reactive.Effect
    @reactive.event(dropdown_step_5_completed)
    def _debug_step_5_completion():
        """Debug effect to test if step 5 completion triggers"""
        logging.debug(f"DEBUG: _debug_step_5_completion called - step 5 completed: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(plot_update_trigger)
    def _debug_plot_trigger():
        """Debug effect to test if plot_update_trigger is working"""
        logging.debug(f"DEBUG: _debug_plot_trigger called - trigger value: {plot_update_trigger.get()}")
        logging.debug(f"DEBUG: Current step completion status:")
        logging.debug(f"DEBUG:   Step 1: {dropdown_step_1_completed.get()}")
        logging.debug(f"DEBUG:   Step 2: {dropdown_step_2_completed.get()}")
        logging.debug(f"DEBUG:   Step 3: {dropdown_step_3_completed.get()}")
        logging.debug(f"DEBUG:   Step 4: {dropdown_step_4_completed.get()}")
        logging.debug(f"DEBUG:   Step 5: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(dropdown_step_1_completed)
    def _trigger_ui_update_on_step1_completion():
        """Trigger UI update when step 1 is completed"""
        logging.debug(f"DEBUG: Step 1 completion changed to: {dropdown_step_1_completed.get()}")
        if dropdown_step_1_completed.get():
            logging.debug(f"DEBUG: Step 1 completed - triggering UI update")
            try:
                plot_update_trigger.set(plot_update_trigger.get() + 1)
            except Exception as e:
                logging.debug(f"DEBUG: Error triggering UI update: {e}")
        
    @reactive.Effect
    @reactive.event(dropdown_step_5_completed)
    def _debug_step5_trigger():
        """Debug effect to test if dropdown_step_5_completed triggers"""
        logging.debug(f"DEBUG: _debug_step5_trigger called - step5 value: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(input.netcdf_model, input.netcdf_variable, input.netcdf_ssp)
    def _debug_input_changes():
        """Debug effect to test if input changes are being detected"""
        logging.debug(f"DEBUG: _debug_input_changes called")
        logging.debug(f"DEBUG: netcdf_model: {input.netcdf_model()}")
        logging.debug(f"DEBUG: netcdf_variable: {input.netcdf_variable()}")
        logging.debug(f"DEBUG: netcdf_ssp: {input.netcdf_ssp()}")



    # Multi-plot gridbox selection is now handled by map clicks, not dropdowns

    @reactive.Effect
    @reactive.event(input.multi_plot_model)
    def _update_multi_plot_selected_model():
        try:
            model_value = input.multi_plot_model()
            logging.debug(f"DEBUG: Multi-plot model changed to: {model_value}")
            if model_value and model_value != "Select a model..." and model_value != "":
                multi_plot_model.set(model_value)
                # FIXED ORDER: Model is now Step 2
                multi_plot_step_2_completed.set(True)
                # Reset subsequent steps when model changes
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                # FIXED: Don't trigger plot update immediately when model changes
                # This prevents losing existing traces during selection changes
                logging.debug(f"DEBUG: Multi-plot step 2 completed, but plot update deferred until selection completion")
            else:
                multi_plot_model.set("")
                # Reset Step 2 when model cleared
                multi_plot_step_2_completed.set(False)
                logging.debug(f"DEBUG: Multi-plot step 2 reset")
        except Exception as e:
            logging.debug(f"DEBUG: Error in _update_multi_plot_selected_model: {e}")
            # Gracefully handle error by resetting
            multi_plot_model.set("")
            multi_plot_step_2_completed.set(False)
    @reactive.Effect
    @reactive.event(input.multi_plot_variable)
    def _update_multi_plot_selected_variable():
        try:
            # The dropdown returns the VALUE from choices dict, which is the technical variable name
            selected_value = input.multi_plot_variable()
            logging.debug(f"DEBUG: _update_multi_plot_selected_variable - Selected value from dropdown: {selected_value}")
            
            if selected_value and selected_value != "" and selected_value != "Select a variable...":
                # The value from the dropdown is already the technical variable name
                # No conversion needed since our choices dict has display names as keys and technical names as values
                technical_var = selected_value
                logging.debug(f"DEBUG: _update_multi_plot_selected_variable - Using technical name: {technical_var}")
                
                # Store the technical variable name
                multi_plot_variable.set(technical_var)
                # FIXED ORDER: Variable is now Step 3
                multi_plot_step_3_completed.set(True)
                logging.debug(f"DEBUG: _update_multi_plot_selected_variable - Step 3 completed, variable: {technical_var}")
                # Reset subsequent steps when variable changes
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                # FIXED: Don't trigger plot update immediately when variable changes
                # This prevents losing existing traces during selection changes
            else:
                multi_plot_variable.set("")
                # Reset Step 3 when variable cleared
                multi_plot_step_3_completed.set(False)
                logging.debug(f"DEBUG: _update_multi_plot_selected_variable - Step 3 reset")
        except Exception as e:
            logging.debug(f"DEBUG: _update_multi_plot_selected_variable - Error: {e}")
            multi_plot_step_3_completed.set(False)

    @reactive.Effect
    @reactive.event(input.multi_plot_frequency)
    def _update_multi_plot_selected_frequency():
        frequency_value = input.multi_plot_frequency()
        logging.debug(f"DEBUG: Multi-plot frequency changed to: {frequency_value}")
        if frequency_value and frequency_value != "Select frequency...":
            multi_plot_frequency.set(frequency_value)
            multi_plot_step_4_completed.set(True)
            logging.debug(f"DEBUG: Multi-plot step 4 completed")
            
            # For observed variables, automatically complete scenario step (Step 5)
            var = multi_plot_variable.get()
            if var and var.startswith('obs_'):
                multi_plot_ssp.set('historical')
                multi_plot_step_5_completed.set(True)
                multi_plot_step_6_completed.set(True)
                logging.debug(f"DEBUG: Observed variable selected, auto-completing scenario step (Step 5)")
            else:
                # Reset subsequent steps when frequency changes for non-observed variables
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
            
            # FIXED: Don't trigger plot update immediately when frequency changes
            # This prevents losing existing traces during selection changes
            logging.debug(f"DEBUG: Multi-plot step 4 completed, but plot update deferred until selection completion")
    @reactive.Effect
    @reactive.event(multi_plot_variable, multi_plot_frequency)
    def _auto_complete_observed_variables():
        """Auto-complete steps 5 and 6 for observed variables when frequency is selected"""
        var = multi_plot_variable.get()
        freq = multi_plot_frequency.get()
        
        if var and var.startswith('obs_') and freq and not multi_plot_step_5_completed.get():
            logging.debug(f"DEBUG: Auto-completing observed variable steps: {var}")
            multi_plot_ssp.set('historical')
            multi_plot_step_5_completed.set(True)
            multi_plot_step_6_completed.set(True)
            logging.debug(f"DEBUG: Observed variable steps 5 and 6 auto-completed")

    @reactive.Effect
    @reactive.event(input.multi_plot_ssp)
    def _update_multi_plot_selected_scenario():
        ssp_value = input.multi_plot_ssp()
        logging.debug(f"DEBUG: Multi-plot SSP changed to: {ssp_value}")
        if ssp_value and ssp_value != "Select scenario...":
            multi_plot_ssp.set(ssp_value)
            multi_plot_step_5_completed.set(True)
            # Automatically complete step 6 when step 5 is done
            multi_plot_step_6_completed.set(True)
            
            # FIXED: Don't trigger plot update immediately when SSP changes
            # This prevents losing existing traces during selection changes
            # The plot will update when the user completes the selection
            logging.debug(f"DEBUG: Multi-plot steps 5 and 6 completed, but plot update deferred until selection completion")

    @reactive.Effect
    @reactive.event(input.multi_plot_action)
    def _update_multi_plot_action():
        action_value = input.multi_plot_action()
        logging.debug(f"DEBUG: Multi-plot action changed to: {action_value}")
        if action_value:
            multi_plot_action.set(action_value)
            # Trigger plot update when action is selected
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            logging.debug(f"DEBUG: Multi-plot action set, triggered plot update")



    @reactive.Effect
    @reactive.event(input.multi_plot_mode)
    def _handle_multi_plot_mode_toggle():
        """Handle multi-plot mode toggle"""
        try:
            multi_mode = input.multi_plot_mode()
            logging.debug(f"DEBUG: Multi-plot mode toggled: {multi_mode}")
            
            # Update the reactive value to match the input
            multi_plot_mode_enabled.set(multi_mode)
            logging.debug(f"DEBUG: Multi-plot mode reactive value updated to: {multi_mode}")
            
            # Handle the toggle logic
            if multi_mode:
                # When enabling multi-plot mode, just enable it without resetting anything
                logging.debug("DEBUG: Enabling Additional Traces mode - keeping all selections")
                
                # CRITICAL: Reset multi-plot step flags to allow map clicks at step 1
                # This ensures the map click handler will work immediately
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                
                # Clear multi-plot grid_id to allow new selection
                multi_plot_grid_id.set(None)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                
                logging.debug("DEBUG: Reset multi-plot steps to allow new gridbox selection via map click")
                
                # Reset multi-plot dropdowns to blank state
                ui.update_select("multi_plot_gridbox_selector", selected="")
                ui.update_select("multi_plot_model", selected="")
                ui.update_select("multi_plot_variable", selected="")
                ui.update_select("multi_plot_frequency", selected="")
                ui.update_select("multi_plot_ssp", selected="")
                logging.debug("DEBUG: Reset multi-plot dropdown UI to blank state")
                
                # Check if there are already additional traces
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent = list(main_plot_persistent_traces.get() or [])
                total_traces = len(current_traces) + len(persistent)
                
                if total_traces > 0:
                    # If there are already traces, just enable multi-plot mode
                    logging.debug(f"DEBUG: Additional traces mode enabled - {total_traces} existing traces preserved")
                    multi_plot_status_message.set(f"âœ… Additional traces mode enabled - {total_traces} existing traces preserved")
                else:
                    # No existing traces - ready for new selections
                    multi_plot_status_message.set("ðŸ”„ Additional traces mode enabled - ready for new selections")
            else:
                # When disabling multi-plot mode, clear ALL additional trace data
                logging.debug("DEBUG: Disabling Additional Traces mode - clearing all additional traces")
                
                # FIXED: Clear additional traces when disabling mode
                main_plot_additional_traces.set([])
                main_plot_persistent_traces.set([])
                
                # Clear multi-plot specific data
                multi_plots_list.set([])
                multi_plot_time_ranges.set({})
                current_multi_plot_id.set(0)
                
                # Reset multi-plot step completion flags
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear selected values
                multi_plot_grid_id.set(None)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                # IMPORTANT: Reset the actual UI dropdown inputs to blank/placeholder values
                ui.update_select("multi_plot_gridbox_selector", selected="")
                ui.update_select("multi_plot_model", selected="")
                ui.update_select("multi_plot_variable", selected="")
                ui.update_select("multi_plot_frequency", selected="")
                ui.update_select("multi_plot_ssp", selected="")
                
                multi_plot_status_message.set("ðŸ”„ Start New Selection mode - additional traces cleared")
                logging.debug("DEBUG: Reset all multi-plot UI dropdowns when disabling Additional Traces mode")
                
                # FIXED: Trigger a plot update to remove additional traces from display
                plot_update_trigger.set(plot_update_trigger.get() + 1)
        
            # FIXED: Only trigger UI update for multi-plot dropdown visibility, not plot updates when enabling
            if multi_mode:
                # When enabling, just update UI without triggering plot refresh
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
            
            logging.debug(f"DEBUG: Multi-plot mode toggle complete")
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in _handle_multi_plot_mode_toggle: {e}")

    @reactive.Calc
    def get_plot_selections():
        """Reactive calculation that returns the current plot selections"""
        variable = selected_netcdf_variable.get()
        model = selected_netcdf_model.get()
        frequency = selected_netcdf_frequency.get()
        ssp = selected_netcdf_ssp.get()
        grid_indices = get_selected_grid_indices()
        
        # Check for loop data
        loop_gridboxes = selected_loop_gridboxes.get()
        loop_name = selected_loop_name.get()
        loop_polygon = selected_loop_polygon.get()
        
        logging.debug(f"DEBUG: get_plot_selections - Variable: {variable}, Model: {model}, Frequency: {frequency}, SSP: {ssp}, Grid: {grid_indices}")
        if loop_gridboxes:
            logging.debug(f"DEBUG: get_plot_selections - Loop detected: {loop_name} with {len(loop_gridboxes)} gridboxes")
        
        return {
            'variable': variable,
            'model': model,
            'frequency': frequency,
            'ssp': ssp,
            'grid_indices': grid_indices,
            'loop_gridboxes': loop_gridboxes,
            'loop_name': loop_name,
            'loop_polygon': loop_polygon
        }
    @reactive.Effect
    @reactive.event(dropdown_step_1_completed, dropdown_step_2_completed, dropdown_step_3_completed, dropdown_step_4_completed, dropdown_step_5_completed, dropdown_step_6_completed)
    def _trigger_main_plot_generation():
        """Trigger main plot generation when all steps are completed"""
        try:
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            all_steps_completed = step1_completed and step2_completed and step3_completed and step4_completed and step5_completed and step6_completed
            
            logging.debug(f"DEBUG: _trigger_main_plot_generation - all_steps_completed: {all_steps_completed}")
            
            if all_steps_completed:
                # Trigger plot update
                plot_update_trigger.set(plot_update_trigger.get() + 1)
                logging.debug(f"DEBUG: Triggered plot update - new trigger value: {plot_update_trigger.get()}")
        except Exception as e:
            logging.debug(f"DEBUG: Error in _trigger_main_plot_generation: {e}")

    @reactive.Effect
    @reactive.event(dropdown_step_1_completed, dropdown_step_2_completed, dropdown_step_3_completed, dropdown_step_4_completed, dropdown_step_5_completed, dropdown_step_6_completed)
    async def _update_progressive_dropdown_classes():
        """Update CSS classes for progressive dropdown steps with error handling"""
        try:
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            # Determine which step is currently active
            active_step = 1
            if step1_completed and not step2_completed:
                active_step = 2
            elif step2_completed and not step3_completed:
                active_step = 3
            elif step3_completed and not step4_completed:
                active_step = 4
            elif step4_completed and not step5_completed:
                active_step = 5
            elif step5_completed and not step6_completed:
                active_step = 6
            elif step6_completed:
                active_step = 7
            
            logging.debug(f"DEBUG: Progressive dropdown classes - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, active: {active_step}")
            
            # Send custom message to update CSS classes via JavaScript with error handling
            try:
                await session.send_custom_message("update_progressive_classes", {
                    "step1_completed": step1_completed,
                    "step2_completed": step2_completed,
                    "step3_completed": step3_completed,
                    "step4_completed": step4_completed,
                    "step5_completed": step5_completed,
                    "step6_completed": step6_completed,
                    "active_step": active_step
                })
            except Exception as msg_error:
                logging.debug(f"DEBUG: Non-critical error sending custom message: {msg_error}")
        except Exception as e:
            logging.debug(f"DEBUG: Error in _update_progressive_dropdown_classes: {e}")

    @reactive.Effect
    @reactive.event(multi_plot_grid_id)
    def _reset_multi_plot_steps_on_grid_change():
        """Reset multi-plot steps 2-6 when grid selection changes"""
        try:
            grid_id = multi_plot_grid_id.get()
            if grid_id:
                logging.debug(f"DEBUG: Multi-plot grid changed to {grid_id} - resetting steps 2-6")
                # Minimal delay for faster response
                time.sleep(0.05)
                # Keep step 1 completed, but reset subsequent steps
                # Don't reset step 1 since the grid was just selected
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear subsequent selections
                multi_plot_model.set(None)
                multi_plot_variable.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                # UI update will be triggered automatically by step completion changes (debounced)
                logging.debug(f"DEBUG: Multi-plot steps reset, ready for Step 2 (UI will update via debounced trigger)")
        except Exception as e:
            logging.debug(f"DEBUG: Error in _reset_multi_plot_steps_on_grid_change: {e}")
    
    # Debounce mechanism for multi-plot UI updates
    _last_multi_plot_ui_update = reactive.Value(time.time())
    
    @reactive.Effect
    @reactive.event(multi_plot_step_1_completed, multi_plot_step_2_completed, multi_plot_step_3_completed, multi_plot_step_4_completed, multi_plot_step_5_completed, multi_plot_step_6_completed)
    def _trigger_multi_plot_ui_update():
        """Trigger multi-plot UI update when steps change (debounced)"""
        logging.debug(f"DEBUG: Multi-plot step changed - scheduling UI update")
        logging.debug(f"DEBUG: Multi-plot steps: {multi_plot_step_1_completed.get()}, {multi_plot_step_2_completed.get()}, {multi_plot_step_3_completed.get()}, {multi_plot_step_4_completed.get()}, {multi_plot_step_5_completed.get()}, {multi_plot_step_6_completed.get()}")
        # Update the last update time
        _last_multi_plot_ui_update.set(time.time())
    
    @reactive.Effect
    @reactive.event(_last_multi_plot_ui_update)
    def _debounced_multi_plot_ui_update():
        """Debounced UI update to prevent race conditions - OPTIMIZED"""
        # OPTIMIZED: Reduced wait time from 0.05 to 0.01 seconds for faster UI updates
        time.sleep(0.01)
        # Check if this is still the most recent update
        if time.time() - _last_multi_plot_ui_update.get() >= 0.01:
            # Trigger multi-plot UI update
            multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)





    @output
    @render.ui
    def progressive_dropdown_ui():
        """Render progressive dropdown steps - only one visible at a time"""
        logging.debug("DEBUG: progressive_dropdown_ui - FUNCTION CALLED")
        try:
            logging.debug("DEBUG: progressive_dropdown_ui - starting function")
            # Only access the essential reactive values to minimize re-renders
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            # Access other reactive values only when needed
            filepaths_map = reactive_netcdf_filepaths.get()
            grid_feature_ids = reactive_grid_feature_ids.get()
            current_grid_id = selected_grid_id.get()
            
            logging.debug(f"DEBUG: Step completion status - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, step6: {step6_completed}")
            
            # Reduced logging for performance
            
            # Check if data is still loading
            if not filepaths_map or not grid_feature_ids:
                logging.debug("DEBUG: Data not loaded yet, showing loading state")
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div(
                        ui.div("Loading...", class_="step-title"),
                        ui.div("Please wait while the climate data is being loaded.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                )
            
            logging.debug("DEBUG: Data loaded, checking step completion")
            
            # Extract available options from filepaths_map
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            
            # If model is selected, filter variables and scenarios
            selected_model = selected_netcdf_model.get()
            if selected_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == selected_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model])

            
            # If variable is selected, filter scenarios further
            selected_variable = selected_netcdf_variable.get()
            if selected_variable and selected_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model and var == selected_variable])
            
            # Prepare choices for dropdowns
            gridbox_choices = {"Select a grid cell...": ""}
            if grid_feature_ids and len(grid_feature_ids) > 0:
                # Use actual grid IDs as both keys and display text, with blank initial option
                for fid in grid_feature_ids:
                    gridbox_choices[fid] = fid
            else:
                # Comprehensive fallback choices with actual grid IDs - include more grid cells
                # Generate a comprehensive list of grid cells
                for lat in range(7, 43):  # Based on the logs showing grid cells up to 42
                    for lon in range(3, 42):  # Based on the logs showing grid cells up to 41
                        grid_id = f"{lat},{lon}"
                        gridbox_choices[grid_id] = grid_id
            
            # Get all available options
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            
            # If model is selected, filter variables and scenarios
            selected_model = selected_netcdf_model.get()
            if selected_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == selected_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model])

            
            # If variable is selected, filter scenarios further
            selected_variable = selected_netcdf_variable.get()
            if selected_variable and selected_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model and var == selected_variable])

            
            # Determine which step to show based on completion status
            if not step1_completed:
                # Step 1: Gridbox selection dropdown
                grid_options = {"Select a gridbox...": ""}
                for grid_id in grid_feature_ids:
                    grid_options[f"Grid {grid_id}"] = grid_id
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 1: Select Grid Location", class_="step-title"),
                    ui.div("Select a gridbox from the dropdown below, click a gridbox on the map, or draw a closed loop over gridboxes to analyze regional average data:", class_="step-explanation"),
                    ui.input_select(
                        "gridbox_selector",
                        "Select Gridbox:",
                        choices=grid_options,
                        selected=selected_grid_id.get() if selected_grid_id.get() else ""
                    ),
                    class_="dropdown-step active"
                )
            elif step1_completed and not step2_completed:
                # FIXED ORDER: Step 2 is now Model/Data Type selection
                # Model choices in Shiny format: {value: label}
                model_choices = {"": "Select a model..."}
                for model in available_models:
                    model_choices[model] = model
                
                # Conditionally add observed data option based on whether this is a loop or gridbox
                # Check if we're in a loop scenario
                loop_available_vars = selected_loop_available_variables.get()
                if loop_available_vars:
                    # For loops: only add "Observed" if at least one obs variable is available across ALL gridboxes
                    obs_vars_available = any(v in loop_available_vars for v in ['obs_tmax', 'obs_tmin', 'obs_pr'])
                    if obs_vars_available:
                        model_choices["Observed"] = "Observational Data"
                        logging.debug(f"DEBUG: Main step 2 - Observed data available for loop")
                    else:
                        logging.debug(f"DEBUG: Main step 2 - Observed data NOT available for loop (filtered by loop)")
                else:
                    # For single gridboxes: always show "Observed" option - all data available for individual gridboxes
                    model_choices["Observed"] = "Observational Data"
                    logging.debug(f"DEBUG: Main step 2 - Showing all models for individual gridbox (no loop filtering)")
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 2: Select Data Type", class_="step-title"),
                    ui.input_select("netcdf_model", "Select data type:", choices=model_choices, selected=""),
                    ui.div("Choose whether to use observational data or climate model projections for your analysis.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step2_completed and not step3_completed:
                # FIXED ORDER: Step 3 is now Variable selection (after model selection)
                # Get the selected model to filter variables
                current_model = selected_netcdf_model.get()
                logging.debug(f"DEBUG: progressive_dropdown_ui - current_model: {current_model}")
                
                # Filter variables based on model selection
                if current_model == "Observed":
                    # For observational data, only show observed variables
                    filtered_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
                    logging.debug(f"DEBUG: Main step 3 - Using observed variables: {filtered_variables}")
                else:
                    # For model data, filter by selected model
                    if current_model:
                            # For specific models, filter by model name
                            model_filtered_variables = sorted(set(var for (var, model, ssp) in filepaths_map.keys() if model == current_model))
                            filtered_variables = model_filtered_variables
                            logging.debug(f"DEBUG: Main step 3 - Using model-filtered variables for {current_model}: {model_filtered_variables}")
                    else:
                        # If no model selected yet, use all variables
                        filtered_variables = list(available_variables)
                        logging.debug(f"DEBUG: Main step 3 - No model selected, using all variables: {available_variables}")
                
                # CONDITIONAL FILTER: If this is a loop, only show variables available across ALL gridboxes
                # For individual gridboxes, show all available variables
                loop_available_vars = selected_loop_available_variables.get()
                if loop_available_vars:
                    # Filter to only include variables that are available in the loop
                    filtered_variables = [v for v in filtered_variables if v in loop_available_vars]
                    logging.debug(f"DEBUG: Main variables filtered by loop availability: {sorted(filtered_variables)}")
                    if not filtered_variables:
                        logging.debug(f"DEBUG: âš ï¸ WARNING: No variables available after loop filtering!")
                else:
                    # For individual gridboxes, show all available variables for the selected model
                    logging.debug(f"DEBUG: Main step 3 - Showing all {len(filtered_variables)} variables for individual gridbox (no loop filtering)")
                
                variable_choices = create_variable_choices(sorted(filtered_variables))
                
                # Get the current selected variable (technical name)
                current_variable = selected_netcdf_variable.get()
                # For Shiny, the selected value should match a choice key (technical name)
                selected_value = current_variable if current_variable and current_variable in variable_choices else ""
                
                # Update step explanation based on whether this is a loop or gridbox
                if loop_available_vars:
                    step_explanation = f"Choose which climate variable. Only showing {len(filtered_variables)} variable(s) with valid data across ALL gridboxes in the loop."
                else:
                    step_explanation = "Choose which climate variable you want to analyze. All variables for the selected data type are shown."
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 3: Choose Climate Variable", class_="step-title"),
                    ui.input_select("netcdf_variable", "Select climate variable:", choices=variable_choices, selected=selected_value),
                    ui.div(step_explanation, class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step3_completed and not step4_completed:
                # Frequency choices in Shiny format: {value: label}
                frequency_choices = {"": "Select frequency...", "daily": "Daily", "monthly": "Monthly", "annual": "Annual"}
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 4: Choose Frequency", class_="step-title"),
                    ui.input_select("netcdf_frequency", "Select data frequency:", choices=frequency_choices, selected=""),
                    ui.div("Choose the frequency of data to display: daily (raw data), monthly (averaged), or annual (averaged).", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step4_completed and not step5_completed:
                # Step 5: Scenario (skipped for observed)
                sel_var = selected_netcdf_variable.get()
                is_observed = sel_var in ("obs_tmin","obs_tmax","obs_pr")
                if is_observed:
                    return ui.div(
                        ui.h4("Step-by-Step Data Selection", class_="text-center"),
                        ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                        ui.div("Step 5: Scenario (skipped for observations)", class_="step-title"),
                        ui.div("Observational datasets do not require an SSP scenario.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                else:
                    scenario_choices = {"Select scenario...": ""}
                    for scenario in available_scenarios:
                        scenario_choices[scenario] = scenario
                    return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 5: Choose Scenario", class_="step-title"),
                    ui.input_select("netcdf_ssp", "Select SSP scenario:", choices=scenario_choices, selected=""),
                    ui.div("Select the Shared Socioeconomic Pathway (SSP) scenario that represents different future emission trajectories.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step5_completed and not step6_completed:
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 6: Generate Plot", class_="step-title"),
                    ui.div("All selections complete! The climate data plot will appear below. You can then adjust the time series range using the slider that will appear.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            else:
                # All steps completed - keep UI open to allow more additions; no completion banner
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Ready to add more or adjust selections.", class_="step-explanation"),
                    ui.div(
                        ui.input_action_button("reset_selection", "Start New Selection", class_="btn btn-primary"),
                        class_="mt-3 text-center"
                    ),
                    class_="dropdown-step active"
                )
        except Exception as e:
            logging.debug(f"DEBUG: Error in progressive_dropdown_ui: {e}")
            logging.debug(f"DEBUG: Full traceback: {traceback.format_exc()}")
            logging.debug(f"DEBUG: Exception type: {type(e)}")
            # Create comprehensive fallback choices
            fallback_choices = {'': 'Select a grid cell...'}
            for lat in range(7, 43):
                for lon in range(3, 42):
                    grid_id = f"{lat},{lon}"
                    fallback_choices[grid_id] = grid_id
            logging.debug(f"DEBUG: Created {len(fallback_choices)} fallback choices for exception handler")
            return ui.div(
                ui.h4("Step-by-Step Data Selection", class_="text-center"),
                ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                ui.div("Step 1: Select Grid Location (Fallback)", class_="step-title"),
                ui.input_select("gridbox_selector", "Choose a grid cell:", choices=fallback_choices, selected=selected_grid_id.get() if selected_grid_id.get() else ""),
                ui.div("Select a specific grid cell on the map to analyze climate data for that location.", class_="step-explanation"),
                class_="dropdown-step active"
            )

    @output
    @render.ui
    def multi_plot_checkbox_ui():
        """Render the Additional Traces Mode checkbox with reactive state"""
        try:
            current_state = multi_plot_mode_enabled.get()
            return ui.input_checkbox("multi_plot_mode", "Enable Additional Traces Mode", value=current_state)
        except Exception as e:
            logging.debug(f"DEBUG: Error rendering multi_plot_checkbox_ui: {e}")
            return ui.input_checkbox("multi_plot_mode", "Enable Additional Traces Mode", value=False)

    @output
    @render.ui
    def multi_plot_progressive_dropdown_ui():
        """Render progressive dropdown steps for multi-plot mode"""
        import time
        start_time = time.time()
        logging.debug(f"DEBUG: â±ï¸ multi_plot_progressive_dropdown_ui - FUNCTION CALLED at {start_time}")
        
        # Make reactive to update trigger
        try:
            _ = multi_plot_update_trigger.get()
        except:
            pass
        
        # Check if multi-plot mode is enabled
        t1 = time.time()
        try:
            multi_mode = multi_plot_mode_enabled.get()
            logging.debug(f"DEBUG: â±ï¸ Check multi_mode: {(time.time() - t1)*1000:.1f}ms - value: {multi_mode}")
            if not multi_mode:
                logging.debug(f"DEBUG: multi_plot_progressive_dropdown_ui - returning None because multi_mode is False")
                return None
        except Exception as e:
            logging.debug(f"DEBUG: Error checking multi_plot_mode: {e}")
            return None
        
        try:
            # Get multi-plot step completion status
            t2 = time.time()
            step1_completed = multi_plot_step_1_completed.get()
            step2_completed = multi_plot_step_2_completed.get()
            step3_completed = multi_plot_step_3_completed.get()
            step4_completed = multi_plot_step_4_completed.get()
            step5_completed = multi_plot_step_5_completed.get()
            step6_completed = multi_plot_step_6_completed.get()
            logging.debug(f"DEBUG: â±ï¸ Get step statuses: {(time.time() - t2)*1000:.1f}ms")
            
            logging.debug(f"DEBUG: Multi-plot step completion - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, step6: {step6_completed}")
            
            # Get available options
            t3 = time.time()
            filepaths_map = reactive_netcdf_filepaths.get()
            logging.debug(f"DEBUG: â±ï¸ Get filepaths_map: {(time.time() - t3)*1000:.1f}ms")
            
            t4 = time.time()
            grid_feature_ids = reactive_grid_feature_ids.get()
            logging.debug(f"DEBUG: â±ï¸ Get grid_feature_ids: {(time.time() - t4)*1000:.1f}ms")
            
            # Check if data is still loading
            if not filepaths_map or not grid_feature_ids:
                return ui.div(
                    ui.div(
                        ui.div("Loading...", class_="step-title"),
                        ui.div("Please wait while the climate data is being loaded.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                )
            
            # Prepare choices for dropdowns
            t5 = time.time()
            gridbox_choices = {"Select a grid cell...": ""}
            if grid_feature_ids and len(grid_feature_ids) > 0:
                for fid in grid_feature_ids:
                    gridbox_choices[fid] = fid
            else:
                # Comprehensive fallback choices with actual grid IDs - include more grid cells
                for lat in range(7, 43):  # Based on the logs showing grid cells up to 42
                    for lon in range(3, 42):  # Based on the logs showing grid cells up to 41
                        grid_id = f"{lat},{lon}"
                        gridbox_choices[grid_id] = grid_id
            logging.debug(f"DEBUG: â±ï¸ Build gridbox_choices: {(time.time() - t5)*1000:.1f}ms")
            
            # Get all available options
            t6 = time.time()
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            logging.debug(f"DEBUG: â±ï¸ Extract variables/models/scenarios from filepaths_map: {(time.time() - t6)*1000:.1f}ms")
            
            # If model is selected, filter variables and scenarios
            current_model = multi_plot_model.get()
            if current_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == current_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == current_model])
            
            # If variable is selected, filter scenarios further
            current_variable = multi_plot_variable.get()
            if current_variable and current_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == current_model and var == current_variable])
            
            # Determine which step to show based on completion status
            if not step1_completed:
                # Step 1: Gridbox selection dropdown for multi-plot mode
                grid_options = {"Select a gridbox...": ""}
                for grid_id in grid_feature_ids:
                    grid_options[f"Grid {grid_id}"] = grid_id
                
                return ui.div(
                    ui.div("Multi-Plot Step 1: Select Grid Location", class_="step-title"),
                    ui.div("Select a gridbox from the dropdown below, click a gridbox on the map, or draw a closed loop over gridboxes to analyze regional average data:", class_="step-explanation"),
                    ui.input_select(
                        "multi_plot_gridbox_selector",
                        "Select Gridbox:",
                        choices=grid_options,
                        selected=""  # Always default to blank in Additional Traces mode
                    ),
                    class_="dropdown-step active"
                )
            elif step1_completed and not step2_completed:
                # FIXED ORDER: Step 2 is now Model (before variable selection)
                t_step2 = time.time()
                model_choices = {"Select a model...": ""}
                for model in available_models:
                    model_choices[model] = model
                logging.debug(f"DEBUG: â±ï¸ Step 2 - Build model_choices: {(time.time() - t_step2)*1000:.1f}ms")
                
                # Conditionally add observed data option based on whether this is a loop or gridbox
                # Check if we're in a loop scenario
                t_obs = time.time()
                loop_available_vars = multi_plot_loop_available_variables.get()
                logging.debug(f"DEBUG: â±ï¸ Step 2 - Get loop_available_vars: {(time.time() - t_obs)*1000:.1f}ms")
                
                if loop_available_vars:
                    # For loops: only add "Observed" if at least one obs variable is available across ALL gridboxes
                    obs_vars_available = any(v in loop_available_vars for v in ['obs_tmax', 'obs_tmin', 'obs_pr'])
                    if obs_vars_available:
                        model_choices["Observed"] = "Observational Data"
                        logging.debug(f"DEBUG: Multi-plot step 2 - Observed data available for loop")
                    else:
                        logging.debug(f"DEBUG: Multi-plot step 2 - Observed data NOT available for loop (filtered by loop)")
                else:
                    # For single gridboxes: always show "Observed" option - all data available for individual gridboxes
                    model_choices["Observed"] = "Observational Data"
                    logging.debug(f"DEBUG: Multi-plot step 2 - Showing all models for individual gridbox (no loop filtering)")
                
                logging.debug(f"DEBUG: â±ï¸ STEP 2 TOTAL TIME: {(time.time() - t_step2)*1000:.1f}ms")
                return ui.div(
                    ui.div("Multi-Plot Step 2: Select Data Type", class_="step-title"),
                    ui.input_select("multi_plot_model", "Select data type:", choices=model_choices, selected=""),
                    ui.div("Choose whether to use observational data or climate model projections for your additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step2_completed and not step3_completed:
                # FIXED ORDER: Step 3 is now Variable (after model selection)
                # Get the selected model to filter variables
                t_step3 = time.time()
                current_model = multi_plot_model.get()
                logging.debug(f"DEBUG: â±ï¸ Step 3 - Get current_model: {(time.time() - t_step3)*1000:.1f}ms")
                
                # Filter variables based on model selection
                t_filter = time.time()
                if current_model == "Observed":
                    # For observational data, only show observed variables
                    filtered_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
                else:
                    # For model data, filter by selected model
                    if current_model:
                            # For specific models, filter by model name
                            model_filtered_variables = sorted(set(var for (var, model, ssp) in filepaths_map.keys() if model == current_model))
                            filtered_variables = model_filtered_variables
                    else:
                        # If no model selected yet, use all variables
                        filtered_variables = list(available_variables)
                logging.debug(f"DEBUG: â±ï¸ Step 3 - Filter variables by model: {(time.time() - t_filter)*1000:.1f}ms")
                
                # CONDITIONAL FILTER: If this is a loop, only show variables available across ALL gridboxes
                # For individual gridboxes, show all available variables
                t_loop = time.time()
                loop_available_vars = multi_plot_loop_available_variables.get()
                logging.debug(f"DEBUG: â±ï¸ Step 3 - Get loop_available_vars: {(time.time() - t_loop)*1000:.1f}ms")
                
                if loop_available_vars:
                    # Filter to only include variables that are available in the loop
                    filtered_variables = [v for v in filtered_variables if v in loop_available_vars]
                    logging.debug(f"DEBUG: Multi-plot variables filtered by loop availability: {sorted(filtered_variables)}")
                    if not filtered_variables:
                        logging.debug(f"DEBUG: âš ï¸ WARNING: No variables available after loop filtering!")
                else:
                    # For individual gridboxes, show all available variables for the selected model
                    logging.debug(f"DEBUG: Multi-plot step 3 - Showing all {len(filtered_variables)} variables for individual gridbox (no loop filtering)")
                
                t_choices = time.time()
                variable_choices = create_variable_choices(sorted(filtered_variables))
                logging.debug(f"DEBUG: â±ï¸ Step 3 - create_variable_choices: {(time.time() - t_choices)*1000:.1f}ms")
                
                # Update step explanation based on whether this is a loop or gridbox
                if loop_available_vars:
                    step_explanation = f"Choose which climate variable. Only showing {len(filtered_variables)} variable(s) with valid data across ALL gridboxes in the loop."
                else:
                    step_explanation = "Choose which climate variable for the additional dataset. All variables for the selected data type are shown."
                
                logging.debug(f"DEBUG: â±ï¸ STEP 3 TOTAL TIME: {(time.time() - t_step3)*1000:.1f}ms")
                logging.debug(f"DEBUG: â±ï¸ ðŸŽ¯ ENTIRE FUNCTION TIME: {(time.time() - start_time)*1000:.1f}ms")
                return ui.div(
                    ui.div("Multi-Plot Step 3: Choose Climate Variable", class_="step-title"),
                    ui.input_select("multi_plot_variable", "Select climate variable:", choices=variable_choices, selected=""),  # Always default to blank
                    ui.div(step_explanation, class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step3_completed and not step4_completed:
                # Frequency choices in Shiny format: {value: label}
                frequency_choices = {"": "Select frequency...", "daily": "Daily", "monthly": "Monthly", "annual": "Annual"}
                return ui.div(
                    ui.div("Multi-Plot Step 4: Choose Frequency", class_="step-title"),
                    ui.input_select("multi_plot_frequency", "Select data frequency:", choices=frequency_choices, selected=""),
                    ui.div("Choose the frequency of data to display: daily (raw data), monthly (averaged), or annual (averaged).", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step4_completed and not step5_completed:
                # Step 5: Scenario (skipped for observations)
                sel_var = multi_plot_variable.get()
                is_observed = sel_var in ("obs_tmin","obs_tmax","obs_pr")
                if is_observed:
                    return ui.div(
                        ui.div("Multi-Plot Step 5: Scenario (skipped for observations)", class_="step-title"),
                        ui.div("Observational datasets do not require an SSP scenario.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                scenario_choices = {"Select scenario...": ""}
                for scenario in available_scenarios:
                    scenario_choices[scenario] = scenario
                return ui.div(
                    ui.div("Multi-Plot Step 5: Choose Scenario", class_="step-title"),
                    ui.input_select("multi_plot_ssp", "Select SSP scenario:", choices=scenario_choices, selected=""),
                    ui.div("Select the scenario for the additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step5_completed and not step6_completed:
                # Show action options immediately without premature unit compatibility check
                current_variable = multi_plot_variable.get()
                plots_list = multi_plots_list.get()
                
                if current_variable:
                    # Only show option to add to main plot - no separate plots
                    return ui.div(
                        ui.div("Multi-Plot Step 6: Add to Main Plot", class_="step-title"),
                        ui.div(f"Ready to add {get_variable_display_name(current_variable)} to your main plot.", class_="step-explanation"),
                        ui.div(
                            ui.div("âœ… This trace will be added to your main plot (if units are compatible)", class_="alert alert-success"),
                            ui.div("Note: Only traces with compatible units will be added to the same plot. You can add the same variable with different frequencies.", class_="text-muted"),
                            class_="mt-3"
                        ),
                        class_="dropdown-step active"
                    )
                else:
                    return ui.div(
                        ui.div("Multi-Plot Step 5: Waiting for Selection", class_="step-title"),
                        ui.div("Please complete the previous steps to select a variable for comparison.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
            else:
                # When steps were previously completed, immediately present Step 1 UI again
                # so the user can start a new selection without seeing a banner
                # Recreate the gridbox choices (already computed above)
                return ui.div(
                    ui.div("Multi-Plot Step 1: Select Grid Location", class_="step-title"),
                    ui.input_select("multi_plot_gridbox_selector", "Choose a grid cell:", choices=gridbox_choices, selected=""),
                    ui.div("Select a specific grid cell for your additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
        except Exception as e:
            logging.debug(f"DEBUG: â±ï¸ ðŸŽ¯ ENTIRE FUNCTION TIME (ERROR): {(time.time() - start_time)*1000:.1f}ms")
            logging.debug(f"DEBUG: Error in multi_plot_progressive_dropdown_ui: {e}")
            import traceback
            traceback.print_exc()
            return ui.div(
                ui.div("Multi-Plot Mode Error", class_="step-title"),
                ui.div("An error occurred while loading multi-plot options.", class_="step-explanation"),
                class_="dropdown-step active"
            )
    @output
    @render.ui
    def map_and_data_output():
        logging.debug("DEBUG: map_and_data_output FUNCTION CALLED")
        
        # FIXED: Batch all reactive reads at the start to ensure atomic rendering
        # This prevents sequential loading by reading all reactive values first
        with reactive.isolate():
            grid_geojson = reactive_grid_geojson.get()
            netcdf_filepaths = reactive_netcdf_filepaths.get()
            grid_feature_ids = reactive_grid_feature_ids.get()
        
        # Re-read without isolation for reactivity
        grid_geojson = reactive_grid_geojson.get()
        netcdf_filepaths = reactive_netcdf_filepaths.get()
        
        # Check if ALL necessary reactive values are ready for map display
        if grid_geojson is None or not netcdf_filepaths:
            logging.debug("DEBUG: map_and_data_output - Data not loaded yet")
            logging.debug(f"DEBUG: reactive_grid_geojson: {grid_geojson}")
            logging.debug(f"DEBUG: reactive_netcdf_filepaths: {len(netcdf_filepaths) if netcdf_filepaths else 0}")
            
            # Show a simple UI even if data is not loaded to test functionality
            # Note: Do NOT include progressive_dropdown_ui here to avoid duplicate output IDs
            return ui.div(
                ui.div(
                    ui.h5("Data Loading Status", class_="text-center"),
                    ui.p(f"Grid GeoJSON: {'Loaded' if grid_geojson else 'Not loaded'}", class_="text-center"),
                    ui.p(f"File paths: {'Loaded' if netcdf_filepaths else 'Not loaded'}", class_="text-center"),
                    ui.p(f"Grid feature IDs: {'Loaded' if grid_feature_ids else 'Not loaded'}", class_="text-center"),
                    class_="alert alert-info"
                ),
                ui.div(
                    ui.div(
                        ui.h5("Test Plot Area", class_="text-center text-muted"),
                        ui.p("Waiting for data to load...", class_="text-center text-muted"),
                        class_="mt-3"
                    ),
                    class_="mt-3"
                )
            )
        else:
            # Check if all progressive steps are completed
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            all_steps_completed = step1_completed and step2_completed and step3_completed and step4_completed and step5_completed and step6_completed
            
            # Debug output
            logging.debug(f"DEBUG: map_and_data_output - Step completion status:")
            logging.debug(f"DEBUG:   Step 1: {step1_completed}")
            logging.debug(f"DEBUG:   Step 2: {step2_completed}")
            logging.debug(f"DEBUG:   Step 3: {step3_completed}")
            logging.debug(f"DEBUG:   Step 4: {step4_completed}")
            logging.debug(f"DEBUG:   Step 5: {step5_completed}")
            logging.debug(f"DEBUG:   Step 6: {step6_completed}")
            logging.debug(f"DEBUG:   All steps completed: {all_steps_completed}")
            logging.debug(f"DEBUG:   Selected grid ID: {selected_grid_id.get()}")
            logging.debug(f"DEBUG:   Selected variable: {selected_netcdf_variable.get()}")
            logging.debug(f"DEBUG:   Selected model: {selected_netcdf_model.get()}")
            logging.debug(f"DEBUG:   Selected frequency: {selected_netcdf_frequency.get()}")
            logging.debug(f"DEBUG:   Selected SSP: {selected_netcdf_ssp.get()}")
            
            # FIXED: Header and description now load with map and data to prevent sequential loading
            return ui.div(
                ui.h3("Explore Climate Data by Grid Location"),
                ui.p("Click a grid cell on the map to view its detailed climate projections. Only grid cells with data available across ALL loaded climate variables, models, and scenarios are shown.", class_="text-muted mb-3"),
                ui.div(
                    ui.tags.div(
                        ui.tags.div(
                            ui.tags.strong("âš ï¸ Map Loading Limitation", style="color: #856404; font-size: 1.1rem;"),
                            ui.tags.p(
                                "If the map appears gray or blank after reloading the page, you'll need to clear your browser's cached images and files:",
                                style="margin: 10px 0 5px 0;"
                            ),
                            ui.tags.ul(
                                ui.tags.li(ui.tags.strong("Chrome/Edge: "), "Press Ctrl+Shift+Delete (Cmd+Shift+Delete on Mac), check 'Cached images and files', click 'Clear data'"),
                                ui.tags.li(ui.tags.strong("Safari: "), "Press Cmd+Option+E to empty caches"),
                                ui.tags.li(ui.tags.strong("Firefox: "), "Press Ctrl+Shift+Delete (Cmd+Shift+Delete on Mac), check 'Cache', click 'Clear Now'"),
                                style="margin: 10px 0; padding-left: 20px; font-size: 0.9rem;"
                            ),
                            ui.tags.p(
                                "This is a known limitation of the ipyleaflet mapping library. The map works correctly on first load.",
                                style="margin-top: 10px; font-size: 0.85rem; color: #666; font-style: italic;"
                            ),
                            style="background-color: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin-bottom: 15px;"
                        )
                    ),
                    output_widget("gridbox_map"),
                    class_="map-container",
                    style="width: 100%; max-width: none; overflow: visible; margin-bottom: 10px;"
                ),
                ui.output_ui("hovered_gridbox_tooltip"),  # Show hovered grid ID
                ui.output_text("selected_gridbox_info"),
                ui.div(
                    # Plot container with absolutely locked dimensions
                    ui.div(
                        output_widget("selected_gridbox_plot"),
                        class_="plot-container",
                        style="width: 100%; max-width: none; overflow: visible; height: 600px !important; min-height: 600px !important; max-height: 600px !important; flex-shrink: 0; flex-grow: 0; flex-basis: 600px; margin-bottom: 0px; padding-bottom: 20px; position: relative;"
                    ),
                    # Mean Absolute Difference Table
                    ui.div(
                        ui.output_ui("mad_table"),
                        style="margin-top: 40px; margin-bottom: 20px; flex-shrink: 0; flex-grow: 0;"
                    ),
                    # Unified trace controls (includes main plot + additional traces in scrollable table)
                    ui.div(
                        ui.output_ui("additional_traces_controls"),
                        style="margin-top: 30px; flex-shrink: 0; flex-grow: 0;"
                    ),
                    class_="mt-3",
                    style="display: flex; flex-direction: column; gap: 20px; width: 100%;"
                ),
                # Multi-plot output - show additional plots when needed (isolated container)
                ui.div(
                    ui.output_ui("multi_plot_output"),
                    style="flex-shrink: 0; flex-grow: 0; margin-top: 30px;"
                ),
                # Multi-plot UI section - show when main plot is complete (isolated container)
                ui.div(
                    ui.div(
                        ui.h4("Additional Traces Mode", class_="mt-4"),
                        ui.div(
                            ui.output_ui("multi_plot_checkbox_ui"),
                            ui.div("Enable this to add additional time series traces to your main plot using the progressive dropdown system. All traces with compatible units will be displayed on the same graph. No separate plots will be created.", class_="step-explanation"),
                            class_="mt-3"
                        ),
                        ui.output_ui("multi_plot_progressive_dropdown_ui"),
                        ui.output_ui("polygon_loading_indicator_multi"),
                        ui.output_ui("multi_loop_naming_ui"),
                        class_="mt-3"
                    ),
                    style="flex-shrink: 0; flex-grow: 0; margin-top: 30px;"
                ) if all_steps_completed else None,
                ui.div(
                    ui.div(
                        ui.h5("Select Options to View Data", class_="text-center text-muted"),
                        ui.p("Please complete the progressive dropdown steps above to view climate data plots. You can also click directly on the map to select a grid cell.", class_="text-center text-muted"),
                        class_="alert alert-info"
                    ),
                    class_="mt-3"
                ) if not all_steps_completed else None
            )
    
    @reactive.Effect
    @reactive.event(input.time_series_range_main)
    def _handle_main_time_range_change():
        """Handle changes to the main time series range slider"""
        try:
            main_range = input.time_series_range_main()
            if main_range and len(main_range) == 2:
                logging.debug(f"DEBUG: Main time series range changed to: {main_range}")
                # Trigger plot update to apply new range to all traces
                plot_update_trigger.set(plot_update_trigger.get() + 1)
        except Exception as e:
            logging.debug(f"DEBUG: Error handling main time range change: {e}")
    
    @reactive.Effect
    @reactive.event(input.trendline_main, ignore_none=True)
    def _handle_main_trendline_change():
        """Handle changes to the main trendline checkbox"""
        try:
            show_trend = input.trendline_main()
            logging.debug(f"DEBUG: Main trendline checkbox changed to: {show_trend}")
            # Trigger plot update to show/hide trendline
            plot_update_trigger.set(plot_update_trigger.get() + 1)
        except Exception as e:
            logging.debug(f"DEBUG: Error handling main trendline change: {e}")
    
    # Note: Additional trace sliders and checkboxes will automatically trigger plot updates
    # because the plot function reads them directly when rendering each trace

    # IMPROVED: Debounced update mechanism with better batching for slider changes
    _last_trace_change_time = reactive.Value(0)
    _update_scheduled = reactive.Value(False)
    
    @reactive.Effect
    @reactive.event(main_plot_additional_traces, main_plot_persistent_traces)
    def _debounced_trace_change_handler():
        """Debounced handler for trace changes to prevent UI errors"""
        try:
            import time
            current_time = time.time()
            _last_trace_change_time.set(current_time)
            _update_scheduled.set(True)
            logging.debug("DEBUG: Trace change detected, scheduling debounced update")
        except Exception as e:
            logging.debug(f"DEBUG: Error in debounced trace change handler: {e}")
    
    @reactive.Effect
    @reactive.event(_last_trace_change_time)
    def _trigger_plot_update_on_trace_changes():
        """Trigger plot update when additional traces change (debounced with better batching)"""
        try:
            import time
            # IMPROVED: Longer delay (0.5s) to properly batch rapid changes
            time.sleep(0.5)
            
            # Check if enough time has passed since the last change
            time_since_change = time.time() - _last_trace_change_time.get()
            
            # IMPROVED: Only update if at least 0.5s has passed AND an update is scheduled
            if time_since_change >= 0.45 and _update_scheduled.get():
                # This will trigger the plot to re-render when traces change
                current_trigger = plot_update_trigger.get()
                plot_update_trigger.set(current_trigger + 1)
                _update_scheduled.set(False)
                logging.debug("DEBUG: Triggered plot update due to trace changes (debounced, batched)")
            else:
                logging.debug(f"DEBUG: Skipping update - more changes incoming (time since: {time_since_change:.2f}s)")
        except Exception as e:
            logging.debug(f"DEBUG: Error triggering plot update: {e}")
            # Don't let errors crash the app - just log them
            import traceback
            traceback.print_exc()
    # Test handler for any plotly events - DISABLED for performance
    # @reactive.Effect
    # def _test_any_input():
    #     try:
    #         # Try to access any plotly-related inputs
    #         for attr in dir(input):
    #             if 'gridbox' in attr.lower() or 'map' in attr.lower():
    #                 try:
    #                     value = getattr(input, attr)()
    #                     if value is not None:
    #                         print(f"DEBUG: Found input {attr}: {value}")
    #                 except:
    #                     pass
    #     except Exception as e:
    #         print(f"DEBUG: Error in test handler: {e}")

    # Add a more comprehensive click detection approach - DISABLED for performance
    # @reactive.Effect
    # def _comprehensive_click_detection():
    #     """Try to detect clicks using multiple approaches"""
    #     try:
    #         # Check all possible plotly input patterns
    #         possible_inputs = [
    #             'gridbox_map_click',
    #             'gridbox_map_selected', 
    #             'gridbox_map_hover',
    #             'gridbox_map_brushed',
    #             'gridbox_map_brushedPoints',
    #             'gridbox_map_selectedPoints',
    #             'gridbox_map_restyle',
    #             'gridbox_map_relayout'
    #         ]
    #         
    #         for input_name in possible_inputs:
    #             if hasattr(input, input_name):
    #                 try:
    #                     value = getattr(input, input_name)()
    #                     if value is not None:
    #                         print(f"DEBUG: ===== CLICK DETECTED via {input_name} =====")
    #                         print(f"DEBUG: Click data: {value}")
    #                         _process_click_data(value)
    #                 except Exception as e:
    #                     pass
    #     except Exception as e:
    #         print(f"DEBUG: Error in comprehensive click detection: {e}")

    def _process_click_data(click_data):
        """Process click data and update selections"""
        logging.debug(f"DEBUG: Processing click data: {click_data}")
        logging.debug(f"DEBUG: Click data type: {type(click_data)}")
        
        # Extract the clicked grid ID from the click data
        clicked_id = None
        if click_data and 'points' in click_data and len(click_data['points']) > 0:
            point = click_data['points'][0]
            logging.debug(f"DEBUG: First point data: {point}")
            logging.debug(f"DEBUG: Point keys: {point.keys() if isinstance(point, dict) else 'Not a dict'}")
            
            if 'customdata' in point:
                clicked_id = point['customdata']
                logging.debug(f"DEBUG: Found customdata: {clicked_id}")
            elif 'text' in point:
                clicked_id = point['text']
                logging.debug(f"DEBUG: Found text: {clicked_id}")
            else:
                logging.debug(f"DEBUG: No customdata or text found in point")
        else:
            logging.debug(f"DEBUG: No points in click data or click_data is None")
        
        logging.debug(f"DEBUG: Extracted clicked ID: {clicked_id}")
        
        if clicked_id and clicked_id in reactive_grid_feature_ids.get():
            # Determine if we are in multi-plot mode
            multi_mode = multi_plot_mode_enabled.get()
            logging.debug(f"DEBUG: Processing click for grid ID: {clicked_id}, multi_mode: {multi_mode}")
            
            if multi_mode:
                # In multi-plot mode, update the multi-plot grid selection
                multi_plot_grid_id.set(clicked_id)
                logging.debug(f"DEBUG: Set multi_plot_grid_id to: {clicked_id}")
            else:
                # In single-plot mode, update the main selection
                selected_grid_id.set(clicked_id)
                logging.debug(f"DEBUG: Set selected_grid_id to: {clicked_id}")
            
            # Update the dropdown to reflect the selection
            ui.update_select("gridbox_selector", selected=clicked_id)
            
            # Mark step 1 as completed
            dropdown_step_1_completed.set(True)
            logging.debug(f"DEBUG: Step 1 completed!")
        else:
            logging.debug(f"DEBUG: Invalid grid ID or not in valid IDs: {clicked_id}")
            logging.debug(f"DEBUG: Valid IDs: {list(reactive_grid_feature_ids.get())[:5]}...")

    # OPTIMIZED: Batch reactive updates to prevent redundant triggers
    # Handle gridbox selector dropdown changes
    @reactive.Effect
    @reactive.event(input.gridbox_selector)
    def _handle_gridbox_selector():
        """Handle gridbox selector dropdown changes - OPTIMIZED to reduce cascading updates"""
        logging.debug("DEBUG: ===== GRIDBOX SELECTOR CHANGED =====")
        selected_display = input.gridbox_selector()
        logging.debug(f"DEBUG: Selected gridbox: {selected_display}")
        
        if selected_display and selected_display != "" and selected_display != "Select a gridbox...":
            # Extract the actual grid ID from the display text (e.g., "Grid 8,19" -> "8,19")
            if selected_display.startswith("Grid "):
                actual_grid_id = selected_display[5:]  # Remove "Grid " prefix
            else:
                actual_grid_id = selected_display
            
            # OPTIMIZED: Check validity without triggering excessive reactive reads
            with reactive.isolate():
                valid_ids = reactive_grid_feature_ids.get()
            
            if actual_grid_id in valid_ids:
                logging.debug(f"DEBUG: Valid grid ID: {actual_grid_id}, processing selection...")
                # OPTIMIZED: Batch updates to prevent cascading reactive triggers
                # Only update if the value actually changed
                current_grid_id = selected_grid_id.get()
                if current_grid_id != actual_grid_id:
                    selected_grid_id.set(actual_grid_id)
                    logging.debug(f"DEBUG: Set selected_grid_id to: {actual_grid_id}")
                    
                    # CRITICAL FIX: Clear loop-related data when selecting an individual gridbox
                    # This ensures loop filtering doesn't apply to individual gridbox selections
                    selected_loop_available_variables.set(None)
                    selected_loop_gridboxes.set(None)
                    selected_loop_name.set(None)
                    selected_loop_polygon.set(None)
                    logging.debug(f"DEBUG: Cleared loop data - this is an individual gridbox selection")
                    
                    # Mark step 1 as completed immediately after grid selection
                    dropdown_step_1_completed.set(True)
                    logging.debug(f"DEBUG: Step 1 completed!")
                else:
                    logging.debug(f"DEBUG: Grid ID unchanged, skipping redundant update")
            else:
                logging.debug(f"DEBUG: Invalid grid ID: {actual_grid_id}")
        else:
            logging.debug(f"DEBUG: No gridbox selected")
    
    # OPTIMIZED: Handle multi-plot gridbox selector dropdown changes
    @reactive.Effect
    @reactive.event(input.multi_plot_gridbox_selector)
    def _handle_multi_plot_gridbox_selector():
        """Handle multi-plot gridbox selector dropdown changes - OPTIMIZED"""
        logging.debug("DEBUG: ===== MULTI-PLOT GRIDBOX SELECTOR CHANGED =====")
        selected_display = input.multi_plot_gridbox_selector()
        logging.debug(f"DEBUG: Selected multi-plot gridbox: {selected_display}")
        
        if selected_display and selected_display != "" and selected_display != "Select a gridbox...":
            # Extract the actual grid ID from the display text (e.g., "Grid 8,19" -> "8,19")
            if selected_display.startswith("Grid "):
                actual_grid_id = selected_display[5:]  # Remove "Grid " prefix
            else:
                actual_grid_id = selected_display
            
            # OPTIMIZED: Check validity without triggering excessive reactive reads
            with reactive.isolate():
                valid_ids = reactive_grid_feature_ids.get()
            
            if actual_grid_id in valid_ids:
                logging.debug(f"DEBUG: Valid grid ID: {actual_grid_id}, processing multi-plot selection...")
                # FIXED: Always reset steps 2-6 when grid is selected (even if same grid)
                # This allows adding multiple traces from the same grid
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear subsequent selections
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                
                # CRITICAL FIX: Clear loop-related data when selecting an individual gridbox
                # This ensures loop filtering doesn't apply to individual gridbox selections
                multi_plot_loop_available_variables.set(None)
                multi_plot_loop_gridboxes.set(None)
                multi_plot_loop_name.set(None)
                multi_plot_loop_polygon.set(None)
                logging.debug(f"DEBUG: Cleared multi-plot loop data - this is an individual gridbox selection")
                
                # OPTIMIZED: Only update grid_id if value changed to avoid cascading updates
                current_grid_id = multi_plot_grid_id.get()
                if current_grid_id != actual_grid_id:
                    multi_plot_grid_id.set(actual_grid_id)
                    logging.debug(f"DEBUG: Set multi_plot_grid_id to: {actual_grid_id}")
                else:
                    logging.debug(f"DEBUG: Multi-plot grid ID unchanged, but resetting workflow for new trace")
                # FIXED: Always mark step 1 as completed when valid grid is selected (even if unchanged)
                # This allows adding multiple traces from the same grid
                multi_plot_step_1_completed.set(True)
                logging.debug(f"DEBUG: Multi-plot Step 1 completed! Ready for new trace from grid {actual_grid_id}")
            else:
                logging.debug(f"DEBUG: Invalid multi-plot grid ID: {actual_grid_id}")
        else:
            logging.debug(f"DEBUG: No multi-plot gridbox selected")


    # Try different input name patterns for plotly clicks - DISABLED for performance
    # @reactive.Effect
    # def _try_all_plotly_inputs():
    #     try:
    #         # Try various possible input names
    #         possible_inputs = [
    #             'gridbox_map_click',
    #             'gridbox_map_selected', 
    #             'gridbox_map_hover',
    #             'gridbox_map_brushed',
    #             'gridbox_map_brushedPoints',
    #             'gridbox_map_selectedPoints',
    #             'gridbox_map_restyle',
    #             'gridbox_map_relayout'
    #         ]
    #         
    #         for input_name in possible_inputs:
    #             try:
    #                 if hasattr(input, input_name):
    #                     value = getattr(input, input_name)()
    #                     if value is not None:
    #                         print(f"DEBUG: Found plotly input {input_name}: {value}")
    #             except:
    #                 pass
    #     except Exception as e:
    #         print(f"DEBUG: Error checking plotly inputs: {e}")

    @reactive.Effect
    @reactive.event(input.gridbox_map_click)
    def _handle_map_click():
        logging.debug("DEBUG: ===== CLICK HANDLER TRIGGERED =====")
        try:
            click_data = input.gridbox_map_click()
            logging.debug(f"DEBUG: Map click data: {click_data}")
        except Exception as e:
            logging.debug(f"DEBUG: Error getting map click input: {e}")
            return
        logging.debug(f"DEBUG: Map click detected! Click data: {click_data}")
        logging.debug(f"DEBUG: Click data type: {type(click_data)}")
        
        # Extract the clicked grid ID from the click data
        clicked_id = None
        if click_data and 'points' in click_data and len(click_data['points']) > 0:
            point = click_data['points'][0]
            logging.debug(f"DEBUG: First point data: {point}")
            logging.debug(f"DEBUG: Point keys: {point.keys() if isinstance(point, dict) else 'Not a dict'}")
            
            if 'customdata' in point:
                clicked_id = point['customdata']
                logging.debug(f"DEBUG: Found customdata: {clicked_id}")
            elif 'text' in point:
                clicked_id = point['text']
                logging.debug(f"DEBUG: Found text: {clicked_id}")
            else:
                logging.debug(f"DEBUG: No customdata or text found in point")
        else:
            logging.debug(f"DEBUG: No points in click data or click_data is None")
        
        logging.debug(f"DEBUG: Extracted clicked ID: {clicked_id}")
        
        if clicked_id and clicked_id in reactive_grid_feature_ids.get():
            # Determine if we are in multi-plot mode
            multi_mode = multi_plot_mode_enabled.get()

            if multi_mode:
                # FIXED: Only allow map selection at step 1 (no other steps completed yet)
                step2 = multi_plot_step_2_completed.get()
                step3 = multi_plot_step_3_completed.get()
                step4 = multi_plot_step_4_completed.get()
                step5 = multi_plot_step_5_completed.get()
                
                if step2 or step3 or step4 or step5:
                    logging.debug(f"DEBUG: Map click ignored - not at step 1 in multi-plot mode (steps 2-5: {step2}, {step3}, {step4}, {step5})")
                    return
                
                # Check if this gridbox is part of an existing loop (just for logging)
                loop_gridboxes = selected_loop_gridboxes.get()
                is_in_loop = False
                if loop_gridboxes:
                    loop_grid_ids = [grid_id for grid_id, weight in loop_gridboxes]
                    is_in_loop = clicked_id in loop_grid_ids
                    if is_in_loop:
                        logging.debug(f"DEBUG: âœ… Selected gridbox {clicked_id} IS inside the existing loop (will create separate trace)")
                    else:
                        logging.debug(f"DEBUG: Selected gridbox {clicked_id} is NOT in the existing loop")
                
                # In multi-plot mode, map clicks should select the grid for the multi-plot flow
                # NOTE: Gridboxes inside existing loops are fully clickable!
                if multi_plot_grid_id.get() != clicked_id:
                    multi_plot_grid_id.set(clicked_id)
                    logging.debug(f"DEBUG:   --> multi_plot_grid_id reactive value set to: {multi_plot_grid_id.get()}")
                
                # CRITICAL FIX: Clear loop-related data when clicking an individual gridbox
                # This ensures loop filtering doesn't apply to individual gridbox selections
                multi_plot_loop_available_variables.set(None)
                multi_plot_loop_gridboxes.set(None)
                multi_plot_loop_name.set(None)
                multi_plot_loop_polygon.set(None)
                logging.debug(f"DEBUG: Cleared multi-plot loop data from map click - this is an individual gridbox selection")
                
                # No need to update dropdown since we removed it
                multi_plot_step_1_completed.set(True)
                logging.debug(f"DEBUG: Multi-plot Step 1 completed - advancing to Step 2")
                # Trigger multi-plot UI update
                try:
                    multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                except Exception:
                    pass
            else:
                # FIXED: Only allow map selection at step 1 (no other steps completed yet)
                step2 = dropdown_step_2_completed.get()
                step3 = dropdown_step_3_completed.get()
                step4 = dropdown_step_4_completed.get()
                step5 = dropdown_step_5_completed.get()
                
                if step2 or step3 or step4 or step5:
                    logging.debug(f"DEBUG: Map click ignored - not at step 1 in main mode (steps 2-5: {step2}, {step3}, {step4}, {step5})")
                    return
                
                # Check if this gridbox is part of an existing loop (just for logging)
                loop_gridboxes = selected_loop_gridboxes.get()
                is_in_loop = False
                if loop_gridboxes:
                    loop_grid_ids = [grid_id for grid_id, weight in loop_gridboxes]
                    is_in_loop = clicked_id in loop_grid_ids
                    if is_in_loop:
                        logging.debug(f"DEBUG: â„¹ï¸ Gridbox {clicked_id} was part of previous loop, but creating new main selection")
                
                # In regular mode, map clicks select the main grid
                # NOTE: Gridboxes inside existing loops are fully clickable!
                if selected_grid_id.get() != clicked_id:
                    selected_grid_id.set(clicked_id)
                    logging.debug(f"DEBUG:   --> selected_grid_id reactive value set to: {selected_grid_id.get()}")
                
                # CRITICAL FIX: Clear loop-related data when clicking an individual gridbox
                # This ensures loop filtering doesn't apply to individual gridbox selections
                selected_loop_available_variables.set(None)
                selected_loop_gridboxes.set(None)
                selected_loop_name.set(None)
                selected_loop_polygon.set(None)
                logging.debug(f"DEBUG: Cleared loop data from map click - this is an individual gridbox selection")
                
                # No need to update dropdown since we removed it
                dropdown_step_1_completed.set(True)
                logging.debug(f"DEBUG: Step 1 completed - advancing to Step 2")
                # Trigger UI update
                try:
                    plot_update_trigger.set(plot_update_trigger.get() + 1)
                except Exception:
                    pass

    @reactive.Calc
    def get_selected_grid_indices():
        grid_id = selected_grid_id.get()
        logging.debug(f"DEBUG: get_selected_grid_indices triggered. current selected_grid_id: {grid_id}")
        if grid_id:
            try:
                lat_idx_str, lon_idx_str = grid_id.split(",")
                new_indices = (int(lat_idx_str), int(lon_idx_str))
                logging.debug(f"DEBUG:   --> calculated new indices: {new_indices}")
                logging.debug(f"DEBUG:   --> lat_idx type: {type(new_indices[0])}, lon_idx type: {type(new_indices[1])}")
                return new_indices
            except (ValueError, TypeError) as e:
                logging.error(f"ERROR: Error parsing grid ID: {grid_id}. Expected format 'lat_idx,lon_idx'.")
                logging.error(f"ERROR: Exception: {e}")
                return None
        return None

    @output
    @render.ui
    def hovered_gridbox_tooltip():
        """Display the grid ID of the currently hovered gridbox"""
        hovered_id = hovered_grid_id.get()
        
        if hovered_id:
            return ui.div(
                ui.span(
                    f"ðŸ“ Hovering: Grid {hovered_id}",
                    style="font-size: 0.9em; color: #6c757d; font-weight: 500; padding: 5px 10px; background-color: #f8f9fa; border-radius: 4px; display: inline-block;"
                ),
                style="margin-bottom: 10px; min-height: 30px;"
            )
        else:
            return ui.div(style="margin-bottom: 10px; min-height: 30px;")  # Empty placeholder to prevent layout shift

    @output
    @render.text
    def selected_gridbox_info():
        current_id = selected_grid_id.get()
        all_filepaths = reactive_netcdf_filepaths.get()
    
        if not all_filepaths:
            return "Map data is loading..."

        # Try to get coordinates from a cached dataset first
        ds_for_coords = None
        for key, ds_obj in _cached_datasets.get().items():
            if 'lat' in ds_obj.coords and 'lon' in ds_obj.coords:
                ds_for_coords = ds_obj
                break
    
        # If not in cache, open the first available file to get coordinates
        if ds_for_coords is None and all_filepaths:
            first_filepath = next(iter(all_filepaths.values()))
            try:
                with xr.open_dataset(first_filepath, engine="netcdf4", decode_times=False) as ds_temp:
                    if 'lat' in ds_temp.coords and 'lon' in ds_temp.coords:
                        ds_for_coords = ds_temp
            except Exception as e:
                logging.error(f"Error opening first file for coordinate reference: {e}")

        if ds_for_coords is None or 'lat' not in ds_for_coords.coords or 'lon' not in ds_for_coords.coords:
            return "Map data is loading (coordinates not found)..."

        grid_indices = get_selected_grid_indices()

        logging.debug(f"DEBUG: selected_gridbox_info rendering. Current ID: {current_id}, Indices: {grid_indices}")
    
        if current_id and grid_indices:
            lat_idx, lon_idx = grid_indices
            if 0 <= lat_idx < ds_for_coords['lat'].size and 0 <= lon_idx < ds_for_coords['lon'].size:
                actual_lat = ds_for_coords['lat'].values[lat_idx]
                actual_lon = ds_for_coords['lon'].values[lon_idx]
                display_lon = actual_lon
                if np.any(ds_for_coords['lon'].values > 180) and actual_lon > 180:
                    display_lon = actual_lon - 360
                return f"Selected Gridbox: (Lat Index: {lat_idx}, Lon Index: {lon_idx}) | Approx. Lat: {actual_lat:.3f}, Approx. Lon: {display_lon:.3f}"
            else:
                logging.error(f"ERROR: Indices ({lat_idx}, {lon_idx}) out of bounds for ds_for_coords in selected_gridbox_info.")
                return f"Selected Gridbox ID: {current_id} (Indices out of bounds or dataset not ready)"
        return "No gridbox selected. Click a cell on the map."

    @output
    @render_widget
    def gridbox_map():
        """Render ipyleaflet gridbox map with clickable gridboxes - ONLY renders ONCE on data load, NEVER re-renders"""
        try:
            logging.debug("DEBUG: gridbox_map FUNCTION CALLED - Creating ipyleaflet map (should only happen ONCE)")
            
            # Depend on reload trigger to force re-render when button is clicked
            # CRITICAL FIX: Only depend on grid data, NOT on any other reactive values
            # This ensures the map renders once and stays visible forever
            grid_geojson_data = reactive_grid_geojson.get()
            grid_feature_ids_data = reactive_grid_feature_ids.get()
            
            # Isolate ALL other reactive values to prevent ANY re-renders
            with reactive.isolate():
                multi_current_id = multi_plot_grid_id.get()
                current_id = multi_current_id or selected_grid_id.get()

            logging.debug(f"DEBUG: gridbox_map rendering. Current highlighted grid_id: {current_id}")
            logging.debug(f"DEBUG: gridbox_map - grid_geojson_data: {grid_geojson_data is not None}, features: {len(grid_geojson_data.features) if grid_geojson_data and grid_geojson_data.features else 0}")
            logging.debug(f"DEBUG: gridbox_map - grid_feature_ids_data: {len(grid_feature_ids_data) if grid_feature_ids_data else 0}")
        except Exception as e:
            logging.debug(f"DEBUG: Error in gridbox_map initialization: {e}")
            import traceback
            traceback.print_exc()
            # Return blank map if initialization fails
            blank_map = Map(
                center=((SB_LAT_MIN + SB_LAT_MAX) / 2, (SB_LON_MIN + SB_LON_MAX) / 2),
                zoom=9,
                scroll_wheel_zoom=True,
                basemap={
                    'url': 'https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png',
                    'max_zoom': 17,
                    'attribution': '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }
            )
            blank_map.layout.height = '500px'
            blank_map.layout.width = '100%'
            return blank_map

        try:
            if not grid_geojson_data or not grid_geojson_data.features or not grid_feature_ids_data:
                logging.debug("DEBUG: Grid data not ready, returning blank map")
                # Return blank map if data not ready
                blank_map = Map(
                    center=((SB_LAT_MIN + SB_LAT_MAX) / 2, (SB_LON_MIN + SB_LON_MAX) / 2),
                    zoom=9,
                    scroll_wheel_zoom=True,
                    basemap={
                        'url': 'https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png',
                        'max_zoom': 17,
                        'attribution': '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                    }
                )
                blank_map.layout.height = '500px'
                blank_map.layout.width = '100%'
                return blank_map

            # Create ipyleaflet Map centered on Santa Barbara with OpenTopoMap basemap
            # Original geologic/topographic map
            leaflet_map = Map(
                center=((SB_LAT_MIN + SB_LAT_MAX) / 2, (SB_LON_MIN + SB_LON_MAX) / 2),
                zoom=9,
                scroll_wheel_zoom=True,
                basemap={
                    'url': 'https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png',
                    'max_zoom': 17,
                    'attribution': '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                },
                dragging=True,
                touch_zoom=True,
                double_click_zoom=True,
                box_zoom=True,
                keyboard=True,
                keyboard_pan_offset=80,
                keyboard_zoom_offset=1,
                inertia=True,
                inertia_deceleration=3000,
                inertia_max_speed=1500,
                world_copy_jump=False,
                max_bounds_viscosity=0.0,
                bounce_at_zoom_limits=True
            )
            leaflet_map.layout.height = '500px'
            leaflet_map.layout.width = '100%'
            
            
            # Force tile loading
            logging.debug("DEBUG: ipyleaflet map created with standard OSM tiles")

            # Define style function for GeoJSON features
            def style_callback(feature):
                """Apply styling to GeoJSON features based on selection state"""
                try:
                    grid_id = feature.get('id', '')
                    if grid_id == current_id:
                        # Selected gridbox: solid red border, no fill
                        return {
                            'color': 'red',
                            'weight': 4,
                            'fillOpacity': 0,
                            'opacity': 1,
                            'dashArray': ''  # Solid line for selected
                        }
                    else:
                        # Non-selected gridbox: black dotted border, no fill
                        return {
                            'color': 'black',
                            'weight': 1.5,
                            'fillOpacity': 0,
                            'opacity': 0.8,
                            'dashArray': '5, 5'  # Dotted line
                        }
                except Exception as e:
                    logging.debug(f"DEBUG: Error in style_callback: {e}")
                    # Return default style on error
                    return {
                        'color': 'black',
                        'weight': 1,
                        'fillOpacity': 0,
                        'opacity': 0.8,
                        'dashArray': '5, 5'
                    }

            # Convert GeoJSON FeatureCollection to dict for ipyleaflet
            try:
                geojson_dict = {
                    'type': 'FeatureCollection',
                    'features': [
                        {
                            'type': 'Feature',
                            'id': feature.id,
                            'geometry': feature.geometry,
                            'properties': {
                                **(feature.properties if hasattr(feature, 'properties') else {}),
                                'name': f"Grid {feature.id}",  # Add grid ID as name for hover tooltip
                                'grid_id': feature.id  # Also keep grid_id for reference
                            }
                        }
                        for feature in grid_geojson_data.features
                    ]
                }
                logging.debug(f"DEBUG: Created GeoJSON dict with {len(geojson_dict['features'])} features (with hover tooltips)")
            except Exception as e:
                logging.debug(f"DEBUG: Error creating GeoJSON dict: {e}")
                import traceback
                traceback.print_exc()
                raise

            # Create GeoJSON layer
            try:
                geo_json_layer = GeoJSON(
                    data=geojson_dict,
                    style_callback=style_callback,
                    hover_style={
                        'fillOpacity': 0.5,      # More visible fill on hover
                        'fillColor': '#FFD700',  # Golden yellow for prominence
                        'color': '#FFA500',      # Orange border for contrast
                        'weight': 5,             # Thicker border for "pop out" effect
                        'opacity': 1             # Full opacity for boldness
                    },
                    name='Grid Cells'
                )
                logging.debug("DEBUG: Created GeoJSON layer")
            except Exception as e:
                logging.debug(f"DEBUG: Error creating GeoJSON layer: {e}")
                import traceback
                traceback.print_exc()
                raise

            # Handle click events on GeoJSON layer
            def handle_click(**kwargs):
                """Handle clicks on gridbox polygons - ONLY when actively on Step 1"""
                try:
                    feature = kwargs.get('feature', {})
                    grid_id = feature.get('id', '')
                    
                    if not grid_id:
                        logging.debug("DEBUG: Click event had no grid ID")
                        return
                    
                    logging.debug(f"DEBUG: Gridbox clicked: {grid_id}")
                    
                    # Check if we're in multi-plot mode - read from reactive value for consistency
                    try:
                        multi_mode = multi_plot_mode_enabled.get()
                        logging.debug(f"DEBUG: ipyleaflet handler - multi_mode from reactive value: {multi_mode}")
                    except Exception:
                        multi_mode = False
                    
                    if multi_mode:
                        # In multi-plot mode, ONLY allow clicks if Step 1 is NOT yet completed
                        # (i.e., user is actively on Step 1 for a new trace selection)
                        try:
                            step1_complete = multi_plot_step_1_completed.get()
                            if step1_complete:
                                logging.debug("DEBUG: Multi-plot mode - ignoring click because Step 1 is already complete (finish current selection or reset)")
                                return
                        except Exception:
                            pass
                        
                        # Update multi-plot grid selection
                        logging.debug(f"DEBUG: Multi-plot mode - setting multi_plot_grid_id to {grid_id}")
                        multi_plot_grid_id.set(grid_id)
                        ui.update_select("multi_plot_gridbox_selector", selected=f"Grid {grid_id}")
                        
                        # CRITICAL FIX: Clear loop-related data when clicking an individual gridbox
                        # This ensures loop filtering doesn't apply to individual gridbox selections
                        multi_plot_loop_available_variables.set(None)
                        multi_plot_loop_gridboxes.set(None)
                        multi_plot_loop_name.set(None)
                        multi_plot_loop_polygon.set(None)
                        logging.debug(f"DEBUG: Cleared multi-plot loop data from ipyleaflet map click - this is an individual gridbox selection")
                        
                        multi_plot_step_1_completed.set(True)
                        logging.debug(f"DEBUG: Multi-plot Step 1 completed via map click!")
                    else:
                        # In regular mode, ONLY allow clicks if Step 1 is NOT yet completed
                        # (i.e., user is actively on Step 1 of starting a new selection)
                        try:
                            step1_complete = dropdown_step_1_completed.get()
                            if step1_complete:
                                logging.debug("DEBUG: Regular mode - ignoring click because Step 1 is already complete (finish current selection or use 'Reset Selection' button)")
                                return
                        except Exception:
                            pass
                        
                        # Update main grid selection
                        logging.debug(f"DEBUG: Regular mode - setting selected_grid_id to {grid_id}")
                        selected_grid_id.set(grid_id)
                        ui.update_select("gridbox_selector", selected=f"Grid {grid_id}")
                        
                        # CRITICAL FIX: Clear loop-related data when clicking an individual gridbox
                        # This ensures loop filtering doesn't apply to individual gridbox selections
                        selected_loop_available_variables.set(None)
                        selected_loop_gridboxes.set(None)
                        selected_loop_name.set(None)
                        selected_loop_polygon.set(None)
                        logging.debug(f"DEBUG: Cleared loop data from ipyleaflet map click - this is an individual gridbox selection")
                        
                        dropdown_step_1_completed.set(True)
                        logging.debug(f"DEBUG: Step 1 completed via map click!")
                    
                except Exception as e:
                    logging.debug(f"DEBUG: Error in gridbox click handler: {e}")
                    import traceback
                    traceback.print_exc()

            # Attach click handler to GeoJSON layer
            try:
                geo_json_layer.on_click(handle_click)
                logging.debug("DEBUG: Attached click handler to GeoJSON layer")
            except Exception as e:
                logging.debug(f"DEBUG: Error attaching click handler: {e}")
                import traceback
                traceback.print_exc()

            # Add hover handler to show grid ID on mouseover
            def handle_hover(**kwargs):
                """Handle hover events to show grid ID"""
                try:
                    feature = kwargs.get('feature', {})
                    properties = feature.get('properties', {})
                    grid_id = properties.get('grid_id', '')
                    
                    if grid_id:
                        hovered_grid_id.set(grid_id)
                except Exception as e:
                    logging.debug(f"DEBUG: Error in hover handler: {e}")
            
            def handle_mouseout(**kwargs):
                """Clear hover tooltip when mouse leaves"""
                try:
                    hovered_grid_id.set(None)
                except Exception as e:
                    logging.debug(f"DEBUG: Error in mouseout handler: {e}")
            
            try:
                geo_json_layer.on_hover(handle_hover)
                # Note: on_mouseout may not be available, so we'll use a timeout in the hover handler
                logging.debug("DEBUG: Attached hover handler to GeoJSON layer")
            except Exception as e:
                logging.debug(f"DEBUG: Error attaching hover handler: {e}")
                import traceback
                traceback.print_exc()

            # Add GeoJSON layer to map
            try:
                leaflet_map.add_layer(geo_json_layer)
                logging.debug(f"DEBUG: Added GeoJSON layer to map. Map now has {len(leaflet_map.layers)} layers")
            except Exception as e:
                logging.debug(f"DEBUG: Error adding GeoJSON layer to map: {e}")
                import traceback
                traceback.print_exc()
                raise

            # Add DrawControl for loop drawing
            # NOTE: DrawControl callbacks don't work in Shiny - using trait observation instead
            try:
                logging.debug("DEBUG: Creating DrawControl with trait observation...")
                draw_control = DrawControl(
                    polygon={
                        'shapeOptions': {
                            'color': '#6bc2e5',
                            'weight': 3,
                            'fillOpacity': 0.2
                        },
                        'repeatMode': False  # Don't automatically start drawing another shape
                    },
                    polyline={},  # Disable polyline
                    circle={},  # Disable circle
                    rectangle={},  # Disable rectangle
                    marker={},  # Disable marker
                    circlemarker={},  # Disable circle marker
                    edit=False  # Disable edit mode for now
                )
                
                # Use trait observation for Shiny compatibility
                def handle_draw_trait_change(change):
                    """Handle polygon drawing completion via trait observation"""
                    logging.debug("DEBUG: ===== DRAW EVENT TRIGGERED (TRAIT) =====")
                    logging.debug(f"DEBUG: Received change: {change}")
                    try:
                        # Check if we're in multi-plot mode
                        try:
                            multi_mode = input.multi_plot_mode()
                        except Exception:
                            multi_mode = False
                        
                        # CRITICAL: Only allow drawing during Step 1
                        # Check step completion status based on mode
                        if multi_mode:
                            # In Additional Traces mode - only allow if Step 1 is NOT complete
                            step1_complete = multi_plot_step_1_completed.get()
                            if step1_complete:
                                logging.debug("DEBUG: Draw ignored - Additional Traces mode Step 1 already complete")
                                logging.debug("DEBUG: User should only draw polygons when on Step 1")
                                return
                        else:
                            # In regular mode - only allow if Step 1 is NOT complete
                            step1_complete = dropdown_step_1_completed.get()
                            if step1_complete:
                                logging.debug("DEBUG: Draw ignored - Regular mode Step 1 already complete")
                                logging.debug("DEBUG: User should only draw polygons when on Step 1")
                                return
                        
                        # The change dict contains 'new' with the last drawn data
                        last_draw = change.get('new', {})
                        if not last_draw:
                            logging.debug("DEBUG: No draw data in change")
                            return
                        
                        action = last_draw.get('type', '')
                        geo_json = last_draw.get('geometry', {})
                        
                        logging.debug(f"DEBUG: Draw event - action/type: {action}")
                        logging.debug(f"DEBUG: geo_json type: {type(geo_json)}")
                        
                        # Check if this is a polygon
                        if geo_json.get('type') == 'Polygon':
                            # Extract polygon coordinates
                            coords = geo_json.get('coordinates', [[]])[0]  # Get outer ring
                            if not coords or len(coords) < 3:
                                logging.debug(f"DEBUG: Invalid polygon coordinates: {coords}")
                                return
                            
                            logging.debug(f"DEBUG: Polygon drawn with {len(coords)} points")
                            
                            # Show loading indicator
                            polygon_processing.set(True)
                            logging.debug("DEBUG: â³ Polygon processing started - showing loading indicator")
                            
                            try:
                                # Calculate gridbox intersections
                                intersections = _calculate_gridbox_intersections(coords, grid_geojson_data)
                                
                                if len(intersections) == 0:
                                    # No gridboxes selected - show error via reactive value
                                    logging.debug("DEBUG: No gridboxes intersected by drawn loop")
                                    # Set a status message that the UI can display
                                    multi_plot_status_message.set("âš ï¸ No gridboxes were drawn over. Please try again.")
                                    polygon_processing.set(False)
                                    return
                                
                                # Store the polygon and intersections temporarily
                                # User will be prompted to name the loop via a modal
                                if multi_mode:
                                    multi_plot_loop_polygon.set(coords)
                                    multi_plot_loop_gridboxes.set(intersections)
                                    logging.debug(f"DEBUG: Multi-plot mode - stored loop with {len(intersections)} gridboxes")
                                    
                                    # Check which variables are available across ALL gridboxes in this loop
                                    available_vars = _get_variables_available_in_loop(
                                        intersections,
                                        reactive_netcdf_filepaths.get(),
                                        _cached_datasets.get()
                                    )
                                    multi_plot_loop_available_variables.set(available_vars)
                                    logging.debug(f"DEBUG: Multi-plot mode - available variables for loop: {sorted(available_vars)}")
                                else:
                                    selected_loop_polygon.set(coords)
                                    selected_loop_gridboxes.set(intersections)
                                    logging.debug(f"DEBUG: Regular mode - stored loop with {len(intersections)} gridboxes")
                                    
                                    # Check which variables are available across ALL gridboxes in this loop
                                    available_vars = _get_variables_available_in_loop(
                                        intersections,
                                        reactive_netcdf_filepaths.get(),
                                        _cached_datasets.get()
                                    )
                                    selected_loop_available_variables.set(available_vars)
                                    logging.debug(f"DEBUG: Regular mode - available variables for loop: {sorted(available_vars)}")
                                
                                # Processing complete - hide loading indicator
                                polygon_processing.set(False)
                                logging.debug("DEBUG: âœ… Polygon processing complete - hiding loading indicator")
                            except Exception as proc_error:
                                logging.debug(f"DEBUG: Error during polygon processing: {proc_error}")
                                polygon_processing.set(False)
                                raise
                            
                            # Trigger modal to appear for loop naming
                            # This will be handled by a reactive effect that watches for polygon changes
                    
                    except Exception as e:
                        logging.debug(f"DEBUG: Error in handle_draw_trait_change: {e}")
                        import traceback
                        traceback.print_exc()
                
                # Observe changes to last_draw trait
                draw_control.observe(handle_draw_trait_change, 'last_draw')
                leaflet_map.add_control(draw_control)
                logging.debug("DEBUG: Added DrawControl to map with trait observation")
            
            except Exception as e:
                logging.debug(f"DEBUG: Error adding DrawControl: {e}")
                import traceback
                traceback.print_exc()
                # Continue without DrawControl if it fails

            logging.debug("DEBUG: ipyleaflet gridbox map created successfully")
            return leaflet_map
            
        except Exception as e:
            logging.debug(f"DEBUG: Error creating gridbox_map: {e}")
            import traceback
            traceback.print_exc()
            # Return fallback blank map
            fallback_map = Map(
                center=((SB_LAT_MIN + SB_LAT_MAX) / 2, (SB_LON_MIN + SB_LON_MAX) / 2),
                zoom=9,
                scroll_wheel_zoom=True,
                basemap={
                    'url': 'https://{s}.tile.opentopomap.org/{z}/{x}/{y}.png',
                    'max_zoom': 17,
                    'attribution': '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }
            )
            fallback_map.layout.height = '500px'
            fallback_map.layout.width = '100%'
            return fallback_map







    def _render_points_map(points, title):
        fig = go.Figure()
        if not points:
            fig.add_annotation(text=f"No points detected for {title}", x=0.5, y=0.5, showarrow=False)
        else:
            lats = [p[0] for p in points]
            lons = [p[1] for p in points]
            # adjust 0..360 to -180..180 if needed
            lons = [lon-360 if lon>180 else lon for lon in lons]
            fig.add_trace(go.Scattermapbox(
                lat=lats,
                lon=lons,
                mode='markers',
                marker=dict(size=6, color='#1f77b4', opacity=0.8),
                name=title,
                hovertemplate="Lat: %{lat}<br>Lon: %{lon}<extra></extra>"
            ))
        # Auto-center/zoom to data (no Santa Barbara bounds)
        if points:
            lat_min, lat_max = min(lats), max(lats)
            lon_min, lon_max = min(lons), max(lons)
            center_lat = (lat_min + lat_max) / 2.0
            center_lon = (lon_min + lon_max) / 2.0
            # Approximate zoom based on extent
            lat_span = max(1e-6, lat_max - lat_min)
            lon_span = max(1e-6, lon_max - lon_min)
            extent = max(lat_span, lon_span)
            if extent > 40:
                zoom = 2
            elif extent > 20:
                zoom = 3
            elif extent > 10:
                zoom = 4
            elif extent > 5:
                zoom = 5
            elif extent > 2:
                zoom = 6
            elif extent > 1:
                zoom = 7
            elif extent > 0.5:
                zoom = 8
            else:
                zoom = 9
        else:
            center_lat = (SB_LAT_MIN + SB_LAT_MAX) / 2
            center_lon = (SB_LON_MIN + SB_LON_MAX) / 2
            zoom = 9

        fig.update_layout(
            mapbox_style="open-street-map",
            mapbox_center={"lat": center_lat, "lon": center_lon},
            mapbox_zoom=zoom,
            margin={"r":0,"t":0,"l":0,"b":0},
            height=600
        )
        return fig
    @output
    @render_widget
    def selected_gridbox_plot():
        """Main plot function - handles single plots and multi-plot combinations"""
        # React to additions to the main plot from multi-plot UI
        try:
            _ = main_plot_update_trigger.get()
        except Exception:
            pass
        
        logging.debug("DEBUG: selected_gridbox_plot FUNCTION CALLED")
        logging.debug(f"DEBUG: plot_update_trigger value: {plot_update_trigger.get()}")
        logging.debug(f"DEBUG: dropdown_step_5_completed value: {dropdown_step_5_completed.get()}")
        
        # Add timeout protection
        def timeout_handler(signum, frame):
            raise TimeoutError("Plot generation timed out")
        # Set a 180-second timeout (3 minutes) to handle large polygons with many gridboxes
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(180)
        try:
            # Get the current selections reactively
            selections = get_plot_selections()
            variable = selections['variable']
            model = selections['model']
            frequency = selections['frequency']
            ssp = selections['ssp']
            grid_indices = selections['grid_indices']
            lat_idx, lon_idx = grid_indices if grid_indices else (None, None)
            
            # Extract loop data if present
            loop_gridboxes = selections.get('loop_gridboxes')
            loop_name = selections.get('loop_name')
            loop_polygon = selections.get('loop_polygon')
            is_loop_selection = loop_gridboxes is not None and len(loop_gridboxes) > 0
            
            logging.debug(f"DEBUG: Plot function received - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
            logging.debug(f"DEBUG: Grid indices: {grid_indices}, lat_idx: {lat_idx}, lon_idx: {lon_idx}")
            logging.debug(f"DEBUG: Step completion status:")
            logging.debug(f"DEBUG:   Step 1: {dropdown_step_1_completed.get()}")
            logging.debug(f"DEBUG:   Step 2: {dropdown_step_2_completed.get()}")
            logging.debug(f"DEBUG:   Step 3: {dropdown_step_3_completed.get()}")
            logging.debug(f"DEBUG:   Step 4: {dropdown_step_4_completed.get()}")
            logging.debug(f"DEBUG:   Step 5: {dropdown_step_5_completed.get()}")
            logging.debug(f"DEBUG: Selected grid ID: {selected_grid_id.get()}")
            
            multi_mode = multi_plot_mode_enabled.get()
            logging.debug(f"DEBUG: Multi-plot mode detected: {multi_mode}")

            # Check if all required values are available (observations skip model/ssp)
            is_observed_main = variable in ("obs_tmin", "obs_tmax", "obs_pr")
            if not variable or not frequency or (not is_observed_main and (not model or not ssp)):
                logging.debug(f"DEBUG: Missing required values - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
                fig = go.Figure()
                fig.add_annotation(text="Please complete all selections to view the plot.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                signal.alarm(0)  # Cancel timeout
                return fig

            # Check if grid indices are available (or if we have a loop selection)
            if (lat_idx is None or lon_idx is None) and not is_loop_selection:
                logging.debug(f"DEBUG: No grid indices or loop selection available - lat_idx: {lat_idx}, lon_idx: {lon_idx}, is_loop: {is_loop_selection}")
                # Don't auto-select grid cell - user must select explicitly
                if selected_grid_id.get() is None and not is_loop_selection:
                    logging.debug(f"DEBUG: No grid or loop selected - user must select explicitly")
                    # Don't try to auto-select - just show error message
                # If still no grid indices or loop, show error
                if (lat_idx is None or lon_idx is None) and not is_loop_selection:
                    logging.debug(f"DEBUG: No grid indices or loop available - user must select a grid cell or draw a loop")
                    fig = go.Figure()
                    fig.add_annotation(text="Please select a grid cell on the map or draw a region loop.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                    fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                    signal.alarm(0)  # Cancel timeout
                    return fig

            filepaths_map = reactive_netcdf_filepaths.get()
            logging.debug(f"DEBUG: Filepaths map has {len(filepaths_map) if filepaths_map else 0} entries")
            
            # Resolve file path for projections; observations handled separately
            if not is_observed_main:
                file_key = (variable, model, ssp)
                logging.debug(f"DEBUG: Looking for file key: {file_key}")
                if file_key not in filepaths_map:
                    logging.debug(f"DEBUG: File key not found in filepaths_map")
                    available_keys = list(filepaths_map.keys())[:5] if filepaths_map else []
                    logging.debug(f"DEBUG: Available keys (first 5): {available_keys}")
                    fig = go.Figure()
                    fig.add_annotation(text=f"No data file found for {variable}, {model}, {ssp}<br>Available combinations: {len(filepaths_map)}", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                    fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                    signal.alarm(0)  # Cancel timeout
                    return fig
                file_path = filepaths_map[file_key]
                logging.debug(f"DEBUG: Found file path: {file_path}")
            else:
                # For observed data, we don't need a file path from filepaths_map
                file_path = None
                logging.debug(f"DEBUG: Processing observed data - no file path needed")
            
            fig = go.Figure()

            # Determine what combinations to plot
            plot_combinations = []
            # FIXED: Include grid coordinates in combo key to prevent overwriting
            # Format: (var, model, freq, ssp, lat_idx, lon_idx)
            
            # Always start with the main selection
            main_combo = (variable, (model if not is_observed_main else 'Observed'), frequency, (ssp if not is_observed_main else 'historical'), lat_idx, lon_idx)
            plot_combinations.append(main_combo)
            logging.debug(f"DEBUG: Main combo: {main_combo[:4]} at grid ({lat_idx},{lon_idx})")
            
            # FIXED: Always include additional traces from main_plot_additional_traces
            try:
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent_traces = list(main_plot_persistent_traces.get() or [])
                
                # Combine current and persistent traces, avoiding duplicates
                all_traces = current_traces + persistent_traces
                seen_traces = set()
                unique_extra_traces = []
                
                for trace_cfg in all_traces:
                    trace_key = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'), trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                    if trace_key not in seen_traces:
                        seen_traces.add(trace_key)
                        unique_extra_traces.append(trace_cfg)
                
                if unique_extra_traces:
                    logging.debug(f"DEBUG: Found {len(unique_extra_traces)} additional traces to include in plot combinations")
                    
                    # Build a list to track loop gridboxes already in plot_combinations (including main plot)
                    # This will be used to compare against new loop traces
                    existing_loop_gridboxes_list = []
                    
                    # Add main plot's loop if it exists
                    main_loop_gridboxes = selected_loop_gridboxes.get()
                    if main_loop_gridboxes:
                        existing_loop_gridboxes_list.append({
                            'gridboxes': sorted(main_loop_gridboxes, key=lambda x: x[0]),
                            'var': variable,
                            'model': model,
                            'freq': frequency,
                            'ssp': ssp
                        })
                    
                    # Add additional traces to plot combinations
                    for trace_cfg in unique_extra_traces:
                        trace_var = trace_cfg.get('variable')
                        trace_model = trace_cfg.get('model')
                        trace_ssp = trace_cfg.get('ssp')
                        trace_freq = trace_cfg.get('frequency', 'annual')
                        trace_lat_idx = trace_cfg.get('lat_idx')
                        trace_lon_idx = trace_cfg.get('lon_idx')
                        trace_loop_gridboxes = trace_cfg.get('loop_gridboxes')
                        
                        # FIXED: Include grid coordinates in combo tuple
                        trace_combo = (trace_var, trace_model, trace_freq, trace_ssp, trace_lat_idx, trace_lon_idx)
                        
                        # Check unit compatibility before adding
                        if trace_var and check_unit_compatibility(variable, trace_var, filepaths_map, frequency, trace_freq):
                            # Check if this trace is already in plot_combinations
                            already_in_plot = False
                            
                            if trace_lat_idx is None and trace_lon_idx is None and trace_loop_gridboxes:
                                # This is a loop trace - check against all existing loops in plot_combinations
                                trace_sorted = sorted(trace_loop_gridboxes, key=lambda x: x[0])
                                
                                for existing_loop in existing_loop_gridboxes_list:
                                    if (existing_loop['var'] == trace_var and 
                                        existing_loop['model'] == trace_model and
                                        existing_loop['freq'] == trace_freq and
                                        existing_loop['ssp'] == trace_ssp and
                                        existing_loop['gridboxes'] == trace_sorted):
                                        already_in_plot = True
                                        logging.debug(f"DEBUG: Skipping duplicate loop: {trace_var} - same {len(trace_sorted)} gridboxes already in plot")
                                        break
                                
                                if not already_in_plot:
                                    logging.debug(f"DEBUG: Different loop area detected: {trace_var} with {len(trace_sorted)} gridboxes")
                            else:
                                # Regular gridbox trace - check coordinates
                                if trace_combo in plot_combinations:
                                    already_in_plot = True
                                    logging.debug(f"DEBUG: Skipping duplicate gridbox trace: {trace_var} at ({trace_lat_idx},{trace_lon_idx})")
                            
                            if not already_in_plot:
                                plot_combinations.append(trace_combo)
                                logging.debug(f"DEBUG: Added additional trace to plot combinations: {trace_var}, {trace_model}, {trace_freq}, {trace_ssp} at grid ({trace_lat_idx},{trace_lon_idx})")
                                
                                # If this was a loop, add it to the tracking list so subsequent traces can check against it
                                if trace_lat_idx is None and trace_lon_idx is None and trace_loop_gridboxes:
                                    existing_loop_gridboxes_list.append({
                                        'gridboxes': sorted(trace_loop_gridboxes, key=lambda x: x[0]),
                                        'var': trace_var,
                                        'model': trace_model,
                                        'freq': trace_freq,
                                        'ssp': trace_ssp
                                    })
                        else:
                            logging.debug(f"DEBUG: Skipping incompatible additional trace: {trace_var} ({trace_freq}) - incompatible with {variable} ({frequency})")
                else:
                    logging.debug(f"DEBUG: No additional traces found")
                    
            except Exception as e:
                logging.debug(f"DEBUG: Error processing additional traces for plot combinations: {e}")
            
            if multi_mode:
                logging.debug(f"DEBUG: Multi-plot mode enabled")
                
                # Preserve existing plots when multi-plot mode is enabled
                plots_list = multi_plots_list.get()
                if plots_list and multi_mode:
                    logging.debug(f"DEBUG: Preserving {len(plots_list)} stored multi-plots")
                    # Add compatible plots from multi_plots_list to plot_combinations
                    for plot_config in plots_list:
                        multi_var = plot_config.get('variable')
                        multi_model = plot_config.get('model')
                        multi_frequency = plot_config.get('frequency')
                        multi_ssp = plot_config.get('ssp')
                        
                        # Check unit compatibility before adding (with frequency check for precipitation)
                        if multi_var and check_unit_compatibility(variable, multi_var, filepaths_map, frequency, multi_frequency):
                            # FIXED: Check if the combo (without grid) already exists - multi_plots don't have grid stored
                            multi_combo_key = (multi_var, multi_model, multi_frequency, multi_ssp)
                            already_exists = any(combo[:4] == multi_combo_key for combo in plot_combinations)
                            if not already_exists:
                                # Add multi-plot at main grid (since multi_plots_list doesn't store grid coords)
                                multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
                                plot_combinations.append(multi_combo)
                                logging.debug(f"DEBUG: Adding compatible multi-plot: {multi_var}, {multi_model}, {multi_frequency}, {multi_ssp}")
                        else:
                            logging.debug(f"DEBUG: Skipping incompatible multi-plot: {multi_var} ({multi_frequency}) - incompatible with {variable} ({frequency})")
                
                # Check if multi-plot progressive system is completed
                multi_plot_completed = (multi_plot_step_1_completed.get() and 
                                      multi_plot_step_2_completed.get() and 
                                      multi_plot_step_3_completed.get() and 
                                      multi_plot_step_4_completed.get() and 
                                      multi_plot_step_5_completed.get() and
                                      multi_plot_step_6_completed.get())
                
                # Check if we have existing completed multi-plots that should be displayed
                plots_list = multi_plots_list.get()
                has_existing_plots = plots_list and len(plots_list) > 0
                
                # Also check if we have at least some multi-plot selections for new additions
                has_multi_selections = (multi_plot_variable.get() and 
                                      multi_plot_model.get() and 
                                      multi_plot_frequency.get() and
                                      multi_plot_ssp.get())
                
                logging.debug(f"DEBUG: Multi-plot completion status: {multi_plot_completed}")
                logging.debug(f"DEBUG: Multi-plot step completion: {multi_plot_step_1_completed.get()}, {multi_plot_step_2_completed.get()}, {multi_plot_step_3_completed.get()}, {multi_plot_step_4_completed.get()}, {multi_plot_step_5_completed.get()}, {multi_plot_step_6_completed.get()}")
                logging.debug(f"DEBUG: Has existing plots: {has_existing_plots}, Has multi-selections: {has_multi_selections}")
                
                # If we have existing plots, include them in the main plot
                if has_existing_plots:
                    logging.debug(f"DEBUG: Found {len(plots_list)} stored multi-plots - including them in main plot")
                    for plot_config in plots_list:
                        multi_var = plot_config.get('variable')
                        multi_model = plot_config.get('model')
                        multi_frequency = plot_config.get('frequency')
                        multi_ssp = plot_config.get('ssp')
                        
                        # Check unit compatibility before adding (with frequency check for precipitation)
                        if multi_var and check_unit_compatibility(variable, multi_var, filepaths_map, frequency, multi_frequency):
                            # FIXED: Check if the combo (without grid) already exists
                            multi_combo_key = (multi_var, multi_model, multi_frequency, multi_ssp)
                            already_exists = any(combo[:4] == multi_combo_key for combo in plot_combinations)
                            if not already_exists:
                                # Add existing multi-plot at main grid
                                multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
                                plot_combinations.append(multi_combo)
                                logging.debug(f"DEBUG: Adding existing multi-plot to main plot: {multi_var}, {multi_model}, {multi_frequency}, {multi_ssp}")
                        else:
                            logging.debug(f"DEBUG: Skipping incompatible existing plot: {multi_var} ({multi_frequency}) - incompatible with {variable} ({frequency})")
                
                if has_multi_selections:
                    # Get multi-plot selections
                    multi_var = multi_plot_variable.get()
                    multi_model = multi_plot_model.get()
                    multi_frequency = multi_plot_frequency.get()
                    multi_ssp = multi_plot_ssp.get()
                    multi_action = multi_plot_action.get()
                    
                    logging.debug(f"DEBUG: Multi-plot selections - var: {multi_var}, model: {multi_model}, frequency: {multi_frequency}, ssp: {multi_ssp}, action: {multi_action}")
                    
                    if multi_var and multi_model and multi_frequency and multi_ssp:
                        multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp)
                        
                        # Check unit compatibility (with frequency check for precipitation)
                        is_compatible = check_unit_compatibility(multi_var, variable, filepaths_map, multi_frequency, frequency)
                        logging.debug(f"DEBUG: Unit compatibility check (with freq) - {multi_var} ({multi_frequency}) vs {variable} ({frequency}): {is_compatible}")
                        
                        # Store the multi-plot info for later use in plotting
                        multi_plot_info = {
                            'combo': multi_combo,
                            'is_compatible': is_compatible,
                            'action': multi_action
                        }
                        
                        # Check unit compatibility and user action
                        if not is_compatible:
                            # Incompatible units - always separate plots
                            logging.debug(f"DEBUG: Incompatible units - main plot shows only main selection")
                        elif multi_action == "new_plot":
                            # User wants separate plot - main plot should only show main selection
                            # Additional plot will be handled by additional_plot function
                            logging.debug(f"DEBUG: User wants separate plot - main plot shows only main selection")
                        else:
                            # Compatible units and user wants same plot - handled by multi-plot UI, not the main plot
                            logging.debug(f"DEBUG: Compatible units, but main plot will not combine; use multi-plot UI instead")
                    else:
                        logging.debug(f"DEBUG: Multi-plot selections incomplete - var: {multi_var}, model: {multi_model}, ssp: {multi_ssp}")
                        multi_plot_info = None
                else:
                    logging.debug(f"DEBUG: No multi-plot selections found")
                    multi_plot_info = None
                
                # Check if we should include current multi-plot selection in main plot
                multi_plot_included = False
                if (multi_plot_info and multi_plot_info['is_compatible'] and 
                    multi_action != "new_plot" and multi_var and multi_model and multi_frequency and multi_ssp):
                    # User wants to combine on same plot - check if it's already in additional traces
                    current_multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp)
                    
                    # Check if this selection already exists in additional traces (include grid in comparison)
                    already_in_additional = False
                    main_extra_traces = list(main_plot_additional_traces.get() or [])
                    # Resolve grids for comparison
                    multi_grid_id = multi_plot_grid_id.get()
                    main_grid_id = get_selected_grid_indices()
                    try:
                        multi_lat, multi_lon = map(int, multi_grid_id.split(",")) if multi_grid_id else (None, None)
                    except Exception:
                        multi_lat, multi_lon = (None, None)
                    for trace_cfg in main_extra_traces:
                        trace_combo = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'))
                        trace_grid = (trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                        if trace_combo == current_multi_combo and trace_grid == (multi_lat, multi_lon):
                            already_in_additional = True
                            logging.debug(f"DEBUG: Current multi-plot selection already exists in additional traces at same grid: {current_multi_combo} @ {trace_grid}")
                            break
                    
                    # FIXED: Check if combo+grid exists in plot_combinations
                    current_multi_combo_with_grid = (multi_var, multi_model, multi_frequency, multi_ssp, multi_lat, multi_lon)
                    same_combo_in_main = current_multi_combo_with_grid in plot_combinations

                    if same_combo_in_main or already_in_additional:
                        logging.debug("DEBUG: Skipping current multi-plot selection - identical to existing plotted series (combo + grid)")
                        multi_plot_included = False
                    else:
                        # If different grid than main, include; otherwise, let additional traces manage
                        # Do not include current multi-plot selection directly in plot_combinations
                        # Different-grid selections should be handled via additional traces to use their grid indices
                        multi_plot_included = False
                        logging.debug("DEBUG: Not including current multi-plot selection in plot_combinations; will be handled as additional trace")
                
                # FIXED: Only process additional traces if the current multi-plot selection wasn't already included
                # to prevent duplication
                if multi_plot_included:
                    logging.debug(f"DEBUG: Skipping additional traces processing - current multi-plot selection already included in plot_combinations")
                else:
                    logging.debug(f"DEBUG: Additional traces processing will handle any remaining multi-plot selections")
                
                # Check if we have additional traces with different frequencies for the same variable
                # FIXED: Only process additional traces if the current multi-plot selection wasn't already included
                additional_traces = []
                added_loop_traces = []  # Track which loop traces we've already added by their gridboxes
                try:
                    main_extra_traces = list(main_plot_additional_traces.get() or [])
                    for trace_cfg in main_extra_traces:
                        trace_var = trace_cfg.get('variable')
                        trace_model = trace_cfg.get('model')
                        trace_ssp = trace_cfg.get('ssp')
                        trace_freq = trace_cfg.get('frequency', 'annual')  # Default to annual if not specified
                        trace_lat_idx = trace_cfg.get('lat_idx')
                        trace_lon_idx = trace_cfg.get('lon_idx')
                        trace_loop_gridboxes = trace_cfg.get('loop_gridboxes')
                        trace_loop_name = trace_cfg.get('loop_name')
                        
                        # FIXED: For loops, also check if gridboxes match to distinguish different loop areas
                        already_in_plot = False
                        for combo_var, combo_model, combo_freq, combo_ssp, combo_lat, combo_lon in plot_combinations:
                            # Check if basic parameters match
                            if (combo_var == trace_var and combo_model == trace_model and 
                                combo_freq == trace_freq and combo_ssp == trace_ssp):
                                
                                # For gridbox traces (not loops), check coordinates
                                if trace_lat_idx is not None and trace_lon_idx is not None:
                                    if combo_lat == trace_lat_idx and combo_lon == trace_lon_idx:
                                        already_in_plot = True
                                        logging.debug(f"DEBUG: Skipping additional trace - gridbox already in plot: {trace_var} @ ({trace_lat_idx},{trace_lon_idx})")
                                        break
                                
                                # For loop traces (both are None,None), check if gridboxes match
                                elif trace_lat_idx is None and trace_lon_idx is None and combo_lat is None and combo_lon is None:
                                    # This trace is a loop AND the combo is a loop
                                    # Need to check if they're the same loop by comparing gridboxes
                                    # The main plot loop info is in selected_loop_gridboxes
                                    combo_loop_gridboxes = selected_loop_gridboxes.get()
                                    
                                    if trace_loop_gridboxes and combo_loop_gridboxes:
                                        # Compare gridbox sets (sorted for consistent comparison)
                                        trace_sorted = sorted(trace_loop_gridboxes, key=lambda x: x[0])
                                        combo_sorted = sorted(combo_loop_gridboxes, key=lambda x: x[0])
                                        
                                        if trace_sorted == combo_sorted:
                                            already_in_plot = True
                                            logging.debug(f"DEBUG: Skipping additional trace - loop already in plot: {trace_var} ({len(trace_sorted)} gridboxes)")
                                            break
                                        else:
                                            logging.debug(f"DEBUG: Different loop areas - main plot: {len(combo_sorted)} gridboxes, this trace: {len(trace_sorted)} gridboxes")
                                    elif not trace_loop_gridboxes and not combo_loop_gridboxes:
                                        # Both loops but no gridbox data - treat as duplicate to be safe
                                        already_in_plot = True
                                        logging.debug(f"DEBUG: Skipping additional trace - both loops without gridbox data")
                                        break
                        
                        if already_in_plot:
                            continue
                        
                        # FIXED: For loop traces, also check against previously added loop traces
                        if trace_lat_idx is None and trace_lon_idx is None and trace_loop_gridboxes:
                            trace_sorted = sorted(trace_loop_gridboxes, key=lambda x: x[0])
                            for added_loop in added_loop_traces:
                                if trace_sorted == added_loop:
                                    already_in_plot = True
                                    logging.debug(f"DEBUG: Skipping additional trace - loop '{trace_loop_name}' already added to additional_traces ({len(trace_sorted)} gridboxes)")
                                    break
                        
                        if already_in_plot:
                            continue
                        
                        # FIXED: Include if units are compatible (allow different variables)
                        # Pass frequency information to properly check precipitation unit compatibility
                        if check_unit_compatibility(variable, trace_var, filepaths_map, frequency, trace_freq):
                            trace_combo = (trace_var, trace_model, trace_freq, trace_ssp, trace_lat_idx, trace_lon_idx)
                            additional_traces.append(trace_combo)
                            logging.debug(f"DEBUG: Adding additional trace: {trace_var} ({trace_model}, {trace_freq}, {trace_ssp}) @ ({trace_lat_idx},{trace_lon_idx})")
                            
                            # Track this loop to avoid adding it again
                            if trace_lat_idx is None and trace_lon_idx is None and trace_loop_gridboxes:
                                added_loop_traces.append(sorted(trace_loop_gridboxes, key=lambda x: x[0]))
                                logging.debug(f"DEBUG: Tracked loop '{trace_loop_name}' with {len(trace_loop_gridboxes)} gridboxes to avoid duplicates")
                        else:
                            logging.debug(f"DEBUG: Skipping incompatible trace: {trace_var} (units not compatible with {variable} - freq: {frequency} vs {trace_freq})")
                except Exception as e:
                    logging.debug(f"DEBUG: Error processing additional traces: {e}")
                
                # FIXED: Add additional traces to plot combinations so they get processed and plotted
                # Additional traces have their own grid coordinates and should be processed independently
                logging.debug(f"DEBUG: Found {len(additional_traces)} additional traces to process separately")
                
                # Add additional traces to plot_combinations
                for add_trace in additional_traces:
                    if add_trace not in plot_combinations:
                        plot_combinations.append(add_trace)
                        logging.debug(f"DEBUG: Added additional trace to plot_combinations: {add_trace}")

            else:
                # Single plot mode: include both observed and projected versions if available
                plot_combinations = []
                
                # Add the main selection with grid coordinates
                main_combo = (variable, (model if not is_observed_main else 'Observed'), frequency, (ssp if not is_observed_main else 'historical'), lat_idx, lon_idx)
                plot_combinations.append(main_combo)
                
                # Note: Removed automatic addition of observed/projected counterparts
                # Users should explicitly select what they want to plot
                
                logging.debug(f"DEBUG: Single plot mode - created {len(plot_combinations)} combinations for variable {variable}")

            logging.debug(f"DEBUG: Plot combinations to process: {plot_combinations}")

            # Load and plot data for each combination
            cached_datasets = _cached_datasets.get()
            plotted_data = []
            # No limit on combinations - allow unlimited traces as long as units are compatible
            logging.debug(f"DEBUG: Processing {len(plot_combinations)} plot combinations (no limit)")
            trace_index = 0  # Track which trace we're processing (0 = main, 1+ = additional)
            for var, mod, freq, scenario, combo_lat_idx, combo_lon_idx in plot_combinations:
                file_key = (var, mod, scenario)
                combo_key = (var, mod, freq, scenario, combo_lat_idx, combo_lon_idx)
                # Grid coordinates are now directly in the combination tuple
                logging.debug(f"DEBUG: Processing combination: {file_key} with frequency: {freq} at grid ({combo_lat_idx},{combo_lon_idx})")
                
                # Load dataset if not cached
                ds = cached_datasets.get(file_key)
                if ds is None:
                    if var in ("obs_tmin","obs_tmax","obs_pr"):
                        # Load observed dataset via discovered paths
                        try:
                            if var == "obs_pr":
                                obs_fp = obs_prec_path.get()
                            elif var == "obs_tmax":
                                obs_fp = obs_tmax_path.get()
                            else:
                                obs_fp = obs_tmin_path.get()
                        except Exception:
                            obs_fp = None
                        if not obs_fp or not os.path.exists(obs_fp):
                            logging.debug(f"DEBUG: Observed dataset path missing for {var}")
                            continue
                        try:
                            with xr.open_dataset(obs_fp, engine="netcdf4") as temp_ds:
                                ds = temp_ds.load()
                            current_cache = _cached_datasets.get()
                            current_cache[file_key] = ds
                            _cached_datasets.set(current_cache)
                            logging.debug(f"DEBUG: Loaded observed dataset for {var}")
                        except Exception as e:
                            logging.debug(f"DEBUG: Error loading observed dataset {var}: {e}")
                            continue
                    else:
                        # FIXED: Load projection data - keep all logic inside else block
                        if file_key not in filepaths_map:
                            logging.debug(f"DEBUG: File key not in filepaths_map: {file_key}")
                            continue
                        file_path = filepaths_map[file_key]
                        logging.debug(f"DEBUG: Loading dataset from: {file_path}")
                        try:
                            with xr.open_dataset(file_path, engine="netcdf4") as temp_ds:
                                ds = temp_ds.load()
                            current_cache = _cached_datasets.get()
                            if len(current_cache) > 10:
                                oldest_keys = list(current_cache.keys())[:-10]
                                for old_key in oldest_keys:
                                    del current_cache[old_key]
                                logging.debug(f"DEBUG: Removed {len(oldest_keys)} old datasets from cache")
                            current_cache[file_key] = ds
                            _cached_datasets.set(current_cache)
                            logging.debug(f"DEBUG: Successfully loaded and cached dataset")
                        except Exception as e:
                            logging.debug(f"DEBUG: Error loading dataset: {e}")
                            continue

                if ds is None:
                    logging.debug(f"DEBUG: Dataset is None for {file_key}")
                    continue

                # Check if this is a loop-based regional average BEFORE processing grid coordinates
                # For main combo, check is_loop_selection
                # For additional traces, check trace config
                combo_is_loop = False
                combo_loop_gridboxes = None
                combo_loop_name = None
                
                # CRITICAL FIX: Use trace_index to distinguish main plot from additional traces
                # trace_index == 0 means this is the main plot
                # trace_index > 0 means this is an additional trace
                
                if trace_index == 0:
                    # This is the MAIN PLOT - check if it's a loop
                    if (var, mod, freq, scenario) == (variable, (model if not is_observed_main else 'Observed'), frequency, (ssp if not is_observed_main else 'historical')):
                        # FIXED: Only apply loop data if grid coordinates ALSO match (both None for main loop)
                        if is_loop_selection and combo_lat_idx is None and combo_lon_idx is None:
                            combo_is_loop = True
                            combo_loop_gridboxes = loop_gridboxes
                            combo_loop_name = loop_name
                            logging.debug(f"DEBUG: Main combo is a loop selection: {combo_loop_name}")
                        elif combo_lat_idx is not None and combo_lon_idx is not None:
                            logging.debug(f"DEBUG: Main combo match but this is a gridbox trace at ({combo_lat_idx},{combo_lon_idx}), not a loop")
                else:
                    # This is an ADDITIONAL TRACE - look it up in main_plot_additional_traces
                # CRITICAL: Must match BOTH the variable/model/freq/ssp AND the grid coordinates
                # to avoid incorrectly applying loop data to single-gridbox traces
                    main_extra_traces = list(main_plot_additional_traces.get() or [])
                    for trace_cfg in main_extra_traces:
                        # Match on variable, model, frequency, ssp
                        if (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp')) == (var, mod, freq, scenario):
                            # ALSO match on grid coordinates to ensure this is the SAME trace
                            # Loop traces have (None, None) for lat/lon, single gridbox traces have specific values
                            if (trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx')) == (combo_lat_idx, combo_lon_idx):
                                if trace_cfg.get('loop_gridboxes'):
                                    combo_is_loop = True
                                    combo_loop_gridboxes = trace_cfg.get('loop_gridboxes')
                                    combo_loop_name = trace_cfg.get('loop_name')
                                    logging.debug(f"DEBUG: Additional trace is a loop: {combo_loop_name} at grid ({combo_lat_idx},{combo_lon_idx})")
                                    break
                                elif combo_lat_idx is not None and combo_lon_idx is not None:
                                    logging.debug(f"DEBUG: Additional trace is a gridbox at ({combo_lat_idx},{combo_lon_idx})")
                                    break

                # Only process single-gridbox coordinate mapping if NOT a loop
                use_lat_idx = combo_lat_idx
                use_lon_idx = combo_lon_idx
                
                if not combo_is_loop:
                    if var in ("obs_tmin","obs_tmax","obs_pr"):
                        # Map selected projection grid center to most encompassing observed grid index
                        # FIXED: Use combo-specific grid coordinates to find the right projection gridbox
                        try:
                            # FIXED: Construct grid_id from combo-specific coordinates (this trace's grid)
                            grid_id_str = f"{combo_lat_idx},{combo_lon_idx}"
                            target_lat = target_lon = None
                            grid_geo = reactive_grid_geojson.get()
                            if grid_geo and grid_id_str:
                                features = grid_geo['features'] if isinstance(grid_geo, dict) else grid_geo.features
                                for feat in features:
                                    fid = feat.get('id') or (feat.get('properties', {}) or {}).get('id')
                                    if fid == grid_id_str:
                                        props = feat.get('properties', {}) if isinstance(feat, dict) else feat.properties
                                        target_lat = float(props.get('lat')) if props and 'lat' in props else None
                                        target_lon = float(props.get('lon')) if props and 'lon' in props else None
                                        break
                            
                            # Find the most encompassing observed grid cell
                            if target_lat is not None and target_lon is not None:
                                obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(target_lat, target_lon, ds)
                                if obs_lat_idx is not None and obs_lon_idx is not None:
                                    use_lat_idx, use_lon_idx = obs_lat_idx, obs_lon_idx
                                    logging.debug(f"DEBUG: Using most encompassing observed grid coordinates ({use_lat_idx}, {use_lon_idx}) for variable {var} from projection grid ({combo_lat_idx}, {combo_lon_idx})")
                                else:
                                    # Fallback to center of observed grid
                                    lat2d, lon2d, _, _ = _extract_spatial_coords(ds)
                                    if lat2d is not None and lon2d is not None:
                                        use_lat_idx = lat2d.shape[0] // 2
                                        use_lon_idx = lat2d.shape[1] // 2
                                        logging.debug(f"DEBUG: Using center of observed grid at ({use_lat_idx}, {use_lon_idx})")
                                    else:
                                        use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx
                            else:
                                # If we can't find target coordinates, this should not happen since
                                # the dropdown filtering should prevent this situation
                                logging.error(f"ERROR: No valid observational gridbox found for grid {grid_id_str} - this should have been filtered out in dropdown")
                                continue
                        except Exception as e:
                            logging.debug(f"DEBUG: Error mapping observed grid coordinates: {e}")
                            # FIXED: Use combo-specific coordinates as fallback
                            use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx
                    else:
                        # FIXED: Use combo-specific grid coordinates
                        if not (0 <= combo_lat_idx < ds['lat'].size and 0 <= combo_lon_idx < ds['lon'].size):
                            logging.debug(f"DEBUG: Indices out of bounds - combo_lat_idx: {combo_lat_idx}, combo_lon_idx: {combo_lon_idx}, ds lat size: {ds['lat'].size}, ds lon size: {ds['lon'].size}")
                            continue
                        use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx

                # Extract time series with error handling
                try:
                    logging.debug(f"DEBUG: Extracting time series for variable: {var}")
                    
                    # Extract data based on type (loop vs single gridbox)
                    if combo_is_loop and combo_loop_gridboxes:
                        # Use regional averaging for loop
                        logging.debug(f"DEBUG: Using regional averaging for loop: {combo_loop_name}")
                        df_raw = _extract_regional_average_timeseries(
                            combo_loop_gridboxes,
                            var,
                            mod,
                            freq,
                            scenario,
                            filepaths_map,
                            _cached_datasets.get(),
                            obs_gridbox_mappings=_obs_gridbox_mapping_cache  # Pass cache for performance
                        )
                        logging.debug(f"DEBUG: Regional average extracted, shape: {df_raw.shape}")
                    else:
                        # Standard single-gridbox extraction
                        if var in ("obs_tmin","obs_tmax","obs_pr"):
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            extract_var = data_vars[0] if data_vars else None
                            if not extract_var:
                                logging.debug("DEBUG: No data variable in observed dataset")
                                continue
                            df_raw = extract_timeseries(ds, use_lat_idx, use_lon_idx, variable_name=extract_var)
                            # Normalize column name to observed alias for downstream processing
                            if extract_var in df_raw.columns:
                                df_raw.rename(columns={extract_var: var}, inplace=True)
                        else:
                            df_raw = extract_timeseries(ds, use_lat_idx, use_lon_idx, variable_name=var)
                        logging.debug(f"DEBUG: Extracted time series shape: {df_raw.shape}")
                except Exception as e:
                    logging.debug(f"DEBUG: Error extracting time series: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

                if df_raw.empty or var not in df_raw.columns:
                    logging.debug(f"DEBUG: No data in extracted time series - empty: {df_raw.empty}, columns: {list(df_raw.columns)}")
                    continue

                # Process data with memory management
                try:
                    df_processed = df_raw.copy()
                    display_name = get_variable_display_name(var)
                    display_units = get_variable_metadata(var)["units"]
                    # Generate distinct colors for multiple plots
                    base_color = get_variable_metadata(var)["color"]
                    # Create a distinct color by modifying the base color based on the combination
                    import colorsys
                    try:
                        if base_color.startswith('#'):
                            rgb = tuple(int(base_color[i:i+2], 16) / 255.0 for i in (1, 3, 5))
                            h, s, v = colorsys.rgb_to_hsv(*rgb)
                            # Shift hue based on the combination to ensure distinctness
                            hue_shift = (hash(f"{var}_{mod}_{freq}_{scenario}") % 360) / 360.0
                            h = (h + hue_shift * 0.3) % 1.0  # Shift hue by up to 30%
                            s = min(1.0, s * 1.1)  # Slightly increase saturation
                            v = min(1.0, v * 0.9)  # Slightly darker
                            rgb_new = colorsys.hsv_to_rgb(h, s, v)
                            line_color = f"rgb({int(rgb_new[0]*255)}, {int(rgb_new[1]*255)}, {int(rgb_new[2]*255)})"
                        else:
                            line_color = base_color
                    except:
                        line_color = base_color

                    # Unit conversions (data conversion only - display_units already set from metadata)
                    if var in ["tasmin", "tasmax"]:
                        original_units = ds[var].attrs.get('units', '').lower()
                        if original_units in ['k', 'kelvin']:
                            df_processed[var] = (df_processed[var] - 273.15) * 9/5 + 32
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "pr":
                        original_units = ds[var].attrs.get('units', '').lower()
                        if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var in ("obs_tmin","obs_tmax"):
                        # Observed temperature to Â°F if needed
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        logging.debug(f"DEBUG: Observed temperature units: {src_units}")
                        
                        # Livneh observed data is in Celsius - always convert to Fahrenheit
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample value: {avg_value:.2f}")
                        
                        # Convert from Celsius to Fahrenheit for observed data
                        if src_units in ['c', 'degc', 'celsius', 'Â°c', 'deg_c', 'degrees_celsius']:
                            logging.debug(f"DEBUG: Units explicitly indicate Celsius - converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] * 9/5) + 32
                            logging.debug(f"DEBUG: Sample values after Celsius conversion: {df_processed[var].head().tolist()}")
                        elif src_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                            logging.debug(f"DEBUG: Units explicitly indicate Kelvin - converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] - 273.15) * 9/5 + 32
                            logging.debug(f"DEBUG: Sample values after Kelvin conversion: {df_processed[var].head().tolist()}")
                        elif src_units in ['f', 'degf', 'fahrenheit', 'Â°f', 'deg_f', 'degrees_fahrenheit']:
                            logging.debug(f"DEBUG: Units explicitly indicate Fahrenheit - no conversion needed")
                            # Data is already in Fahrenheit, no conversion needed
                        else:
                            # For Livneh observed data, if no units specified, assume Celsius (as confirmed by user)
                            logging.debug(f"DEBUG: No units specified - Livneh data is in Celsius, converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] * 9/5) + 32
                            logging.debug(f"DEBUG: Sample values after Celsius conversion: {df_processed[var].head().tolist()}")
                        # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "obs_pr":
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        if src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                            df_processed[var] = df_processed[var] / 25.4
                            logging.debug(f"DEBUG: Converted from mm/day to inches")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            logging.debug(f"DEBUG: Converted from kg m-2 s-1 to inches")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # For Livneh observed data, if no units specified, assume mm/day (as confirmed by user)
                            logging.debug(f"DEBUG: No units specified - Livneh precipitation data is in mm/day, converting to inches")
                            df_processed[var] = df_processed[var] / 25.4
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "pr":
                        # Projected precipitation conversion - typically in kg m-2 s-1
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample projected precipitation values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample projected precipitation value: {avg_value:.2f}")
                        logging.debug(f"DEBUG: Projected precipitation units: {src_units}")
                        
                        if src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            logging.debug(f"DEBUG: Converted projected precipitation from kg m-2 s-1 to inches")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                            df_processed[var] = df_processed[var] / 25.4
                            logging.debug(f"DEBUG: Converted projected precipitation from mm/day to inches")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume kg m-2 s-1 for projected precipitation
                            logging.debug(f"DEBUG: No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var in ["hursmax", "hursmin"]:
                        # Relative humidity conversion - typically in % but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        logging.debug(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['%', 'percent', 'pct']:
                            # Already in percentage, no conversion needed
                            logging.debug(f"DEBUG: {var} already in percentage - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                            # Convert from fraction to percentage
                            df_processed[var] = df_processed[var] * 100
                            logging.debug(f"DEBUG: Converted {var} from fraction to percentage")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume percentage
                            logging.debug(f"DEBUG: No units specified - assuming {var} is in percentage")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "huss":
                        # Specific humidity conversion - typically in kg/kg but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        logging.debug(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                            # Already in kg/kg, no conversion needed
                            logging.debug(f"DEBUG: {var} already in kg/kg - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['g/kg', 'g kg-1', 'g per kg']:
                            # Convert from g/kg to kg/kg
                            df_processed[var] = df_processed[var] / 1000
                            logging.debug(f"DEBUG: Converted {var} from g/kg to kg/kg")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume kg/kg
                            logging.debug(f"DEBUG: No units specified - assuming {var} is in kg/kg")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "rsds":
                        # Radiation - typically already in W/mÂ² (no conversion needed)
                        # NOTE: Climate model radiation data is standardized to W/mÂ² (watts per square meter)
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        logging.debug(f"DEBUG: {var} units: {src_units}")
                        
                        # Radiation is typically already in W/mÂ² - no conversion needed
                        # If data were in J/mÂ²/day (joules per square meter per day), we'd divide by 86400
                        # But standard climate models use W/mÂ² directly
                        logging.debug(f"DEBUG: {var} assumed to be in W/mÂ² (standard for climate models) - no conversion needed")
                        # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "wspeed":
                        # Wind speed conversion - typically in m/s but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        logging.debug(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        logging.debug(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        logging.debug(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['m/s', 'm s-1', 'meter/s', 'meter s-1', 'ms-1']:
                            # Already in m/s, no conversion needed
                            logging.debug(f"DEBUG: {var} already in m/s - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['km/h', 'kmh', 'km per hour']:
                            # Convert from km/h to m/s
                            df_processed[var] = df_processed[var] * 1000 / 3600
                            logging.debug(f"DEBUG: Converted {var} from km/h to m/s")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['knots', 'kt', 'kts']:
                            # Convert from knots to m/s
                            df_processed[var] = df_processed[var] * 0.514444
                            logging.debug(f"DEBUG: Converted {var} from knots to m/s")
                            logging.debug(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume m/s
                            logging.debug(f"DEBUG: No units specified - assuming {var} is in m/s")
                            # display_units already set from get_variable_metadata() - keep consistent

                    if var in df_processed.columns and not df_processed[var].isnull().all():
                        try:
                            # Process data based on selected frequency
                            if freq == 'daily':
                                # Use daily data as-is, but create a proper year column for filtering
                                df_freq = df_processed.copy()
                                df_freq["Year"] = df_freq["Date"].dt.year
                                # For daily data, we'll use the actual date for x-axis but filter by year
                                logging.debug(f"DEBUG: Using daily data with {len(df_freq)} rows")
                            elif freq == 'monthly':
                                # For precipitation, sum to get monthly totals; for other variables, average
                                if var in ["pr", "obs_pr"]:
                                    df_freq = df_processed.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                                    logging.debug(f"DEBUG: Created monthly total precipitation data with {len(df_freq)} rows")
                                else:
                                    df_freq = df_processed.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                                    logging.debug(f"DEBUG: Created monthly average data with {len(df_freq)} rows")
                                # Update units based on frequency
                                display_units = get_variable_units_for_frequency(var, freq)
                                df_freq["Year"] = df_freq["Date"].dt.year
                            elif freq == 'annual':
                                # For precipitation, sum to get annual totals; for other variables, average
                                if var in ["pr", "obs_pr"]:
                                    df_freq = df_processed.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                                    logging.debug(f"DEBUG: Created annual total precipitation data with {len(df_freq)} rows")
                                else:
                                    df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                    logging.debug(f"DEBUG: Created annual average data with {len(df_freq)} rows")
                                # Update units based on frequency
                                display_units = get_variable_units_for_frequency(var, freq)
                                df_freq["Year"] = df_freq["Date"].dt.year
                            else:
                                # Default to annual if frequency not specified
                                df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                df_freq["Year"] = df_freq["Date"].dt.year
                                logging.debug(f"DEBUG: Defaulted to annual averages with {len(df_freq)} rows")
                        except Exception as freq_error:
                            logging.debug(f"DEBUG: Error creating frequency data: {freq_error}")
                            continue
                        
                        # Filter by time series range
                        try:
                            # Use the appropriate slider based on trace index:
                            # trace_index = 0 -> main slider (time_series_range_main)
                            # trace_index = 1+ -> additional trace slider (time_series_range_extra_{trace_index-1})
                            time_range = None
                            if trace_index == 0:
                                # Main trace - use main slider
                                try:
                                    time_range = input.time_series_range_main()
                                except:
                                    time_range = None
                            else:
                                # Additional trace - use its specific slider
                                try:
                                    extra_index = trace_index - 1
                                    slider_id = f"time_series_range_extra_{extra_index}"
                                    time_range = getattr(input, slider_id)()
                                except:
                                    # Fallback to main slider if individual slider doesn't exist
                                    try:
                                        time_range = input.time_series_range_main()
                                    except:
                                        time_range = None
                            
                            if time_range and len(time_range) == 2:
                                plot_start_year, plot_end_year = time_range
                                # For observed data, ensure the range is within the available data range (1950-2013)
                                if var in ("obs_tmin", "obs_tmax", "obs_pr"):
                                    plot_start_year = max(1950, plot_start_year)
                                    plot_end_year = min(2013, plot_end_year)
                                # Validate that the range makes sense for the scenario
                                elif scenario == "historical" and plot_start_year >= 2015:
                                    # If historical scenario but slider shows future range, use historical defaults
                                    plot_start_year, plot_end_year = 1950, 2014
                                elif scenario != "historical" and plot_start_year < 2015:
                                    # If future scenario but slider shows historical range, use future defaults
                                    plot_start_year, plot_end_year = 2015, 2100
                            else:
                                # Use scenario-based defaults when slider is not available
                                if var in ("obs_tmin", "obs_tmax", "obs_pr"):
                                    plot_start_year, plot_end_year = 1950, 2013
                                elif scenario == "historical":
                                    plot_start_year, plot_end_year = 1950, 2014
                                else:
                                    plot_start_year, plot_end_year = 2015, 2100
                            
                            # Filter based on frequency type
                            if freq == 'daily':
                                # For daily data, filter by date range
                                start_date = pd.Timestamp(f"{plot_start_year}-01-01")
                                end_date = pd.Timestamp(f"{plot_end_year}-12-31")
                                df_freq = df_freq[(df_freq["Date"] >= start_date) & (df_freq["Date"] <= end_date)]
                                logging.debug(f"DEBUG: Filtered daily data to date range {start_date.date()}-{end_date.date()}, {len(df_freq)} rows remaining")
                            else:
                                # For monthly and annual data, filter by year range
                                df_freq = df_freq[(df_freq["Year"] >= plot_start_year) & (df_freq["Year"] <= plot_end_year)]
                                logging.debug(f"DEBUG: Filtered {freq} data to time range {plot_start_year}-{plot_end_year}, {len(df_freq)} rows remaining")
                        except Exception as filter_error:
                            logging.debug(f"DEBUG: Error filtering time series: {filter_error}")
                            pass
                        
                        # Create legend name - FIXED: Show projection grid ID for ALL data types
                        # For observational data, show the projection grid ID (what user selected)
                        # For projection data, show the projection grid ID (same as extraction coordinates)
                        try:
                            # Check if this trace uses a loop
                            if combo_is_loop and combo_loop_name:
                                # Use loop name instead of grid coordinates
                                # Truncate long loop names for legend readability
                                display_loop_name = combo_loop_name if len(combo_loop_name) <= 25 else (combo_loop_name[:22] + "...")
                                legend_grid_label = f"Loop: {display_loop_name}"
                            else:
                                # FIXED: Always use combo_lat_idx, combo_lon_idx for legend (projection grid coordinates)
                                legend_grid_lat_idx = combo_lat_idx
                                legend_grid_lon_idx = combo_lon_idx
                                legend_grid_label = f"Grid {legend_grid_lat_idx},{legend_grid_lon_idx}"
                        except Exception:
                            legend_grid_label = None
                        
                        # Always include location in legend for clarity - FULL DETAILS
                        if legend_grid_label:
                            if var in ("obs_tmin","obs_tmax","obs_pr"):
                                legend_name = f"{display_name} (Observed, {freq}, historical) - {legend_grid_label}"
                            else:
                                legend_name = f"{display_name} ({mod}, {freq}, {scenario}) - {legend_grid_label}"
                        else:
                            legend_name = f"{display_name} (Observed, {freq}, historical)" if var in ("obs_tmin","obs_tmax","obs_pr") else f"{display_name} ({mod}, {freq}, {scenario})"
                        
                        if not df_freq.empty:
                            logging.debug(f"DEBUG: Adding trace for {legend_name}")
                            # Use appropriate x-axis based on frequency for proper visualization
                            if freq == 'daily':
                                # For daily data, use years for x-axis (convert dates to years)
                                x_values = pd.to_datetime(df_freq["Date"]).dt.year + (pd.to_datetime(df_freq["Date"]).dt.dayofyear - 1) / 365.25
                            elif freq == 'monthly':
                                # For monthly data, use years for x-axis (convert dates to years with fractional months)
                                x_values = df_freq["Year"] + (pd.to_datetime(df_freq["Date"]).dt.month - 1) / 12.0
                            else:
                                # For annual data, use integer years
                                x_values = df_freq["Year"]
                            
                            # Generate distinct color for this trace using trace_index for guaranteed uniqueness
                            import colorsys
                            try:
                                # Use trace_index to ensure each trace gets a unique color
                                # Define a palette of highly distinguishable colors
                                color_palette = [
                                    "rgb(31, 119, 180)",    # Blue
                                    "rgb(255, 127, 14)",    # Orange
                                    "rgb(44, 160, 44)",     # Green
                                    "rgb(214, 39, 40)",     # Red
                                    "rgb(148, 103, 189)",   # Purple
                                    "rgb(140, 86, 75)",     # Brown
                                    "rgb(227, 119, 194)",   # Pink
                                    "rgb(127, 127, 127)",   # Gray
                                    "rgb(188, 189, 34)",    # Olive
                                    "rgb(23, 190, 207)",    # Cyan
                                    "rgb(255, 187, 120)",   # Light Orange
                                    "rgb(152, 223, 138)",   # Light Green
                                    "rgb(255, 152, 150)",   # Light Red
                                    "rgb(197, 176, 213)",   # Light Purple
                                    "rgb(196, 156, 148)",   # Light Brown
                                    "rgb(247, 182, 210)",   # Light Pink
                                    "rgb(199, 199, 199)",   # Light Gray
                                    "rgb(219, 219, 141)",   # Light Olive
                                    "rgb(158, 218, 229)",   # Light Cyan
                                    "rgb(65, 68, 81)",      # Dark Blue-Gray
                                ]
                                
                                # Use trace_index to cycle through the palette
                                if trace_index < len(color_palette):
                                    distinct_color = color_palette[trace_index]
                                else:
                                    # For traces beyond the palette, generate colors using HSV
                                    # Distribute hues evenly across the color wheel for maximum distinction
                                    hue = (trace_index * 0.618033988749895) % 1.0  # Golden ratio for better distribution
                                    saturation = 0.75 + (trace_index % 3) * 0.08  # Vary saturation slightly
                                    value = 0.85 - (trace_index % 4) * 0.05  # Vary brightness slightly
                                    rgb = colorsys.hsv_to_rgb(hue, saturation, value)
                                    distinct_color = f"rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})"
                            except Exception as color_error:
                                logging.debug(f"DEBUG: Error generating color for trace {trace_index}: {color_error}")
                                # Fallback to a simple color cycle
                                fallback_colors = ["blue", "red", "green", "orange", "purple", "brown", "pink", "gray", "olive", "cyan"]
                                distinct_color = fallback_colors[trace_index % len(fallback_colors)]
                            
                            # Optimize for large datasets: use WebGL-accelerated rendering
                            # This dramatically improves browser performance for large time series
                            num_points = len(df_freq)
                            use_webgl = num_points > 500 or len(plotted_data) > 5  # Use WebGL for large datasets or many traces
                            
                            trace_type = go.Scattergl if use_webgl else go.Scatter
                            
                            if use_webgl:
                                logging.debug(f"DEBUG: Using WebGL rendering for {legend_name} ({num_points} points)")
                            
                            # Round y-values to reduce JSON size and speed up transmission
                            # Precision of 3 decimal places is sufficient for visualization
                            y_values = df_freq[var].round(3) if num_points > 100 else df_freq[var]
                            
                            fig.add_trace(trace_type(
                                x=x_values, 
                                y=y_values, 
                                mode="lines", 
                                name=legend_name, 
                                line=dict(color=distinct_color, width=2),
                                hovertemplate='<b>Date:</b> %{x}<br><b>Value:</b> %{y:.2f}<extra></extra>'  # Always include hover
                            ))
                            plotted_data.append(legend_name)
                            # Optional trendline, controlled by individual checkbox for each trace
                            try:
                                # Main trace (trace_index == 0) uses trendline_main checkbox
                                # Additional traces (trace_index >= 1) use show_trendline_extra_{trace_index - 1} checkbox
                                if trace_index == 0:
                                    show_trend = input.trendline_main()
                                else:
                                    # For additional traces, use their individual checkbox
                                    checkbox_id = f"show_trendline_extra_{trace_index - 1}"
                                    show_trend = getattr(input, checkbox_id, lambda: False)()
                            except Exception as e:
                                logging.debug(f"DEBUG: Error reading trendline checkbox for trace {trace_index}: {e}")
                                show_trend = False
                            
                            if show_trend:
                                tl = compute_trendline(x_values, df_freq[var].values)
                                if tl is not None:
                                    slope, intercept, x_line, y_line, p_value = tl
                                    # Format p-value for display with significance indicators
                                    if p_value is not None:
                                        if p_value < 0.001:
                                            p_str = "p<0.001***"
                                            sig_indicator = "***"
                                        elif p_value < 0.01:
                                            p_str = f"p={p_value:.3f}**"
                                            sig_indicator = "**"
                                        elif p_value < 0.05:
                                            p_str = f"p={p_value:.3f}*"
                                            sig_indicator = "*"
                                        else:
                                            p_str = f"p={p_value:.3f}"
                                            sig_indicator = ""
                                        trend_name = f"{legend_name} (trend, {p_str})"
                                        hover_p_text = f"<br><b>P-value:</b> {p_value:.4f}{sig_indicator}<br><i>{'Statistically significant' if p_value < 0.05 else 'Not significant'}</i>"
                                    else:
                                        trend_name = f"{legend_name} (trend)"
                                        hover_p_text = ""
                                    
                                    # Use same WebGL optimization for trendlines
                                    fig.add_trace(trace_type(
                                        x=x_line,
                                        y=y_line,
                                        mode="lines",
                                        name=trend_name,
                                        line=dict(color=distinct_color, width=2, dash="dash"),
                                        showlegend=False,  # Don't show trendline in legend to avoid duplicates
                                        hovertemplate=f'<b>Date:</b> %{{x}}<br><b>Value:</b> %{{y:.2f}}{hover_p_text}<extra></extra>'  # Include p-value in hover
                                    ))
                                    plotted_data.append(trend_name)
                        else:
                            logging.debug(f"DEBUG: Frequency data is empty for {legend_name}")
                except Exception as e:
                    logging.debug(f"DEBUG: Error processing data for {var}: {e}")
                    continue
                
                # Increment trace index for next iteration
                trace_index += 1

            logging.debug(f"DEBUG: Total plotted data series: {len(plotted_data)}")
            logging.debug(f"DEBUG: Plot data series: {plotted_data}")
            
            # Keep grid coordinates in legend names for clarity
            # No post-processing needed

            # Additional traces are now processed earlier in plot_combinations
            # No need to process them again here

            # Get grid coordinates for title (moved outside conditional)
            grid_id = selected_grid_id.get()
            grid_coords = ""
            if grid_id:
                try:
                    lat_idx, lon_idx = grid_id.split(",")
                    grid_coords = f" (Grid: {lat_idx}, {lon_idx})"
                except:
                    grid_coords = ""

            if not plotted_data:
                logging.debug(f"DEBUG: No data plotted, showing error message")
                fig.add_annotation(text="No data available for the selected combinations.<br>Try selecting different options.", showarrow=False, x=0.5, y=0.5, font=dict(size=14, color="gray"))
                fig.update_layout(height=600, margin=dict(l=50, r=50, t=50, b=50))  # Match container height
            
            # Set up plot layout based on multi-plot mode and unit compatibility
            if multi_mode and len(plot_combinations) > 1:
                # Check if we have incompatible units
                main_var_meta = get_variable_metadata(variable)
                main_units = main_var_meta["units"]
                # Adjust units for aggregated precipitation
                if variable in ["pr", "obs_pr"] and frequency in ["monthly", "annual"]:
                    main_units = "inches"
                
                # Group by units
                plots_by_units = {}
                for combo in plot_combinations:
                    combo_var = combo[0]
                    combo_meta = get_variable_metadata(combo_var)
                    combo_units = combo_meta["units"]
                    
                    if combo_units not in plots_by_units:
                        plots_by_units[combo_units] = []
                    plots_by_units[combo_units].append(combo)
                
                if len(plots_by_units) > 1:
                    # Incompatible units - main plot should only show main selection
                    # Additional plot will be handled by additional_plot function
                    title = f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}){grid_coords}"
                    
                    fig.update_layout(
                        title=title,
                        xaxis_title="Year",
                        yaxis_title=main_units, 
                        hovermode="x unified",
                        showlegend=True,  # FIXED: Explicitly enable legend
                        legend=dict(
                            orientation="h",  # Horizontal below plot for unlimited traces
                            yanchor="top",
                            y=-0.12,  # Below plot area
                            xanchor="center",
                            x=0.5,  # Centered below plot
                            bgcolor="rgba(255,255,255,0.95)",
                            bordercolor="rgba(0,0,0,0.3)",
                            borderwidth=1,
                            font=dict(size=9),  # Compact font for many traces
                            itemsizing="constant",
                            itemclick="toggle",  # Click to toggle traces
                            itemdoubleclick="toggleothers",  # Double-click to isolate
                            tracegroupgap=5,  # Small gap between items
                            traceorder="normal"  # Keep trace order
                        ),
                        margin=dict(l=60, t=80, r=60, b=300),  # Large bottom margin for unlimited traces legend wrapping
                        # FIXED: Constant plot height matching container
                        height=600,  # Match the 350px container height
                        xaxis=dict(
                            showgrid=True,
                            gridwidth=1,
                            gridcolor='lightgray',
                            tickmode='auto',
                            nticks=10,
                            tickangle=0,
                            range=[1950, 2100],  # Force full range from 1950 to 2100
                            autorange=False,  # Disable auto-ranging to force our range
                            constrain='domain',
                            fixedrange=False,  # Allow zooming
                            side='bottom',
                            showline=True,
                            linewidth=1,
                            linecolor='black',
                            mirror=True,
                            rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                        ),
                        autosize=True,  # Let Plotly auto-size
                        width=None,  # Let Plotly auto-size
                        uirevision=True  # Maintain zoom/pan state
                    )
                else:
                    # All data has same units - check if user wants separate plots
                    if multi_plot_info and multi_plot_info['action'] == 'new_plot':
                        # User wants separate plot - main plot should only show main selection
                        title = f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}){grid_coords}"
                    else:
                        # Combine on same plot
                        title = f"{get_variable_display_name(variable)}{grid_coords}"
                    
                    fig.update_layout(
                        title=title,
                        xaxis_title="Year", 
                        yaxis_title=main_units, 
                        xaxis=dict(
                            range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                            automargin=True,
                            dtick=10,  # Show ticks every 10 years
                            tick0=1950,  # Start ticks at 1950
                            tickmode='linear',  # Linear tick spacing
                            rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                        ),
                        yaxis=dict(
                            autorange=True,  # Automatically adjust y-axis to show full data range
                            automargin=True  # Automatically adjust margins
                        ),
                        height=600,  # Match the 350px container height
                        width=None,
                        hovermode="x unified",
                        showlegend=True,  # FIXED: Explicitly enable legend
                        legend=dict(
                            orientation="h",  # Horizontal below plot for unlimited traces
                            yanchor="top",
                            y=-0.12,  # Below plot area
                            xanchor="center",
                            x=0.5,  # Centered below plot
                            bgcolor="rgba(255,255,255,0.95)",
                            bordercolor="rgba(0,0,0,0.3)",
                            borderwidth=1,
                            font=dict(size=9),  # Compact font for many traces
                            itemsizing="constant",
                            itemclick="toggle",  # Click to toggle traces
                            itemdoubleclick="toggleothers",  # Double-click to isolate
                            tracegroupgap=5,  # Small gap between items
                            traceorder="normal"  # Keep trace order
                        ),
                        margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                    )
            else:
                # Single plot mode or compatible units
                # Set initial layout without title (we'll set it after all traces are added)
                x_axis_title = "Year"  # Always label x-axis in years per preference
                
                # Determine correct y-axis units based on variable and frequency
                base_units = get_variable_metadata(variable)["units"]
                if variable in ["pr", "obs_pr"] and frequency in ["monthly", "annual"]:
                    # For aggregated precipitation, use "inches" instead of "inches/day"
                    y_axis_units = "inches"
                else:
                    y_axis_units = base_units
                
                fig.update_layout(
                    xaxis_title=x_axis_title, 
                    yaxis_title=y_axis_units, 
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    width=None,
                    hovermode="x unified",
                    showlegend=True,  # FIXED: Explicitly enable legend
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    ),
                    margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                )

            # Add a simple test trace to ensure the plot is working
            if len(fig.data) == 0:
                logging.debug(f"DEBUG: No traces in figure, adding test trace")
                fig.add_trace(go.Scatter(
                    x=[2020, 2021, 2022, 2023, 2024],
                    y=[1, 2, 3, 4, 5],
                    mode="lines",
                    name="Test Plot",
                    line=dict(color="red")
                ))
                
                fig.update_layout(
                    title="Test Plot - No Data Available",
                    xaxis_title="Year",
                    yaxis_title="Test Values",
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    width=None,
                    margin=dict(l=60, t=80, r=60, b=150),  # Extra bottom margin for legend
                    showlegend=True,  # FIXED: Explicitly enable legend
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    )
                )
            
            # FIXED: Set the title after all traces are added
            if len(fig.data) > 1:
                # Multiple plots - show just the variable name (no grid coordinates)
                title = f"{get_variable_display_name(variable)}"
                # Always keep x-axis title as Years
                x_axis_title = "Year"
            else:
                # Single plot - show descriptive title with frequency information
                if is_observed_main:
                    # For observed data, show frequency in title
                    if frequency == "daily":
                        frequency_text = "Daily"
                    elif frequency == "monthly":
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Monthly Total"
                        else:
                            frequency_text = "Monthly Average"
                    else:  # annual
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Annual Total"
                        else:
                            frequency_text = "Annual Average"
                    title = f"{frequency_text} {get_variable_display_name(variable)}{grid_coords}"
                else:
                    # For model data, show frequency in title
                    if frequency == "daily":
                        frequency_text = "Daily"
                    elif frequency == "monthly":
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Monthly Total"
                        else:
                            frequency_text = "Monthly Average"
                    else:  # annual
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Annual Total"
                        else:
                            frequency_text = "Annual Average"
                    title = f"{frequency_text} {get_variable_display_name(variable)} ({model}, {ssp}){grid_coords}"
                x_axis_title = "Year"
            
            # FIXED: Only update title, don't override legend or height settings
            fig.update_layout(
                title=title,
                xaxis_title=x_axis_title
            )
            
            logging.debug(f"DEBUG: Returning figure with {len(fig.data)} traces")
            
            # CRITICAL: Aggressive optimization for very large polygon plots
            # Must minimize JSON payload size to prevent connection timeout
            is_very_large = len(plotted_data) > 5 or any(
                hasattr(trace, 'y') and len(trace.y) > 500 for trace in fig.data
            )
            
            if is_very_large:
                logging.debug(f"DEBUG: ðŸ”§ Applying optimization for large plot")
                
                # Optimize layout for large datasets while keeping hover tooltips
                fig.update_layout(
                    hovermode='x unified',  # Show all traces on hover - efficient for large plots
                    dragmode='pan',   # Pan only
                    modebar=dict(remove=['lasso2d', 'select2d', 'zoom2d', 'pan2d']),
                    uirevision='constant',
                    # Minimal margins
                    margin=dict(l=50, r=20, t=70, b=50),
                    # Reduce font complexity
                    font=dict(size=10),
                    # Disable animations
                    transition={'duration': 0},
                )
                
                # Simplified hover template for large plots - fast rendering with year and value
                # Use %{x} for automatic date formatting - works for all frequencies
                for trace in fig.data:
                    if trace.mode and 'lines' in trace.mode:
                        trace.update(
                            hovertemplate='<b>Date:</b> %{x}<br><b>Value:</b> %{y:.2f}<extra></extra>'
                        )
                
                logging.debug(f"DEBUG: âœ… Large plot optimized with hover tooltips enabled")
            else:
                # Standard optimization
                fig.update_layout(
                    modebar_remove=['lasso2d', 'select2d'] if len(plotted_data) > 3 else [],
                    hovermode='x unified',  # Always enable hover for all plots
                    uirevision='constant',
                )
                
                # ALWAYS ensure hover tooltips are enabled for all traces
                for trace in fig.data:
                    if trace.mode and 'lines' in trace.mode:
                        trace.update(
                            hovertemplate='<b>Date:</b> %{x}<br><b>Value:</b> %{y:.2f}<extra></extra>'
                        )
            
            signal.alarm(0)  # Cancel timeout
            return fig
        except TimeoutError:
            logging.debug("DEBUG: Plot generation timed out")
            signal.alarm(0)  # Cancel timeout
            fig = go.Figure()
            fig.add_annotation(text="Plot generation timed out. Please try again with fewer combinations.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="orange"))
            fig.update_layout(height=600, autosize=True, margin=dict(l=60, t=80, r=60, b=100))  # Match container height
            return fig
        except Exception as e:
            logging.error(f"ERROR in selected_gridbox_plot: {type(e).__name__}: {e}")
            logging.debug(f"DEBUG: Full traceback: {traceback.format_exc()}")
            signal.alarm(0)  # Cancel timeout
            # Return a simple error plot
            fig = go.Figure()
            fig.add_annotation(text=f"Error generating plot: {str(e)}", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="red"))
            fig.update_layout(height=600, autosize=True, margin=dict(l=60, t=80, r=60, b=100))  # Match container height
            return fig
    
    @output
    @render.ui
    def additional_traces_controls():
        """Render time series sliders and trendline checkboxes for additional traces"""
        logging.debug("DEBUG: additional_traces_controls function called")
        try:
            # Get additional traces from both current and persistent storage with error handling
            current_traces = []
            persistent_traces = []
            
            try:
                current_traces = list(main_plot_additional_traces.get() or [])
            except Exception as e:
                logging.debug(f"DEBUG: Error getting current traces: {e}")
                current_traces = []
                
            try:
                persistent_traces = list(main_plot_persistent_traces.get() or [])
            except Exception as e:
                logging.debug(f"DEBUG: Error getting persistent traces: {e}")
                persistent_traces = []
            
            # Combine and deduplicate traces safely
            all_traces = current_traces + persistent_traces
            seen_traces = set()
            unique_traces = []
            
            for tr in all_traces:
                try:
                    if not tr or not isinstance(tr, dict):
                        continue
                    # Create trace key that handles both grid and loop traces
                    loop_key = tr.get('loop_name') if tr.get('loop_gridboxes') else None
                    if loop_key:
                        trace_key = (tr.get('variable'), tr.get('model'), tr.get('ssp'), f"loop_{loop_key}", None)
                    else:
                        trace_key = (tr.get('variable'), tr.get('model'), tr.get('ssp'), tr.get('lat_idx'), tr.get('lon_idx'))
                    if trace_key not in seen_traces and (loop_key or all(k is not None for k in trace_key)):
                        seen_traces.add(trace_key)
                        unique_traces.append(tr)
                except Exception as e:
                    logging.debug(f"DEBUG: Error processing trace: {e}")
                    continue
            
            # Get main plot information for inclusion in the table
            main_plot_info = None
            try:
                selections = get_plot_selections()
                if selections and selections.get('variable'):
                    main_plot_info = {
                        'variable': selections['variable'],
                        'model': selections['model'],
                        'frequency': selections['frequency'],
                        'ssp': selections['ssp'],
                        'grid_indices': selections['grid_indices'],
                        'loop_name': selections.get('loop_name'),
                        'loop_gridboxes': selections.get('loop_gridboxes')
                    }
            except Exception as e:
                logging.debug(f"DEBUG: Error getting main plot info: {e}")
            
            # If no additional traces and no main plot, return empty message
            if not unique_traces and not main_plot_info:
                return ui.div("No traces to display", class_="text-muted mt-3")
            
            logging.debug(f"DEBUG: Rendering {len(unique_traces)} additional trace controls + main plot")
            logging.debug(f"DEBUG: Additional trace controls - unique_traces: {unique_traces}")
            logging.debug(f"DEBUG: Main plot info: {main_plot_info}")
            
            # Create a more stable container structure with performance optimization
            container_elements = []
            
            # Add main plot controls as the first row if available
            if main_plot_info:
                try:
                    main_var = main_plot_info['variable']
                    main_model = main_plot_info['model']
                    main_freq = main_plot_info['frequency']
                    main_ssp = main_plot_info['ssp']
                    main_grid = main_plot_info['grid_indices']
                    
                    # Determine time range for main plot
                    if main_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                        main_min_year, main_max_year = 1950, 2013
                        main_default_range = (1950, 2013)
                    elif main_ssp == "historical":
                        main_min_year, main_max_year = 1950, 2014
                        main_default_range = (1950, 2014)
                    else:
                        main_min_year, main_max_year = 2015, 2100
                        main_default_range = (2015, 2100)
                    
                    # Create main plot slider (compact for table layout)
                    main_slider = ui.input_slider(
                        "time_series_range_main",
                        "Year Range",
                        min=main_min_year,
                        max=main_max_year,
                        value=main_default_range,
                        step=1,
                        sep=""
                    )
                    
                    # Create main plot trendline checkbox (compact for table layout)
                    main_checkbox = ui.input_checkbox(
                        "trendline_main",
                        "Show Trendline",
                        value=False
                    )
                    
                    # Determine location label (grid or loop)
                    main_loop_name = main_plot_info.get('loop_name')
                    if main_loop_name:
                        main_location_label = f"Loop: {main_loop_name}"
                    else:
                        main_location_label = f"Grid {main_grid[0]},{main_grid[1]}"
                    
                    # Create main plot row
                    main_plot_row = ui.div(
                        ui.div(
                            ui.div(
                                ui.div("Main Plot", class_="font-weight-bold"),
                                ui.div(f"{get_variable_display_name(main_var)} ({main_model}, {main_freq}, {main_ssp}) - {main_location_label}", class_="text-muted small", style="word-wrap: break-word; max-width: 300px;"),
                                class_="col-8"
                            ),
                            ui.div(
                                main_slider,
                                class_="col-3"
                            ),
                            ui.div(
                                main_checkbox,
                                class_="col-1 text-center"
                            ),
                            class_="row align-items-center mb-2"
                        ),
                        class_="border-bottom pb-2"
                    )
                    
                    container_elements.append(main_plot_row)
                    logging.debug(f"DEBUG: Added main plot controls to table")
                    
                except Exception as e:
                    logging.debug(f"DEBUG: Error creating main plot controls: {e}")
            
            # If no additional traces, still show the table with just main plot
            if not unique_traces:
                traces_to_render = []
            else:
                traces_to_render = unique_traces
            
            logging.debug(f"DEBUG: Rendering {len(traces_to_render)} additional trace controls (unlimited)")
            
            # Render all traces
            for idx, trace in enumerate(traces_to_render):
                try:
                    trace_var = trace.get('variable')
                    trace_model = trace.get('model')
                    trace_freq = trace.get('frequency', 'annual')
                    trace_ssp = trace.get('ssp')
                    trace_lat = trace.get('lat_idx')
                    trace_lon = trace.get('lon_idx')
                    trace_loop_name = trace.get('loop_name')
                    trace_loop_gridboxes = trace.get('loop_gridboxes')
                    
                    # Validate trace data (allow either grid coordinates OR loop data)
                    is_loop_trace = trace_loop_name and trace_loop_gridboxes
                    if not all([trace_var, trace_model, trace_ssp]):
                        logging.debug(f"DEBUG: Skipping invalid trace at index {idx}: missing basic fields")
                        continue
                    if not is_loop_trace and (trace_lat is None or trace_lon is None):
                        logging.debug(f"DEBUG: Skipping invalid trace at index {idx}: neither valid grid nor loop data")
                        continue
                    
                    # Determine time range based on scenario
                    if trace_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                        min_year, max_year = 1950, 2013
                        default_range = (1950, 2013)
                    elif trace_ssp == "historical":
                        min_year, max_year = 1950, 2014
                        default_range = (1950, 2014)
                    else:
                        min_year, max_year = 2015, 2100
                        default_range = (2015, 2100)
                    
                    # Determine location label
                    if is_loop_trace:
                        location_label = f"Loop: {trace_loop_name} ({len(trace_loop_gridboxes)} gridboxes)"
                        trace_label = f"Additional trace {idx+1}: {get_variable_display_name(trace_var)} ({trace_model}, {trace_freq}, {trace_ssp}) - {location_label}"
                    else:
                        location_label = f"Grid {trace_lat},{trace_lon}"
                        trace_label = f"Additional trace {idx+1}: {get_variable_display_name(trace_var)} ({trace_model}, {trace_freq}, {trace_ssp}) - {location_label}"
                    
                    # Create a more stable structure with proper error handling
                    try:
                        # Use consistent IDs that match what the rest of the code expects
                        slider_id = f"time_series_range_extra_{idx}"
                        checkbox_id = f"show_trendline_extra_{idx}"
                        
                        # Create slider with error handling (compact for table layout)
                        try:
                            slider_element = ui.input_slider(
                                slider_id,
                                "Year Range",
                            min=min_year,
                            max=max_year,
                            value=default_range,
                            step=1,
                            sep=""
                            )
                        except Exception as slider_error:
                            logging.debug(f"DEBUG: Error creating slider {slider_id}: {slider_error}")
                            # Create a fallback element
                            slider_element = ui.div(f"Slider {idx+1}: {trace_label}", class_="text-muted")
                        
                        # Create checkbox with error handling (compact for table layout)
                        try:
                            checkbox_element = ui.input_checkbox(
                                checkbox_id, 
                                "Show Trendline", 
                                value=False
                            )
                        except Exception as checkbox_error:
                            logging.debug(f"DEBUG: Error creating checkbox {checkbox_id}: {checkbox_error}")
                            # Create a fallback element
                            checkbox_element = ui.div(f"Trendline option for trace {idx+1}", class_="text-muted")
                        
                        # Create container for this trace in table row format
                        trace_info = f"{trace_model} ({trace_freq}, {trace_ssp}) - {location_label}"
                        trace_container = ui.div(
                            ui.div(
                                ui.div(
                                    ui.div(f"Additional Trace {idx+1}: {get_variable_display_name(trace_var)}", class_="font-weight-bold"),
                                    ui.div(trace_info, class_="text-muted small", style="word-wrap: break-word; max-width: 300px;"),
                                    class_="col-8"
                                ),
                                ui.div(
                            slider_element,
                                    class_="col-3"
                                ),
                                ui.div(
                            checkbox_element,
                                    class_="col-1 text-center"
                                ),
                                class_="row align-items-center mb-2"
                            ),
                            class_="border-bottom pb-2"
                        )
                        
                        container_elements.append(trace_container)
                        
                    except Exception as element_error:
                        logging.debug(f"DEBUG: Error creating UI elements for trace {idx}: {element_error}")
                        # Create a fallback element
                        fallback_element = ui.div(
                            f"Error loading controls for trace {idx+1}",
                            class_="mt-2 p-2 border rounded bg-warning",
                            id=f"trace_control_error_{idx}"
                        )
                        container_elements.append(fallback_element)
                        continue
                except Exception as trace_error:
                    logging.debug(f"DEBUG: Error processing trace {idx}: {trace_error}")
                    continue
            
            if not container_elements:
                return ui.div("Error loading trace controls", class_="text-danger mt-3")
            
            # Create the main container with a stable structure
            try:
                total_traces = len(container_elements)  # Includes main plot + additional traces
                additional_count = len(unique_traces)
                if main_plot_info and additional_count > 0:
                    trace_info = f"(Main plot + {additional_count} additional trace{'s' if additional_count != 1 else ''})"
                elif main_plot_info and additional_count == 0:
                    trace_info = "(Main plot only)"
                elif not main_plot_info and additional_count > 0:
                    trace_info = f"({additional_count} additional trace{'s' if additional_count != 1 else ''})"
                else:
                    trace_info = "(No traces)"
                
                # Create scrollable table layout with fixed height to maintain plot size
                scrollable_controls = ui.div(
                    # Table header
                    ui.div(
                        ui.div("Trace", class_="col-6 font-weight-bold"),
                        ui.div("Time Range", class_="col-4 font-weight-bold"),
                        ui.div("Trendline", class_="col-2 font-weight-bold text-center"),
                        class_="row border-bottom pb-2 mb-2",
                        id="trace-controls-header"
                    ),
                    # Scrollable content area - unlimited traces supported
                    ui.div(
                        *container_elements,
                        style="height: 400px; min-height: 400px; max-height: 600px; overflow-y: auto; overflow-x: auto; padding-right: 10px; width: 100%;",
                        id="trace-controls-content"
                    ),
                    # Scroll indicator for many traces
                    ui.div(
                        ui.span("â†“ Scroll down for more traces", class_="text-info small"),
                        class_="text-center mt-2"
                    ) if total_traces > 4 else None,
                    class_="border rounded p-3 bg-light"
                )
                
                # Create a simple, stable container without dynamic IDs
                main_container = ui.div(
                    ui.div(
                        ui.h6(f"Trace Controls {trace_info}", style="display: inline-block; margin-top: 0px; margin-bottom: 8px;"),
                        ui.tags.button(
                            "ðŸ“¥ Download as CSV",
                            onclick="downloadTraceControlsAsCSV()",
                            style="float: right; padding: 6px 12px; background-color: #2196F3; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 13px; font-weight: bold; margin-top: 0px;",
                            onmouseover="this.style.backgroundColor='#0b7dda'",
                            onmouseout="this.style.backgroundColor='#2196F3'"
                        ),
                        style="overflow: auto;"
                    ),
                    scrollable_controls,
                    ui.div("Use these sliders to adjust the time period for each line. Scroll down to see more traces." + 
                           (f" ({total_traces} traces total)" if total_traces > 3 else ""), 
                           class_="text-muted mt-2"),
                    ui.div(f"All traces have compatible units and can be plotted together.", class_="text-success small mt-1"),
                    ui.tags.script("""
                        function downloadTraceControlsAsCSV() {
                            console.log('downloadTraceControlsAsCSV called');
                            let csv = [];
                            csv.push('"Trace","Time Range","Trendline"');
                            
                            const content = document.getElementById('trace-controls-content');
                            if (!content) {
                                console.error('trace-controls-content not found');
                                return;
                            }
                            
                            const traces = content.querySelectorAll('.border-bottom');
                            console.log('Found', traces.length, 'traces');
                            
                            for (let trace of traces) {
                                const traceNameDiv = trace.querySelector('[class*="col-"]');
                                const checkbox = trace.querySelector('input[type="checkbox"]');
                                
                                if (traceNameDiv) {
                                    const traceName = traceNameDiv.innerText.replace(/"/g, '""').replace(/\\n/g, ' ').trim();
                                    const timeRange = 'See dashboard';
                                    const trendline = checkbox ? (checkbox.checked ? 'Yes' : 'No') : 'N/A';
                                    csv.push(`"${traceName}","${timeRange}","${trendline}"`);
                                }
                            }
                            
                            console.log('CSV rows:', csv.length);
                            const csvContent = csv.join('\\n');
                            const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
                            const url = window.URL.createObjectURL(blob);
                            const a = document.createElement('a');
                            a.href = url;
                            a.download = 'Trace_Controls_' + new Date().toISOString().slice(0,10) + '.csv';
                            document.body.appendChild(a);
                            a.click();
                            document.body.removeChild(a);
                            window.URL.revokeObjectURL(url);
                            console.log('Download triggered');
                        }
                    """),
                    class_="mb-4",
                    style="margin-top: 0px; clear: both;"
                )
                logging.debug(f"DEBUG: Returning trace controls with download button - total_traces: {total_traces}, additional: {additional_count}")
                return main_container
            except Exception as container_error:
                logging.debug(f"DEBUG: Error creating main container: {container_error}")
                # Return a simple fallback without complex UI elements
                return ui.div(
                    ui.h6("Additional Trace Controls", class_="mt-3 mb-2"),
                    ui.div(f"Found {len(unique_traces)} compatible traces", class_="text-info"),
                    ui.div("Trace controls temporarily unavailable due to UI error.", class_="text-warning"),
                    class_="mt-3"
                )
            
        except Exception as e:
            logging.debug(f"DEBUG: Critical error in additional_traces_controls: {e}")
            return ui.div("Critical error loading trace controls", class_="text-danger mt-3")
    
    @output
    @render.ui
    def mad_table():
        """Render Mean Absolute Difference table for trace combinations"""
        # Make reactive to time series slider changes
        try:
            _ = input.time_series_range_main()
        except:
            pass
        
        # Make reactive to additional traces changes
        try:
            _ = main_plot_additional_traces.get()
            _ = main_plot_persistent_traces.get()
        except:
            pass
        
        # Also react to additional trace sliders - but don't fail if they don't exist yet
        try:
            additional_traces = main_plot_additional_traces.get() or []
            for i in range(len(additional_traces)):
                try:
                    # Only try to access slider if it exists
                    slider_id = f"time_series_range_extra_{i}"
                    if hasattr(input, slider_id):
                        _ = getattr(input, slider_id)()
                except:
                    pass  # Slider not ready yet, that's okay
        except:
            pass
        
        try:
            # Get current plot data from the main plot
            selections = get_plot_selections()
            variable = selections['variable']
            model = selections['model']
            frequency = selections['frequency']
            ssp = selections['ssp']
            grid_indices = selections['grid_indices']
            lat_idx, lon_idx = grid_indices if grid_indices else (None, None)
            
            # Get loop data if present
            loop_gridboxes = selections.get('loop_gridboxes')
            loop_name = selections.get('loop_name')
            is_loop = loop_gridboxes is not None and len(loop_gridboxes) > 0
            
            logging.debug(f"DEBUG: mad_table - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
            logging.debug(f"DEBUG: mad_table - grid_indices: {grid_indices}, is_loop: {is_loop}")
            
            # Check if we have valid data (either grid or loop)
            if not variable or not frequency:
                logging.debug(f"DEBUG: mad_table - Missing variable or frequency, returning None")
                return None
            if not is_loop and (lat_idx is None or lon_idx is None):
                logging.debug(f"DEBUG: mad_table - Missing grid indices and no loop data, returning None")
                return None
            
            # Get additional traces
            current_traces = list(main_plot_additional_traces.get() or [])
            persistent_traces = list(main_plot_persistent_traces.get() or [])
            all_traces = current_traces + persistent_traces
            
            logging.debug(f"DEBUG: mad_table - current_traces: {len(current_traces)}, persistent_traces: {len(persistent_traces)}")
            
            # Remove duplicates (handle loops)
            seen_traces = set()
            unique_traces = []
            for trace_cfg in all_traces:
                # Create trace key that handles both grid and loop traces
                trace_loop_name = trace_cfg.get('loop_name') if trace_cfg.get('loop_gridboxes') else None
                if trace_loop_name:
                    trace_key = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'), f"loop_{trace_loop_name}", None)
                else:
                    trace_key = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'), trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                if trace_key not in seen_traces:
                    seen_traces.add(trace_key)
                    unique_traces.append(trace_cfg)
            
            logging.debug(f"DEBUG: mad_table - unique_traces: {len(unique_traces)}")
            
            # Always show MAD table if we have a main trace, even without additional traces
            logging.debug(f"DEBUG: mad_table - Processing {len(unique_traces)} additional traces with main trace")
            
            # Create trace data for main selection
            if is_loop:
                main_location_label = f"Loop: {loop_name}"
                main_trace_data = {
                    'variable': variable,
                    'model': model,
                    'frequency': frequency,
                    'ssp': ssp,
                    'loop_gridboxes': loop_gridboxes,
                    'loop_name': loop_name,
                    'name': f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}) - {main_location_label}"
                }
            else:
                main_trace_data = {
                    'variable': variable,
                    'model': model,
                    'frequency': frequency,
                    'ssp': ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx,
                    'name': f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}) - Grid {lat_idx},{lon_idx}"
                }
            
            # Add name field to additional traces if missing
            for trace in unique_traces:
                if 'name' not in trace:
                    trace_loop_name = trace.get('loop_name') if trace.get('loop_gridboxes') else None
                    if trace_loop_name:
                        trace_location = f"Loop: {trace_loop_name}"
                    else:
                        trace_location = f"Grid {trace['lat_idx']},{trace['lon_idx']}"
                    trace['name'] = f"{get_variable_display_name(trace['variable'])} ({trace['model']}, {trace['frequency']}, {trace['ssp']}) - {trace_location}"
            
            # Combine main trace with additional traces
            all_trace_data = [main_trace_data] + unique_traces
            
            logging.debug(f"DEBUG: MAD - Total traces for comparison: {len(all_trace_data)}")
            logging.debug(f"DEBUG: MAD - Trace names: {[trace['name'] for trace in all_trace_data]}")
            
            # Calculate MAD for all pairs
            mad_results = []
            for i in range(len(all_trace_data)):
                for j in range(i + 1, len(all_trace_data)):
                    trace1 = all_trace_data[i]
                    trace2 = all_trace_data[j]
                    
                    # Skip if frequencies don't match - can't compare different frequencies
                    if trace1.get('frequency') != trace2.get('frequency'):
                        logging.debug(f"DEBUG: MAD - Skipping comparison due to frequency mismatch: {trace1.get('frequency')} vs {trace2.get('frequency')}")
                        continue
                    
                    # Get time ranges for each trace from their respective sliders
                    trace1_time_range = None
                    trace2_time_range = None
                    
                    # For main trace, use main slider
                    if i == 0:  # Main trace is always first
                        try:
                            trace1_time_range = input.time_series_range_main()
                        except:
                            pass
                    
                    # For additional traces, try to get their individual sliders
                    if i > 0:  # Additional trace
                        try:
                                trace1_time_range = getattr(input, f"time_series_range_extra_{i-1}")()
                        except:
                            # Fallback to main slider if individual slider doesn't exist
                            try:
                                trace1_time_range = input.time_series_range_main()
                            except:
                                pass
                    
                    if j == 0:  # Main trace is always first
                        try:
                            trace2_time_range = input.time_series_range_main()
                        except:
                            pass
                    
                    if j > 0:  # Additional trace
                        try:
                                trace2_time_range = getattr(input, f"time_series_range_extra_{j-1}")()
                        except:
                            # Fallback to main slider if individual slider doesn't exist
                            try:
                                trace2_time_range = input.time_series_range_main()
                            except:
                                pass
                    
                    # Get trace data with their time ranges
                    logging.debug(f"DEBUG: MAD calculation - Trace1: {trace1['variable']} (range: {trace1_time_range})")
                    logging.debug(f"DEBUG: MAD calculation - Trace2: {trace2['variable']} (range: {trace2_time_range})")
                    
                    trace1_data = get_trace_data_for_mad(trace1, trace1_time_range)
                    trace2_data = get_trace_data_for_mad(trace2, trace2_time_range)
                    
                    logging.debug(f"DEBUG: MAD - Trace1 data: {'Success' if trace1_data is not None else 'None'}")
                    logging.debug(f"DEBUG: MAD - Trace2 data: {'Success' if trace2_data is not None else 'None'}")
                    
                    if trace1_data is None or trace2_data is None:
                        logging.debug(f"DEBUG: MAD - Skipping comparison due to missing data")
                        continue
                    
                    # Calculate MAD
                    mad_result = calculate_mad(
                        trace1_data, trace2_data,
                        trace1['frequency'], trace2['frequency'],
                        trace1['name'], trace2['name']
                    )
                    
                    # FIXED: Always add results, even if MAD is None, to show why comparison failed
                    if mad_result['mad'] is not None:
                        mad_results.append({
                            'trace1': trace1['name'],
                            'trace2': trace2['name'],
                            'mad': mad_result['mad'],
                            'categorization': mad_result['categorization'],
                            'common_years': mad_result['common_years'],
                            'n_points': mad_result['n_points'],
                            'p_value': mad_result.get('p_value'),
                            'warning': mad_result.get('warning', '')
                        })
                    else:
                        # Show why MAD couldn't be calculated
                        reason = mad_result.get('categorization', 'Cannot compare')
                        if mad_result.get('error'):
                            reason = mad_result['error']
                        mad_results.append({
                            'trace1': trace1['name'],
                            'trace2': trace2['name'],
                            'mad': None,
                            'categorization': reason,
                            'common_years': mad_result.get('common_years', 'N/A'),
                            'n_points': mad_result.get('n_points', 0),
                            'p_value': None,
                            'warning': ''
                        })
            
            logging.debug(f"DEBUG: MAD - Total MAD results generated: {len(mad_results)}")
            
            if not mad_results:
                logging.debug(f"DEBUG: MAD - No MAD results, showing message")
                return ui.div(
                    ui.h5("Mean Absolute Difference Analysis", class_="mt-4 mb-3"),
                    ui.div(
                        ui.p("MAD table will appear when comparing multiple traces with the same frequency.", class_="text-muted"),
                        ui.p(f"Currently tracking: {len(all_trace_data)} trace(s) total", class_="text-muted"),
                        class_="alert alert-secondary"
                    ),
                    class_="mt-3"
                )
            
            # Create HTML table rows
            logging.debug(f"DEBUG: MAD HTML - Creating {len(mad_results)} table rows")
            table_rows = []
            for idx, result in enumerate(mad_results):
                logging.debug(f"DEBUG: MAD HTML - Row {idx}: {result['trace1'][:40]}... vs {result['trace2'][:40]}...")
                
                # Format p-value with significance indicators
                p_value = result.get('p_value')
                if p_value is not None:
                    if p_value < 0.001:
                        p_str = f"{p_value:.4f}***"
                        p_color = "#d32f2f"  # Red for highly significant difference
                    elif p_value < 0.01:
                        p_str = f"{p_value:.4f}**"
                        p_color = "#f57c00"  # Orange for very significant
                    elif p_value < 0.05:
                        p_str = f"{p_value:.4f}*"
                        p_color = "#fbc02d"  # Yellow for significant
                    else:
                        p_str = f"{p_value:.4f}"
                        p_color = "#666"  # Gray for not significant
                else:
                    p_str = "N/A"
                    p_color = "#999"
                
                table_rows.append(
                    ui.tags.tr(
                        ui.tags.td(result['trace1'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: left; word-wrap: break-word;"),
                        ui.tags.td(result['trace2'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: left; word-wrap: break-word;"),
                        ui.tags.td(f"{result['mad']:.4f}" if result['mad'] is not None else "N/A", style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center; font-weight: bold;"),
                        ui.tags.td(result['categorization'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        ui.tags.td(result['common_years'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        ui.tags.td(f"{result['n_points']}", style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        ui.tags.td(p_str, style=f"padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center; font-weight: bold; color: {p_color};"),
                        style="background-color: #fff;" if idx % 2 == 0 else "background-color: #f8f9fa;"
                    )
                )
            
            # Create complete HTML table
            logging.debug(f"DEBUG: MAD HTML - Building final table with {len(table_rows)} rows")
            html_table = ui.div(
                ui.div(
                    ui.h4("Mean Absolute Difference Analysis", style="display: inline-block; margin-bottom: 5px; color: #333;"),
                    ui.tags.button(
                        "ðŸ“¥ Download as CSV",
                        onclick="downloadMADTableAsCSV()",
                        style="float: right; padding: 8px 16px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 14px; font-weight: bold;",
                        onmouseover="this.style.backgroundColor='#45a049'",
                        onmouseout="this.style.backgroundColor='#4CAF50'"
                    ),
                    style="text-align: center; margin-bottom: 10px; overflow: auto;"
                ),
                ui.p("Comparison of all trace combinations displayed on the main graph", 
                     style="text-align: center; margin-bottom: 5px; color: #666; font-size: 13px;"),
                ui.p("P-value significance: * p<0.05, ** p<0.01, *** p<0.001 (smaller p-value = more statistically significant difference)", 
                     style="text-align: center; margin-bottom: 15px; color: #999; font-size: 11px; font-style: italic;"),
                ui.div(
                    ui.tags.table(
                        ui.tags.thead(
                            ui.tags.tr(
                                ui.tags.th("Trace 1", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: left; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Trace 2", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: left; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("MAD", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Match Quality", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Time Period", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Points", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("P-value", title="Statistical significance of the difference (*, **, *** indicate p<0.05, p<0.01, p<0.001)", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1; cursor: help;")
                            )
                        ),
                        ui.tags.tbody(*table_rows),
                        style="width: 100%; border-collapse: collapse; font-family: Arial, sans-serif; font-size: 14px;",
                        id="mad-table-content"
                    ),
                    style="max-height: 600px; overflow-y: auto; overflow-x: auto; border: 1px solid #ddd; border-radius: 5px;"
                ),
                ui.p("Scroll to see more combinations", style="text-align: center; margin-top: 10px; color: #999; font-size: 12px; font-style: italic;"),
                ui.tags.script("""
                    function downloadMADTableAsCSV() {
                        const table = document.getElementById('mad-table-content');
                        if (!table) return;
                        
                        let csv = [];
                        const rows = table.querySelectorAll('tr');
                        
                        for (let row of rows) {
                            let cols = row.querySelectorAll('td, th');
                            let csvRow = [];
                            for (let col of cols) {
                                let text = col.innerText.replace(/"/g, '""');
                                csvRow.push('"' + text + '"');
                            }
                            csv.push(csvRow.join(','));
                        }
                        
                        const csvContent = csv.join('\\n');
                        const blob = new Blob([csvContent], { type: 'text/csv' });
                        const url = window.URL.createObjectURL(blob);
                        const a = document.createElement('a');
                        a.href = url;
                        a.download = 'MAD_Analysis_' + new Date().toISOString().slice(0,10) + '.csv';
                        document.body.appendChild(a);
                        a.click();
                        document.body.removeChild(a);
                        window.URL.revokeObjectURL(url);
                    }
                """),
                style="padding: 20px; background-color: #fafafa; border-radius: 8px; margin-top: 0px; clear: both;"
            )
            
            return html_table
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in mad_table: {e}")
            import traceback
            traceback.print_exc()
            # Return a helpful message instead of None so user knows what's happening
            return ui.div(
                ui.h5("Mean Absolute Difference Analysis", class_="mt-4 mb-3"),
                ui.div(
                    ui.p("MAD table is loading... Please wait a moment for all traces to be ready.", class_="text-info"),
                    class_="alert alert-info"
                ),
                class_="mt-3"
            )

    @output
    @render.ui
    def multi_plot_output():
        """DISABLED: Only main plot is used - additional traces are added to main plot"""
        # Always return None to disable separate plot creation
        return None
            
    # Event handlers for multi-plot actions
    @reactive.Effect
    @reactive.event(input.add_current_plot)
    def _add_current_plot():
        """Add the current multi-plot selection to the plots list"""
        multi_var = multi_plot_variable.get()
        multi_model = multi_plot_model.get()
        multi_frequency = multi_plot_frequency.get()
        multi_ssp = multi_plot_ssp.get()
        multi_action = multi_plot_action.get()
        
        if not (multi_var and multi_model and multi_frequency and multi_ssp):
            return
            
        # Get current plots list
        plots_list = list(multi_plots_list.get())
        
        # FIXED: Use multi-plot specific grid selection instead of global grid selection
        multi_grid_id = multi_plot_grid_id.get()
        if not multi_grid_id:
            logging.debug("DEBUG: No grid selected for multi-plot")
            return
            
        # Convert grid ID to lat/lon indices
        try:
            # Parse the grid ID to get lat_idx and lon_idx
            # Grid ID format is typically "lat_idx,lon_idx"
            if ',' in multi_grid_id:
                lat_idx, lon_idx = map(int, multi_grid_id.split(','))
            else:
                # Fallback: try to get from current global selection
                grid_indices = get_selected_grid_indices()
                if not grid_indices:
                    logging.debug("DEBUG: No grid selected for multi-plot")
                    return
                lat_idx, lon_idx = grid_indices
        except Exception as e:
            logging.debug(f"DEBUG: Error parsing grid ID '{multi_grid_id}': {e}")
            # Fallback: try to get from current global selection
            grid_indices = get_selected_grid_indices()
            if not grid_indices:
                logging.debug("DEBUG: No grid selected for multi-plot")
                return
            lat_idx, lon_idx = grid_indices
        
        logging.debug(f"DEBUG: _add_current_plot - Using grid coordinates: ({lat_idx}, {lon_idx}) from grid ID: {multi_grid_id}")
        
        # Check unit compatibility with existing plots
        filepaths_map = reactive_netcdf_filepaths.get()
        compatible_plots = []
        multi_freq = multi_plot_frequency.get()
        
        for plot_config in plots_list:
            existing_var = plot_config['variable']
            existing_freq = plot_config.get('frequency', 'annual')
            if check_unit_compatibility(multi_var, existing_var, filepaths_map, multi_freq, existing_freq):
                compatible_plots.append(plot_config)
                logging.debug(f"DEBUG: Compatible with existing plot: {existing_var} ({existing_freq})")
            else:
                logging.debug(f"DEBUG: Incompatible with existing plot: {existing_var} ({existing_freq}) vs {multi_var} ({multi_freq})")
        
        # Decide where to add based on selection (handle main plot even if no existing multi-plots)
        # Determine main plot compatibility
        main_compatible = False
        try:
            main_var = selected_netcdf_variable.get()
            main_freq = selected_netcdf_frequency.get()
            multi_freq = multi_plot_frequency.get()
            if main_var:
                # FIXED: Pass frequency information for proper precipitation compatibility check
                main_compatible = check_unit_compatibility(multi_var, main_var, filepaths_map, multi_freq, main_freq)
                logging.debug(f"DEBUG: Unit compatibility check (with freq) - {multi_var} ({multi_freq}) vs {main_var} ({main_freq}): {main_compatible}")
        except Exception as e:
            logging.debug(f"DEBUG: Error in compatibility check: {e}")
            main_compatible = False

        # Read user's target selection from dropdown if available
        target_choice = None
        try:
            target_choice = input.target_plot_id()
        except Exception:
            target_choice = None

        logging.debug(f"DEBUG: _add_current_plot decision -> multi_action={multi_action}, target_choice={target_choice}, main_compatible={main_compatible}, compatible_plots={len(compatible_plots)}")

        # If user chose add_to_existing but did not change dropdown, pick sensible default
        if (target_choice is None or target_choice == "new") and multi_action == "add_to_existing":
            if main_compatible:
                target_choice = "main"
            elif compatible_plots:
                target_choice = str(compatible_plots[0]['id'])

        # Case 1: Add to main plot
        if target_choice == "main" and main_compatible:
            try:
                # FIXED: Get traces from both current and persistent storage with error handling
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent_traces = list(main_plot_persistent_traces.get() or [])
                
                # Create new trace configuration
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_frequency,
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx
                }
                
                # Check if this trace already exists to prevent duplicates
                trace_exists = False
                for existing_trace in current_traces + persistent_traces:
                    if (existing_trace.get('variable') == multi_var and 
                        existing_trace.get('model') == multi_model and 
                        existing_trace.get('ssp') == multi_ssp and
                        existing_trace.get('lat_idx') == lat_idx and 
                        existing_trace.get('lon_idx') == lon_idx):
                        trace_exists = True
                        break
                
                if not trace_exists:
                    # Add to both current and persistent storage
                    current_traces.append(new_trace)
                    persistent_traces.append(new_trace)
                    
                    # Safe reactive value updates with error handling
                    try:
                        main_plot_additional_traces.set(current_traces)
                        main_plot_persistent_traces.set(persistent_traces)
                        
                        # Increment trigger safely
                        current_trigger = main_plot_update_trigger.get()
                        main_plot_update_trigger.set(current_trigger + 1)
                        
                        multi_plot_status_message.set(f"âœ… Added {get_variable_display_name(multi_var)} to main plot")
                        logging.debug(f"DEBUG: Added trace to main plot: {new_trace}")
                        
                        # CRITICAL: Reset multi-plot steps after successful trace addition
                        # This allows the user to immediately add another trace
                        multi_plot_grid_id.set(None)
                        multi_plot_variable.set(None)
                        multi_plot_model.set(None)
                        multi_plot_frequency.set(None)
                        multi_plot_ssp.set(None)
                        multi_plot_action.set(None)
                        
                        multi_plot_step_1_completed.set(False)
                        multi_plot_step_2_completed.set(False)
                        multi_plot_step_3_completed.set(False)
                        multi_plot_step_4_completed.set(False)
                        multi_plot_step_5_completed.set(False)
                        multi_plot_step_6_completed.set(False)
                        
                        # Reset dropdown UI to blank state
                        ui.update_select("multi_plot_gridbox_selector", selected="")
                        ui.update_select("multi_plot_model", selected="")
                        ui.update_select("multi_plot_variable", selected="")
                        ui.update_select("multi_plot_frequency", selected="")
                        ui.update_select("multi_plot_ssp", selected="")
                        
                        logging.debug("DEBUG: âœ… Reset multi-plot steps after successful trace addition - ready for next selection!")
                        logging.debug("DEBUG: ðŸ—ºï¸ Map should remain interactive - steps reset to allow new clicks")
                        
                    except Exception as update_error:
                        logging.debug(f"DEBUG: Error updating reactive values: {update_error}")
                        # Fallback: try individual updates
                        try:
                            main_plot_additional_traces.set(current_traces)
                        except:
                            pass
                        try:
                            main_plot_persistent_traces.set(persistent_traces)
                        except:
                            pass
                        try:
                            multi_plot_status_message.set(f"âœ… Added {get_variable_display_name(multi_var)} to main plot")
                        except:
                            pass
            except Exception as trace_error:
                logging.debug(f"DEBUG: Error managing additional traces: {trace_error}")
                multi_plot_status_message.set(f"âš ï¸ Error adding trace: {str(trace_error)}")
                
                # Clear reactive values first
                multi_plot_grid_id.set(None)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                # Reset multi-plot step completion flags so user can make new selections
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                
                # Force UI update by triggering multi-plot update
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                
                logging.debug("DEBUG: Reset multi-plot selections after error")
            else:
                multi_plot_status_message.set(f"â„¹ï¸ {get_variable_display_name(multi_var)} is already on the main plot")
                logging.debug(f"DEBUG: Trace already exists on main plot: {new_trace}")
            
            # Reset all selections after successfully adding to main plot
            multi_plot_step_1_completed.set(False)
            multi_plot_step_2_completed.set(False)
            multi_plot_step_3_completed.set(False)
            multi_plot_step_4_completed.set(False)
            multi_plot_step_5_completed.set(False)
            multi_plot_step_6_completed.set(False)
            multi_plot_variable.set(None)
            multi_plot_model.set(None)
            multi_plot_frequency.set(None)
            multi_plot_ssp.set(None)
            multi_plot_action.set(None)
            multi_plot_grid_id.set(None)
            
            # IMPORTANT: Reset the actual UI dropdown inputs to blank/placeholder values
            ui.update_select("multi_plot_gridbox_selector", selected="")
            ui.update_select("multi_plot_model", selected="")
            ui.update_select("multi_plot_variable", selected="")
            ui.update_select("multi_plot_frequency", selected="")
            ui.update_select("multi_plot_ssp", selected="")
            
            # Trigger plot update to show the new trace
            main_plot_update_trigger.set(main_plot_update_trigger.get() + 1)
            
            multi_plot_status_message.set(f"âœ… Added {get_variable_display_name(multi_var)} to main plot. Selections reset for next addition.")
            logging.debug("DEBUG: Reset all selections and UI dropdowns after adding to main plot")
            return

        # Case 2: Add to an existing multi-plot by ID
        if target_choice and target_choice not in (None, "new", "main"):
            try:
                target_plot_id = int(target_choice)
            except Exception:
                target_plot_id = compatible_plots[0]['id'] if compatible_plots else None
            if target_plot_id is not None:
                updated_plots_list = []
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_frequency,
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx
                }
                for plot_config in plots_list:
                    if plot_config['id'] == target_plot_id:
                        plot_copy = dict(plot_config)
                        additional = list(plot_copy.get('additional_traces', []))
                        additional.append(new_trace)
                        plot_copy['additional_traces'] = additional
                        updated_plots_list.append(plot_copy)
                    else:
                        updated_plots_list.append(plot_config)
                multi_plots_list.set(updated_plots_list)
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                
                # Reset all selections after successfully adding to existing plot
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                multi_plot_grid_id.set(None)
                
                # IMPORTANT: Reset the actual UI dropdown inputs to blank/placeholder values
                ui.update_select("multi_plot_gridbox_selector", selected="")
                ui.update_select("multi_plot_model", selected="")
                ui.update_select("multi_plot_variable", selected="")
                ui.update_select("multi_plot_frequency", selected="")
                ui.update_select("multi_plot_ssp", selected="")
                
                multi_plot_status_message.set(f"âœ… Added to selected plot. Selections reset for next addition.")
                logging.debug("DEBUG: Reset all selections and UI dropdowns after adding to existing plot")
                return

        # Case 3: Check if we can create a new plot (only if no existing plots or units are compatible)
        if plots_list:
            # Check if there are any existing plots with incompatible units
            has_incompatible_plots = False
            for plot_config in plots_list:
                existing_var = plot_config['variable']
                existing_freq = plot_config.get('frequency', 'annual')
                if not check_unit_compatibility(multi_var, existing_var, filepaths_map, multi_freq, existing_freq):
                    has_incompatible_plots = True
                    logging.debug(f"DEBUG: Found incompatible plot: {existing_var} ({existing_freq}) vs {multi_var} ({multi_freq})")
                    break
            
            if has_incompatible_plots:
                multi_plot_status_message.set("âŒ Cannot create new plot: Units are not compatible with existing plots. Only one plot is allowed on the gridbox map.")
                # Reset all selections when units are incompatible
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                multi_plot_grid_id.set(None)
                
                # IMPORTANT: Reset the actual UI dropdown inputs to blank/placeholder values
                ui.update_select("multi_plot_gridbox_selector", selected="")
                ui.update_select("multi_plot_model", selected="")
                ui.update_select("multi_plot_variable", selected="")
                ui.update_select("multi_plot_frequency", selected="")
                ui.update_select("multi_plot_ssp", selected="")
                
                logging.debug("DEBUG: Reset all selections and UI dropdowns due to incompatible units")
                return
        
        # Create new plot only if no existing plots or all units are compatible
        plot_id = current_multi_plot_id.get()
        new_plot_config = {
            'id': plot_id,
            'variable': multi_var,
            'model': multi_model,
            'frequency': multi_frequency,
            'ssp': multi_ssp,
            'action': 'new_plot',
            'lat_idx': lat_idx,
            'lon_idx': lon_idx,
            'grid_id': multi_grid_id.get()  # Preserve the grid context
        }
        
        # Check unit compatibility with existing plots
        filepaths_map = reactive_netcdf_filepaths.get()
        plots_list = multi_plots_list.get() or []
        
        # Check if this exact trace already exists (to prevent duplicates)
        trace_signature = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
        existing_signatures = [(p['variable'], p['model'], p['frequency'], p['ssp'], p['lat_idx'], p['lon_idx']) for p in plots_list]
        
        if trace_signature in existing_signatures:
            logging.debug(f"DEBUG: Duplicate trace detected - not adding: {trace_signature}")
            multi_plot_status_message.set(f"âš ï¸ Trace already exists: {get_variable_display_name(multi_var)} ({multi_model}, {multi_frequency})")
            # Don't proceed with adding or incrementing plot ID
            return
        
        # If no existing plots, add the new plot
        if not plots_list:
            multi_plots_list.set([new_plot_config])
            logging.debug(f"DEBUG: Added first multi-plot: {new_plot_config}")
        else:
            # Check compatibility with existing plots (with frequency check for precipitation)
            compatible_plots = []
            for existing_plot in plots_list:
                existing_freq = existing_plot.get('frequency', 'annual')
                if check_unit_compatibility(multi_var, existing_plot['variable'], filepaths_map, multi_freq, existing_freq):
                    compatible_plots.append(existing_plot)
                    logging.debug(f"DEBUG: Compatible with existing plot: {existing_plot['variable']} ({existing_freq})")
                else:
                    logging.debug(f"DEBUG: Incompatible with existing plot: {existing_plot['variable']} ({existing_freq}) vs {multi_var} ({multi_freq})")
            
            # Add new plot if compatible
            if compatible_plots or len(plots_list) == 0:
                updated_plots_list = plots_list + [new_plot_config]
                multi_plots_list.set(updated_plots_list)
                logging.debug(f"DEBUG: Added compatible multi-plot: {new_plot_config}")
            else:
                logging.debug(f"DEBUG: Incompatible units - not adding plot: {new_plot_config}")
                return
        
        # Initialize time range for this plot based on SSP
        time_ranges = dict(multi_plot_time_ranges.get())
        if multi_var in ("obs_tmin", "obs_tmax", "obs_pr"):
            time_ranges[plot_id] = (1950, 2013)
        elif multi_ssp == "historical":
            time_ranges[plot_id] = (1950, 2014)
        else:
            time_ranges[plot_id] = (2015, 2100)
        multi_plot_time_ranges.set(time_ranges)
        
        current_multi_plot_id.set(plot_id + 1)
        multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
        multi_plot_status_message.set(f"âœ… Created new plot {plot_id + 1} with {get_variable_display_name(multi_var)}")
        
        logging.debug(f"DEBUG: Created new plot {plot_id} with grid coordinates ({lat_idx}, {lon_idx})")
        
        # Clear reactive values first
        multi_plot_grid_id.set(None)
        multi_plot_variable.set(None)
        multi_plot_model.set(None)
        multi_plot_frequency.set(None)
        multi_plot_ssp.set(None)
        multi_plot_action.set(None)
        
        # Reset multi-plot step completion flags so user can make new selections
        multi_plot_step_1_completed.set(False)
        multi_plot_step_2_completed.set(False)
        multi_plot_step_3_completed.set(False)
        multi_plot_step_4_completed.set(False)
        multi_plot_step_5_completed.set(False)
        multi_plot_step_6_completed.set(False)
        
        # Force UI update by triggering multi-plot update
        multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
        
        logging.debug("DEBUG: Reset multi-plot selections and triggered UI update after creating new plot")
        
        logging.debug("DEBUG: Reset all selections after creating new plot")

    # Auto-add a multi-plot when the progressive dropdown is completed
    @reactive.Effect
    @reactive.event(multi_plot_step_5_completed)
    def _auto_add_plot_when_complete():
        try:
            logging.debug("DEBUG: _auto_add_plot_when_complete triggered")
            if not multi_plot_step_5_completed.get():
                logging.debug("DEBUG: step 5 not completed; exiting auto-add")
                return
            
            # Add protection against recursive calls
            if hasattr(_auto_add_plot_when_complete, '_processing'):
                logging.debug("DEBUG: Already processing auto-add; skipping to prevent recursion")
                return
            _auto_add_plot_when_complete._processing = True
            
            # Check if we've already processed this selection to prevent duplicates
            multi_var = multi_plot_variable.get()
            multi_model = multi_plot_model.get()
            multi_ssp = multi_plot_ssp.get()
            
            # For observed variables, model and ssp might be None or set to defaults
            if not multi_var:
                logging.debug("DEBUG: auto-add missing variable; exiting")
                return
            
            # Handle observed variables that don't have explicit model/ssp
            if multi_var.startswith('obs_'):
                if not multi_model:
                    multi_model = 'Observed'
                if not multi_ssp:
                    multi_ssp = 'historical'
            elif not (multi_model and multi_ssp):
                logging.debug("DEBUG: auto-add missing model/ssp for non-observed variable; exiting")
                return
            
            # Defer duplicate checks until after we resolve grid indices, so
            # same variable/model/ssp from different grid cells are allowed
            
            # FIXED: Also check if this selection is already in the current plot combinations
            # This prevents auto-add from running when the trace is already being processed by the plot function
            current_plot_combinations = []
            try:
                # Get the current plot combinations from the main plot function
                main_var = selected_netcdf_variable.get()
                main_model = selected_netcdf_model.get()
                main_freq = selected_netcdf_frequency.get()
                main_ssp = selected_netcdf_ssp.get()
                
                if main_var and main_model and main_freq and main_ssp:
                    current_plot_combinations.append((main_var, main_model, main_freq, main_ssp))
                    
                    # Check if the current multi-plot selection is already included
                    multi_freq = multi_plot_frequency.get() or 'annual'
                    current_multi_combo = (multi_var, multi_model, multi_freq, multi_ssp)
                    
                    # Check if this is a loop-based selection
                    multi_loop_gridboxes = multi_plot_loop_gridboxes.get()
                    is_loop = multi_loop_gridboxes is not None and len(multi_loop_gridboxes) > 0
                    
                    # If it's a loop, it can never be a duplicate of a single-gridbox main plot
                    # Only check for duplicates if both are single-gridbox traces
                    if not is_loop:
                        # Allow adding the same variable/model/ssp if it's from a different grid location
                        multi_grid_id = multi_plot_grid_id.get()
                        main_grid_id = get_selected_grid_indices()
                        if main_grid_id and multi_grid_id:
                            try:
                                multi_lat, multi_lon = map(int, multi_grid_id.split(","))
                                main_lat, main_lon = main_grid_id
                                if (multi_lat, multi_lon) == (main_lat, main_lon) and current_multi_combo in current_plot_combinations:
                                    logging.debug("DEBUG: auto-add selection already in current plot combinations (same grid); exiting to prevent duplicates")
                                    return
                                else:
                                    logging.debug(f"DEBUG: Different gridboxes - main ({main_lat},{main_lon}) vs multi ({multi_lat},{multi_lon}) - allowing trace")
                            except Exception as e:
                                logging.debug(f"DEBUG: Error comparing grid locations in auto-add: {e}")
                                # REMOVED: The overly broad duplicate check that didn't compare grid coordinates
                                # This was blocking different gridboxes with the same parameters
                    else:
                        logging.debug(f"DEBUG: auto-add loop detected - skipping duplicate check (loops are never duplicates of single-gridbox traces)")
            except Exception as e:
                logging.debug(f"DEBUG: Error checking current plot combinations: {e}")
            
            # Always add to main plot - no action selection needed
            logging.debug(f"DEBUG: auto-add selections -> var={multi_var}, model={multi_model}, ssp={multi_ssp}")
            # Check main plot compatibility using the standard compatibility function
            filepaths_map = reactive_netcdf_filepaths.get()
            main_var = selected_netcdf_variable.get()
            main_model = selected_netcdf_model.get()
            main_freq = selected_netcdf_frequency.get()
            main_ssp = selected_netcdf_ssp.get()
            multi_freq = multi_plot_frequency.get()
            try:
                # FIXED: Pass frequency information for proper precipitation compatibility check
                main_compatible = check_unit_compatibility(multi_var, main_var, filepaths_map, multi_freq, main_freq) if main_var else False
                logging.debug(f"DEBUG: auto-add compatibility check (with freq) - {multi_var} ({multi_freq}) vs {main_var} ({main_freq}): {main_compatible}")
            except Exception as e:
                logging.debug(f"DEBUG: Error in auto-add compatibility check: {e}")
                main_compatible = False
            logging.debug(f"DEBUG: auto-add main_compatible: {main_compatible} (main={main_var}, {main_model}, {main_ssp})")
            # Always try to add to main plot - check compatibility first
            if main_compatible:
                # Check if this is a loop selection
                multi_loop_gridboxes = multi_plot_loop_gridboxes.get()
                multi_loop_name = multi_plot_loop_name.get()
                multi_loop_polygon = multi_plot_loop_polygon.get()
                
                if multi_loop_gridboxes and multi_loop_name:
                    # This is a loop-based trace
                    logging.debug(f"DEBUG: auto-add to main using loop: {multi_loop_name} with {len(multi_loop_gridboxes)} gridboxes")
                    lat_idx = lon_idx = None  # Loops don't use single grid indices
                else:
                    # Determine grid coordinates for the additional trace
                    multi_grid_id = multi_plot_grid_id.get()
                    if multi_grid_id and "," in multi_grid_id:
                        try:
                            lat_idx, lon_idx = map(int, multi_grid_id.split(","))
                            logging.debug(f"DEBUG: auto-add to main using multi-plot grid coordinates: ({lat_idx}, {lon_idx})")
                        except Exception:
                            lat_idx = lon_idx = None
                            logging.debug(f"DEBUG: auto-add to main error parsing multi_grid_id: {multi_grid_id}")
                    else:
                        # Fallback to main plot grid if multi-plot grid not available
                        grid_indices = get_selected_grid_indices()
                        if grid_indices:
                            lat_idx, lon_idx = grid_indices
                            logging.debug(f"DEBUG: auto-add to main fallback to main plot grid coordinates: ({lat_idx}, {lon_idx})")
                        else:
                            lat_idx = lon_idx = None
                
                # Only check for missing grid indices if NOT a loop
                # Loops intentionally have None for lat_idx and lon_idx
                if not (multi_loop_gridboxes and multi_loop_name):
                    if lat_idx is None or lon_idx is None:
                        logging.debug("DEBUG: auto-add to main missing grid indices; exiting")
                        return

                # Prepare new trace configuration for main plot
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_plot_frequency.get() or 'annual',
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx,
                    'loop_gridboxes': multi_loop_gridboxes if multi_loop_gridboxes else None,
                    'loop_name': multi_loop_name if multi_loop_name else None,
                    'loop_polygon': multi_loop_polygon if multi_loop_polygon else None
                }

                # Append to main plot additional traces if not already present
                try:
                    current_traces = list(main_plot_additional_traces.get() or [])
                    persistent_traces = list(main_plot_persistent_traces.get() or [])
                    trace_exists = False
                    for existing_trace in current_traces + persistent_traces:
                        # Check if all parameters match: variable, model, frequency, and SSP
                        params_match = (existing_trace.get('variable') == new_trace['variable'] and
                            existing_trace.get('model') == new_trace['model'] and
                                       existing_trace.get('frequency') == new_trace['frequency'] and
                                       existing_trace.get('ssp') == new_trace['ssp'])
                        
                        if not params_match:
                            continue
                        
                        # For gridbox traces (lat_idx and lon_idx are not None)
                        # Check if coordinates match
                        existing_lat = existing_trace.get('lat_idx')
                        existing_lon = existing_trace.get('lon_idx')
                        new_lat = new_trace['lat_idx']
                        new_lon = new_trace['lon_idx']
                        
                        # Both are gridbox traces (not loops)
                        if existing_lat is not None and existing_lon is not None and new_lat is not None and new_lon is not None:
                            if existing_lat == new_lat and existing_lon == new_lon:
                                trace_exists = True
                                logging.debug(f"DEBUG: Duplicate detected - same gridbox ({new_lat},{new_lon}) with identical parameters")
                                break
                        
                        # Both are loop traces (lat_idx and lon_idx are None)
                        elif existing_lat is None and existing_lon is None and new_lat is None and new_lon is None:
                            # Compare loop gridboxes WITH weights to see if they're identical
                            existing_loop_gridboxes = existing_trace.get('loop_gridboxes')
                            new_loop_gridboxes = new_trace['loop_gridboxes']
                            
                            if existing_loop_gridboxes and new_loop_gridboxes:
                                # Sort both lists by grid_id for consistent comparison
                                existing_sorted = sorted(existing_loop_gridboxes, key=lambda x: x[0])
                                new_sorted = sorted(new_loop_gridboxes, key=lambda x: x[0])
                                
                                # Compare as lists of tuples (grid_id, weight)
                                # This checks both gridbox IDs AND their weights
                                if existing_sorted == new_sorted:
                                    trace_exists = True
                                    logging.debug(f"DEBUG: Duplicate detected - same loop gridboxes with identical weights ({len(new_sorted)} gridboxes)")
                                    break
                                else:
                                    logging.debug(f"DEBUG: Different loops - existing: {len(existing_sorted)} gridboxes, new: {len(new_sorted)} gridboxes (or different weights)")
                            elif not existing_loop_gridboxes and not new_loop_gridboxes:
                                # Both are loops but no gridbox data - treat as duplicate to be safe
                                trace_exists = True
                                logging.debug(f"DEBUG: Duplicate detected - both loops without gridbox data")
                                break
                        
                        # One is loop, one is gridbox - not a duplicate
                        else:
                            logging.debug(f"DEBUG: Not duplicate - one is loop, one is gridbox")
                    if not trace_exists:
                        current_traces.append(new_trace)
                        persistent_traces.append(new_trace)
                        
                        # Safe reactive value updates with error handling
                        try:
                            main_plot_additional_traces.set(current_traces)
                            main_plot_persistent_traces.set(persistent_traces)
                            logging.debug(f"DEBUG: auto-added trace to main plot: {new_trace}")
                            multi_plot_status_message.set(f"âœ… Added {get_variable_display_name(multi_var)} to main plot")
                            # Trigger a plot update safely
                            current_main_trigger = main_plot_update_trigger.get()
                            main_plot_update_trigger.set(current_main_trigger + 1)
                            current_plot_trigger = plot_update_trigger.get()
                            plot_update_trigger.set(current_plot_trigger + 1)
                            
                        except Exception as update_error:
                            logging.debug(f"DEBUG: Error updating reactive values in auto-add: {update_error}")
                            # Fallback: try individual updates
                            try:
                                main_plot_additional_traces.set(current_traces)
                            except:
                                pass
                            try:
                                main_plot_persistent_traces.set(persistent_traces)
                            except:
                                pass
                            try:
                                multi_plot_status_message.set(f"âœ… Added {get_variable_display_name(multi_var)} to main plot")
                            except:
                                pass
                    else:
                        multi_plot_status_message.set(f"â„¹ï¸ {get_variable_display_name(multi_var)} with the same options is already on the main plot. Try selecting a different frequency to add another trace.")
                        logging.debug(f"DEBUG: auto-add to main skipped duplicate trace: {new_trace}")
                                
                except Exception as auto_add_error:
                    logging.debug(f"DEBUG: Error in auto-add trace logic: {auto_add_error}")
                    multi_plot_status_message.set(f"âš ï¸ Error auto-adding trace: {str(auto_add_error)}")
                    
                # CRITICAL: Always reset multi-plot steps after auto-add (whether successful, duplicate, or error)
                # This allows the user to make a new selection
                    multi_plot_grid_id.set(None)
                    multi_plot_variable.set(None)
                    multi_plot_model.set(None)
                    multi_plot_frequency.set(None)
                    multi_plot_ssp.set(None)
                    multi_plot_action.set(None)
                
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                
                # Reset dropdown UI to blank
                ui.update_select("multi_plot_gridbox_selector", selected="")
                ui.update_select("multi_plot_model", selected="")
                ui.update_select("multi_plot_variable", selected="")
                ui.update_select("multi_plot_frequency", selected="")
                ui.update_select("multi_plot_ssp", selected="")
                
                # Clear loop data for multi-plot selection (always do this)
                multi_plot_loop_polygon.set(None)
                multi_plot_loop_gridboxes.set(None)
                multi_plot_loop_name.set(None)
                logging.debug("DEBUG: âœ… Reset multi-plot steps after auto-add - ready for next selection!")
                logging.debug("DEBUG: ðŸ—ºï¸ Map should remain interactive - steps reset to allow new clicks")
                logging.debug("DEBUG: Cleared multi-plot loop data after trace processing")
                return
            else:
                # Incompatible trace - show error message
                multi_plot_status_message.set(f"âš ï¸ {get_variable_display_name(multi_var)} has incompatible units with the main plot. Please start a new plot or select a compatible variable.")
                logging.debug(f"DEBUG: Incompatible trace rejected: {multi_var} vs {main_var}")
                
            # Clear all selections after successful auto-add or rejection (this should not be reached due to early returns above)
            multi_plot_variable.set(None)
            multi_plot_model.set(None)
            multi_plot_frequency.set(None)
            multi_plot_ssp.set(None)
            multi_plot_action.set(None)
            multi_plot_grid_id.set(None)
            
            # Clear loop data for multi-plot selection
            multi_plot_loop_polygon.set(None)
            multi_plot_loop_gridboxes.set(None)
            multi_plot_loop_name.set(None)
            
            # Reset multi-plot step completion flags so user can make new selections
            multi_plot_step_1_completed.set(False)
            multi_plot_step_2_completed.set(False)
            multi_plot_step_3_completed.set(False)
            multi_plot_step_4_completed.set(False)
            multi_plot_step_5_completed.set(False)
            multi_plot_step_6_completed.set(False)
            
            # Force UI update by triggering multi-plot update
            multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
            
            logging.debug("DEBUG: auto-add reset steps and cleared selections for next multi-plot selection")
        except Exception as e:
            logging.debug(f"DEBUG: _auto_add_plot_when_complete error: {e}")
        finally:
            # Clear the processing flag
            if hasattr(_auto_add_plot_when_complete, '_processing'):
                delattr(_auto_add_plot_when_complete, '_processing')

    # Event handlers for individual time series sliders
    @reactive.Effect
    @reactive.event(
        input.multi_plot_time_range_0, input.multi_plot_time_range_1, input.multi_plot_time_range_2,
        input.multi_plot_time_range_3, input.multi_plot_time_range_4, input.multi_plot_time_range_5,
        input.multi_plot_time_range_6, input.multi_plot_time_range_7, input.multi_plot_time_range_8,
        input.multi_plot_time_range_9, input.multi_plot_time_range_10, input.multi_plot_time_range_11,
        input.multi_plot_time_range_12, input.multi_plot_time_range_13, input.multi_plot_time_range_14,
        input.multi_plot_time_range_15, input.multi_plot_time_range_16, input.multi_plot_time_range_17,
        input.multi_plot_time_range_18, input.multi_plot_time_range_19, input.multi_plot_time_range_20,
        # Add more sliders dynamically if needed
    )
    def _update_multi_plot_time_range():
        """Update time range for a specific multi-plot"""
        # Determine which slider triggered this effect via reactive context
        try:
            # Get the current reactive context to determine which input changed
            context = reactive.get_current_context()
            if context and hasattr(context, 'name'):
                input_name = context.name
                logging.debug(f"DEBUG: Multi-plot time range slider changed: {input_name}")
                
                # Extract plot ID from input name (e.g., "multi_plot_time_range_0" -> 0)
                if "_" in input_name and input_name.endswith(("_0", "_1", "_2", "_3", "_4", "_5", "_6", "_7", "_8", "_9")):
                    plot_id_str = input_name.split("_")[-1]
                    try:
                        plot_id = int(plot_id_str)
                        logging.debug(f"DEBUG: Updating time range for multi-plot {plot_id}")
                        
                        # Get the current time range value
                        time_range = input[input_name]()
                        if time_range and len(time_range) == 2:
                            start_year, end_year = time_range
                            logging.debug(f"DEBUG: New time range for plot {plot_id}: {start_year}-{end_year}")
                            
                            # Update the specific plot's time range
                            plots_list = multi_plots_list.get()
                            if plots_list and plot_id < len(plots_list):
                                plot_config = plots_list[plot_id]
                                plot_config['time_range'] = [start_year, end_year]
                                
                                # Update the plots list
                                updated_plots = list(plots_list)
                                updated_plots[plot_id] = plot_config
                                multi_plots_list.set(updated_plots)
                                
                                # Trigger plot update
                                plot_update_trigger.set(plot_update_trigger.get() + 1)
                                logging.debug(f"DEBUG: Updated time range for plot {plot_id} to {start_year}-{end_year}")
                            else:
                                logging.debug(f"DEBUG: Plot {plot_id} not found in plots list")
                        else:
                            logging.debug(f"DEBUG: Invalid time range format: {time_range}")
                    except ValueError:
                        logging.debug(f"DEBUG: Could not parse plot ID from input name: {input_name}")
                else:
                    logging.debug(f"DEBUG: Unexpected input name format: {input_name}")
            else:
                logging.debug("DEBUG: Could not determine which input changed")
        except Exception as e:
            logging.debug(f"DEBUG: Error updating multi-plot time range: {e}")

    # Event handlers for individual trace time series sliders within multi-plots
    @reactive.Effect
    @reactive.event(
        input.trace_time_range_0_0, input.trace_time_range_0_1, input.trace_time_range_0_2, input.trace_time_range_0_3, input.trace_time_range_0_4,
        input.trace_time_range_1_0, input.trace_time_range_1_1, input.trace_time_range_1_2, input.trace_time_range_1_3, input.trace_time_range_1_4,
        input.trace_time_range_2_0, input.trace_time_range_2_1, input.trace_time_range_2_2, input.trace_time_range_2_3, input.trace_time_range_2_4,
        input.trace_time_range_3_0, input.trace_time_range_3_1, input.trace_time_range_3_2, input.trace_time_range_3_3, input.trace_time_range_3_4,
        input.trace_time_range_4_0, input.trace_time_range_4_1, input.trace_time_range_4_2, input.trace_time_range_4_3, input.trace_time_range_4_4,
        input.trace_time_range_5_0, input.trace_time_range_5_1, input.trace_time_range_5_2, input.trace_time_range_5_3, input.trace_time_range_5_4,
        input.trace_time_range_6_0, input.trace_time_range_6_1, input.trace_time_range_6_2, input.trace_time_range_6_3, input.trace_time_range_6_4,
        input.trace_time_range_7_0, input.trace_time_range_7_1, input.trace_time_range_7_2, input.trace_time_range_7_3, input.trace_time_range_7_4,
        input.trace_time_range_8_0, input.trace_time_range_8_1, input.trace_time_range_8_2, input.trace_time_range_8_3, input.trace_time_range_8_4,
        input.trace_time_range_9_0, input.trace_time_range_9_1, input.trace_time_range_9_2, input.trace_time_range_9_3, input.trace_time_range_9_4
    )
    def _update_trace_time_range():
        """Update time range for a specific trace within a multi-plot"""
        # Determine which slider triggered this effect via reactive context
        try:
            # Get the current reactive context to determine which input changed
            context = reactive.get_current_context()
            if context and hasattr(context, 'name'):
                input_name = context.name
                logging.debug(f"DEBUG: Trace time range slider changed: {input_name}")
                
                # Extract plot ID and trace ID from input name (e.g., "trace_time_range_0_1" -> plot 0, trace 1)
                if "_" in input_name:
                    parts = input_name.split("_")
                    if len(parts) >= 5 and parts[0] == "trace" and parts[1] == "time" and parts[2] == "range":
                        try:
                            plot_id = int(parts[3])
                            trace_id = int(parts[4])
                            logging.debug(f"DEBUG: Updating time range for plot {plot_id}, trace {trace_id}")
                            
                            # Get the current time range value
                            time_range = input[input_name]()
                            if time_range and len(time_range) == 2:
                                start_year, end_year = time_range
                                logging.debug(f"DEBUG: New time range for plot {plot_id}, trace {trace_id}: {start_year}-{end_year}")
                                
                                # Update the specific trace's time range
                                plots_list = multi_plots_list.get()
                                if plots_list and plot_id < len(plots_list):
                                    plot_config = plots_list[plot_id]
                                    additional_traces = plot_config.get('additional_traces', [])
                                    
                                    if trace_id < len(additional_traces):
                                        trace_config = additional_traces[trace_id]
                                        trace_config['time_range'] = [start_year, end_year]
                                        
                                        # Update the plots list
                                        updated_plots = list(plots_list)
                                        updated_plots[plot_id] = plot_config
                                        multi_plots_list.set(updated_plots)
                                        
                                        # Trigger plot update
                                        plot_update_trigger.set(plot_update_trigger.get() + 1)
                                        logging.debug(f"DEBUG: Updated time range for plot {plot_id}, trace {trace_id} to {start_year}-{end_year}")
                                    else:
                                        logging.debug(f"DEBUG: Trace {trace_id} not found in plot {plot_id}")
                                else:
                                    logging.debug(f"DEBUG: Plot {plot_id} not found in plots list")
                            else:
                                logging.debug(f"DEBUG: Invalid time range format: {time_range}")
                        except ValueError:
                            logging.debug(f"DEBUG: Could not parse plot/trace IDs from input name: {input_name}")
                    else:
                        logging.debug(f"DEBUG: Unexpected input name format: {input_name}")
                else:
                    logging.debug(f"DEBUG: Unexpected input name format: {input_name}")
            else:
                logging.debug("DEBUG: Could not determine which input changed")
        except Exception as e:
            logging.debug(f"DEBUG: Error updating trace time range: {e}")
    # Function to generate multi-plot visualizations
    def generate_multi_plot(plot_config, plot_id):
        """Generate a single multi-plot visualization"""
        try:
            logging.debug(f"DEBUG: generate_multi_plot called for plot {plot_id}")
            
            # Extract plot configuration
            plot_var = plot_config.get('variable')
            plot_model = plot_config.get('model')
            plot_frequency = plot_config.get('frequency', 'annual')
            plot_ssp = plot_config.get('ssp')
            plot_lat_idx = plot_config.get('lat_idx')
            plot_lon_idx = plot_config.get('lon_idx')
            additional_traces = plot_config.get('additional_traces', [])
            
            logging.debug(f"DEBUG: generate_multi_plot - Plot {plot_id}: {plot_var}, {plot_model}, {plot_frequency}, {plot_ssp}, grid ({plot_lat_idx}, {plot_lon_idx})")
            logging.debug(f"DEBUG: generate_multi_plot - Additional traces: {len(additional_traces)}")
            
            if not all([plot_var, plot_model, plot_ssp, plot_lat_idx is not None, plot_lon_idx is not None]):
                logging.debug(f"DEBUG: generate_multi_plot - Incomplete plot configuration for plot {plot_id}")
                return None
            
            # Determine file path for main plot
            filepaths_map = reactive_netcdf_filepaths.get()
            if not filepaths_map:
                logging.debug(f"DEBUG: generate_multi_plot - No filepaths available")
                return None
            
            # Handle observed variables
            if plot_var.startswith('obs_'):
                # For observed data, use the observed dataset loading function
                ds = load_observed_dataset(plot_var)
                if ds is None:
                    logging.debug(f"DEBUG: generate_multi_plot - Could not load observed dataset for {plot_var}")
                    return None
                file_path = None  # Observed data doesn't use file paths
                logging.debug(f"DEBUG: generate_multi_plot - Loaded observed dataset for {plot_var}")
            else:
                # For projected data, find the appropriate file
                file_key = (plot_var, plot_model, plot_ssp)
                if file_key not in filepaths_map:
                    logging.debug(f"DEBUG: generate_multi_plot - File not found for key: {file_key}")
                    return None
                
                file_path = filepaths_map[file_key]
                logging.debug(f"DEBUG: generate_multi_plot - Using file: {file_path}")
                
                # Load the dataset
                try:
                    ds = xr.open_dataset(file_path, engine="netcdf4", decode_times=False)
                    logging.debug(f"DEBUG: generate_multi_plot - Successfully loaded dataset")
                except Exception as e:
                    logging.debug(f"DEBUG: generate_multi_plot - Error loading dataset: {e}")
                    return None
            
            # Extract data for the specified grid cell
            try:
                if plot_var.startswith('obs_'):
                    # For observed data, extract using the observed data function
                    df_processed = extract_timeseries(ds, plot_lat_idx, plot_lon_idx, plot_var)
                    if df_processed is None or df_processed.empty:
                        logging.debug(f"DEBUG: generate_multi_plot - No observed data found for grid ({plot_lat_idx}, {plot_lon_idx})")
                        return None
                else:
                    # For projected data, extract using the standard function
                    df_processed = extract_timeseries(ds, plot_lat_idx, plot_lon_idx, plot_var)
                    if df_processed is None or df_processed.empty:
                        logging.debug(f"DEBUG: generate_multi_plot - No projected data found for grid ({plot_lat_idx}, {plot_lon_idx})")
                        return None
                
                logging.debug(f"DEBUG: generate_multi_plot - Extracted {len(df_processed)} rows of data")
            except Exception as e:
                logging.debug(f"DEBUG: generate_multi_plot - Error extracting data: {e}")
                return None
            
            # Process the data based on frequency
            try:
                if plot_frequency == 'daily':
                    # Daily data - no resampling needed
                    df_freq = df_processed.copy()
                    logging.debug(f"DEBUG: generate_multi_plot - Using daily data with {len(df_freq)} rows")
                elif plot_frequency == 'monthly':
                    # For precipitation, sum to get monthly totals; for other variables, average
                    if plot_var in ["pr", "obs_pr"]:
                        df_freq = df_processed.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                        logging.debug(f"DEBUG: generate_multi_plot - Created monthly total precipitation data with {len(df_freq)} rows")
                    else:
                        df_freq = df_processed.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                        logging.debug(f"DEBUG: generate_multi_plot - Created monthly average data with {len(df_freq)} rows")
                    df_freq["Year"] = df_freq["Date"].dt.year
                elif plot_frequency == 'annual':
                    # For precipitation, sum to get annual totals; for other variables, average
                    if plot_var in ["pr", "obs_pr"]:
                        df_freq = df_processed.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                        logging.debug(f"DEBUG: generate_multi_plot - Created annual total precipitation data with {len(df_freq)} rows")
                    else:
                        df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                        logging.debug(f"DEBUG: generate_multi_plot - Created annual average data with {len(df_freq)} rows")
                    df_freq["Year"] = df_freq["Date"].dt.year
                else:
                    # Default to annual if frequency not specified
                    df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                    df_freq["Year"] = df_freq["Date"].dt.year
                    logging.debug(f"DEBUG: generate_multi_plot - Defaulted to annual averages with {len(df_freq)} rows")
                
                # Handle temperature unit conversion for observed data
                if plot_var in ("obs_tmin", "obs_tmax"):
                    # Observed temperature to Â°F - Livneh data is always in Celsius
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    # Livneh observed data is in Celsius - always convert to Fahrenheit
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    logging.debug(f"DEBUG: generate_multi_plot - Sample values before conversion: {sample_values}")
                    logging.debug(f"DEBUG: generate_multi_plot - Average sample value: {avg_value:.2f}")
                    
                    # Convert from Celsius to Fahrenheit for observed data
                    if src_units in ['c', 'degc', 'celsius', 'Â°c', 'deg_c', 'degrees_celsius']:
                        logging.debug(f"DEBUG: generate_multi_plot - Units explicitly indicate Celsius - converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] * 9/5) + 32
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after Celsius conversion: {df_freq[plot_var].head().tolist()}")
                    elif src_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                        logging.debug(f"DEBUG: generate_multi_plot - Units explicitly indicate Kelvin - converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after Kelvin conversion: {df_freq[plot_var].head().tolist()}")
                    elif src_units in ['f', 'degf', 'fahrenheit', 'Â°f', 'deg_f', 'degrees_fahrenheit']:
                        logging.debug(f"DEBUG: generate_multi_plot - Units explicitly indicate Fahrenheit - no conversion needed")
                        # Data is already in Fahrenheit, no conversion needed
                    else:
                        # For Livneh observed data, if no units specified, assume Celsius (as confirmed by user)
                        logging.debug(f"DEBUG: generate_multi_plot - No units specified - Livneh data is in Celsius, converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] * 9/5) + 32
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after Celsius conversion: {df_freq[plot_var].head().tolist()}")
                    display_units = "Â°F"
                
                # Handle precipitation unit conversion for observed data
                elif plot_var == "obs_pr":
                    # Observed precipitation to inches - Livneh data is in mm/day
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    # Livneh observed precipitation data is in mm/day - always convert to inches
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    logging.debug(f"DEBUG: generate_multi_plot - Sample precipitation values before conversion: {sample_values}")
                    logging.debug(f"DEBUG: generate_multi_plot - Average sample precipitation value: {avg_value:.2f}")
                    
                    # Convert from mm/day to inches
                    if src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                        df_freq[plot_var] = df_freq[plot_var] / 25.4
                        logging.debug(f"DEBUG: generate_multi_plot - Converted from mm/day to inches")
                        display_units = "inches/day" # Always inches for all frequencies
                    elif src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        logging.debug(f"DEBUG: generate_multi_plot - Converted from kg m-2 s-1 to inches")
                        display_units = "inches/day" # Always inches for all frequencies
                    else:
                        # For Livneh observed data, if no units specified, assume mm/day (as confirmed by user)
                        logging.debug(f"DEBUG: generate_multi_plot - No units specified - Livneh precipitation data is in mm/day, converting to inches")
                        df_freq[plot_var] = (df_freq[plot_var] / 25.4)
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day" # Always inches for all frequencies
                
                # Handle projected precipitation unit conversion
                elif plot_var == "pr":
                    # Projected precipitation conversion - typically in kg m-2 s-1
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    logging.debug(f"DEBUG: generate_multi_plot - Sample projected precipitation values before conversion: {sample_values}")
                    logging.debug(f"DEBUG: generate_multi_plot - Average sample projected precipitation value: {avg_value:.2f}")
                    logging.debug(f"DEBUG: generate_multi_plot - Projected precipitation units: {src_units}")
                    
                    if src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        logging.debug(f"DEBUG: generate_multi_plot - Converted projected precipitation from kg m-2 s-1 to inches")
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                    elif src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                        df_freq[plot_var] = df_freq[plot_var] / 25.4
                        logging.debug(f"DEBUG: generate_multi_plot - Converted projected precipitation from mm/day to inches")
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                    else:
                        # Default: assume kg m-2 s-1
                        logging.debug(f"DEBUG: generate_multi_plot - No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        logging.debug(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                
                # Handle projected data unit conversion
                elif not plot_var.startswith('obs_'):
                    # Projected data unit conversion
                    if plot_var in ["tasmin", "tasmax"]:
                        # Temperature from Kelvin to Fahrenheit
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                            df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                            display_units = "Â°F"
                        elif original_units in ['c', 'degc', 'celsius', 'Â°c', 'deg_c', 'degrees_celsius']:
                            df_freq[plot_var] = df_freq[plot_var] * 9/5 + 32
                            display_units = "Â°F"
                        elif original_units in ['f', 'degf', 'fahrenheit', 'Â°f', 'deg_f', 'degrees_fahrenheit']:
                            display_units = "Â°F"
                        else:
                            # Default: assume Kelvin
                            df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                            display_units = "Â°F"
                    elif plot_var == "pr":
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                            df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                            # All precipitation data should be in "inches" - daily totals, monthly averages, annual averages
                            display_units = "inches/day"
                        else:
                            # Default: assume kg m-2 s-1
                            df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                            display_units = "inches/day"
                    elif plot_var in ["hursmax", "hursmin"]:
                        # Relative humidity conversion - typically in % but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['%', 'percent', 'pct']:
                            # Already in percentage, no conversion needed
                            logging.debug(f"DEBUG: generate_multi_plot - {plot_var} already in percentage - no conversion needed")
                            display_units = "%"
                        elif original_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                            # Convert from fraction to percentage
                            df_freq[plot_var] = df_freq[plot_var] * 100
                            logging.debug(f"DEBUG: generate_multi_plot - Converted {plot_var} from fraction to percentage")
                            display_units = "%"
                        else:
                            # Default: assume percentage
                            logging.debug(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in percentage")
                            display_units = "%"
                    elif plot_var == "huss":
                        # Specific humidity conversion - typically in kg/kg but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                            # Already in kg/kg, no conversion needed
                            logging.debug(f"DEBUG: generate_multi_plot - {plot_var} already in kg/kg - no conversion needed")
                            display_units = "kg/kg"
                        elif original_units in ['g/kg', 'g kg-1', 'g per kg']:
                            # Convert from g/kg to kg/kg
                            df_freq[plot_var] = df_freq[plot_var] / 1000
                            logging.debug(f"DEBUG: generate_multi_plot - Converted {plot_var} from g/kg to kg/kg")
                            display_units = "kg/kg"
                        else:
                            # Default: assume kg/kg
                            logging.debug(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in kg/kg")
                            display_units = "kg/kg"
                    elif plot_var == "rsds":
                        # Radiation conversion - typically in W/mÂ² but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['w/m2', 'w m-2', 'watt/m2', 'watt m-2', 'w/mÂ²']:
                            # Already in W/mÂ², no conversion needed
                            logging.debug(f"DEBUG: generate_multi_plot - {plot_var} already in W/mÂ² - no conversion needed")
                            display_units = "W/mÂ²"
                        elif original_units in ['w/m2/day', 'w m-2 day-1', 'watt/m2/day']:
                            # Convert from W/mÂ²/day to W/mÂ² (divide by 86400 seconds)
                            df_freq[plot_var] = df_freq[plot_var] / 86400
                            logging.debug(f"DEBUG: generate_multi_plot - Converted {plot_var} from W/mÂ²/day to W/mÂ²")
                            display_units = "W/mÂ²"
                        else:
                            # Default: assume W/mÂ²
                            logging.debug(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in W/mÂ²")
                            display_units = "W/mÂ²"
                    elif plot_var == "wspeed":
                        # Wind speed conversion - typically in m/s but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['m/s', 'm s-1', 'meter/s', 'meter s-1', 'ms-1']:
                            # Already in m/s, no conversion needed
                            logging.debug(f"DEBUG: generate_multi_plot - {plot_var} already in m/s - no conversion needed")
                            display_units = "m/s"
                        elif original_units in ['km/h', 'kmh', 'km per hour']:
                            # Convert from km/h to m/s
                            df_freq[plot_var] = df_freq[plot_var] * 1000 / 3600
                            logging.debug(f"DEBUG: generate_multi_plot - Converted {plot_var} from km/h to m/s")
                            display_units = "m/s"
                        elif original_units in ['knots', 'kt', 'kts']:
                            # Convert from knots to m/s
                            df_freq[plot_var] = df_freq[plot_var] * 0.514444
                            logging.debug(f"DEBUG: generate_multi_plot - Converted {plot_var} from knots to m/s")
                            display_units = "m/s"
                        else:
                            # Default: assume m/s
                            logging.debug(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in m/s")
                            display_units = "m/s"
                    else:
                        # For other variables, use the units from the dataset
                        display_units = ds[plot_var].attrs.get('units', '')
                
                logging.debug(f"DEBUG: generate_multi_plot - Data processing complete, display units: {display_units}")
                
            except Exception as e:
                logging.debug(f"DEBUG: generate_multi_plot - Error processing data: {e}")
                return None
            
            # Create the plot
            try:
                fig = go.Figure()
                
                # Get plot metadata
                plot_meta = get_variable_metadata(plot_var)
                plot_title = f"{get_variable_display_name(plot_var)} - Grid {plot_lat_idx},{plot_lon_idx}"
                
                # Add main trace
                if plot_frequency == 'daily':
                    x_values = df_freq["Date"]
                    y_values = df_freq[plot_var]
                    x_label = "Date"
                else:
                    x_values = df_freq["Year"]
                    y_values = df_freq[plot_var]
                    x_label = "Year"
                
                # Add main trace
                fig.add_trace(go.Scatter(
                    x=x_values, 
                    y=y_values,
                    mode='lines+markers',
                    name=f"{get_variable_display_name(plot_var)} ({plot_model}, {plot_frequency}, {plot_ssp})",
                    line=dict(color=plot_meta['color'], width=2),
                    marker=dict(size=4)
                ))
                
                # Add additional traces if any
                for trace_idx, trace_config in enumerate(additional_traces):
                    try:
                        trace_var = trace_config.get('variable')
                        trace_model = trace_config.get('model')
                        trace_frequency = trace_config.get('frequency', 'annual')
                        trace_ssp = trace_config.get('ssp')
                        trace_lat_idx = trace_config.get('lat_idx')
                        trace_lon_idx = trace_config.get('lon_idx')
                        
                        if not all([trace_var, trace_model, trace_ssp, trace_lat_idx is not None, trace_lon_idx is not None]):
                            logging.debug(f"DEBUG: generate_multi_plot - Incomplete trace configuration for trace {trace_idx}")
                            continue
                        
                        # Load trace data
                        if trace_var.startswith('obs_'):
                            trace_ds = load_observed_dataset(trace_var)
                            if trace_ds is None:
                                logging.debug(f"DEBUG: generate_multi_plot - Could not load observed dataset for trace {trace_var}")
                                continue
                            trace_file_path = None
                        else:
                            trace_file_key = (trace_var, trace_model, trace_ssp)
                            if trace_file_key not in filepaths_map:
                                logging.debug(f"DEBUG: generate_multi_plot - File not found for trace key: {trace_file_key}")
                                continue
                            trace_file_path = filepaths_map[trace_file_key]
                            try:
                                trace_ds = xr.open_dataset(trace_file_path, engine="netcdf4", decode_times=False)
                            except Exception as e:
                                logging.debug(f"DEBUG: generate_multi_plot - Error loading trace dataset: {e}")
                                continue
                    
                        # Extract trace data
                        if trace_var.startswith('obs_'):
                            trace_df = extract_timeseries(trace_ds, trace_lat_idx, trace_lon_idx, trace_var)
                        else:
                            trace_df = extract_timeseries(trace_ds, trace_var, trace_lat_idx, trace_lon_idx)
                        
                        if trace_df is None or trace_df.empty:
                            logging.debug(f"DEBUG: generate_multi_plot - No data found for trace {trace_idx}")
                            continue
                        
                        # Process trace data based on frequency
                        if trace_frequency == 'daily':
                            trace_df_freq = trace_df.copy()
                            # Add Year column for consistency
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        elif trace_frequency == 'monthly':
                            # For precipitation, sum to get monthly totals; for other variables, average
                            if trace_var in ["pr", "obs_pr"]:
                                trace_df_freq = trace_df.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                                logging.debug(f"DEBUG: Multi-plot trace - Created monthly total precipitation data with {len(trace_df_freq)} rows")
                            else:
                                trace_df_freq = trace_df.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                                logging.debug(f"DEBUG: Multi-plot trace - Created monthly average data with {len(trace_df_freq)} rows")
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        elif trace_frequency == 'annual':
                            # For precipitation, sum to get annual totals; for other variables, average
                            if trace_var in ["pr", "obs_pr"]:
                                trace_df_freq = trace_df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                                logging.debug(f"DEBUG: Multi-plot trace - Created annual total precipitation data with {len(trace_df_freq)} rows")
                            else:
                                trace_df_freq = trace_df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                logging.debug(f"DEBUG: Multi-plot trace - Created annual average data with {len(trace_df_freq)} rows")
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        else:
                            trace_df_freq = trace_df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                            logging.debug(f"DEBUG: Multi-plot trace - Defaulted to annual averages with {len(trace_df_freq)} rows")
                        
                        # Handle trace unit conversion for all climate variables
                        if trace_var in ("obs_tmin", "obs_tmax"):
                            # Observed temperature conversion - Livneh data is always in Celsius
                            logging.debug(f"DEBUG: Multi-plot trace - Converting observed temperature {trace_var} from Celsius to Fahrenheit")
                            trace_df_freq[trace_var] = (trace_df_freq[trace_var] * 9/5) + 32
                        elif trace_var == "obs_pr":
                            # Observed precipitation conversion - Livneh data is in mm/day
                            logging.debug(f"DEBUG: Multi-plot trace - Converting observed precipitation from mm/day to inches")
                            trace_df_freq[trace_var] = trace_df_freq[trace_var] / 25.4
                        elif not trace_var.startswith('obs_'):
                            # Projected data conversion for all climate variables
                            if trace_var in ["tasmin", "tasmax"]:
                                # Projected temperature conversion - typically in Kelvin
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] - 273.15) * 9/5 + 32
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted projected temperature {trace_var} from Kelvin to Fahrenheit")
                                elif original_units in ['c', 'celsius', 'Â°c']:
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] * 9/5) + 32
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted projected temperature {trace_var} from Celsius to Fahrenheit")
                                else:
                                    # Default: assume Kelvin for projected temperature
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] - 273.15) * 9/5 + 32
                                    logging.debug(f"DEBUG: Multi-plot trace - No units specified - assuming projected temperature {trace_var} is in Kelvin, converting to Fahrenheit")
                            elif trace_var == "pr":
                                # Projected precipitation conversion - typically in kg m-2 s-1
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 86400 / 25.4
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted projected precipitation from kg m-2 s-1 to inches")
                                elif original_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] / 25.4
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted projected precipitation from mm/day to inches")
                                else:
                                    # Default: assume kg m-2 s-1 for projected precipitation
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 86400 / 25.4
                                    logging.debug(f"DEBUG: Multi-plot trace - No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                            elif trace_var in ["hursmax", "hursmin"]:
                                # Relative humidity conversion - typically in % but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['%', 'percent', 'pct']:
                                    # Already in percentage, no conversion needed
                                    logging.debug(f"DEBUG: Multi-plot trace - {trace_var} already in percentage - no conversion needed")
                                elif original_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                                    # Convert from fraction to percentage
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 100
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted {trace_var} from fraction to percentage")
                                else:
                                    # Default: assume percentage
                                    logging.debug(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in percentage")
                            elif trace_var == "huss":
                                # Specific humidity conversion - typically in kg/kg but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                                    # Already in kg/kg, no conversion needed
                                    logging.debug(f"DEBUG: Multi-plot trace - {trace_var} already in kg/kg - no conversion needed")
                                elif original_units in ['g/kg', 'g kg-1', 'g per kg']:
                                    # Convert from g/kg to kg/kg
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] / 1000
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted {trace_var} from g/kg to kg/kg")
                                else:
                                    # Default: assume kg/kg
                                    logging.debug(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in kg/kg")
                            elif trace_var == "rsds":
                                # Radiation - typically already in W/mÂ² (no conversion needed)
                                # NOTE: Climate model radiation data is standardized to W/mÂ² (watts per square meter)
                                logging.debug(f"DEBUG: Multi-plot trace - {trace_var} assumed to be in W/mÂ² (standard for climate models) - no conversion needed")
                            elif trace_var == "wspeed":
                                # Wind speed conversion - typically in m/s but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['m/s', 'm s-1', 'm per s', 'meter/s', 'meter per second']:
                                    # Already in m/s, no conversion needed
                                    logging.debug(f"DEBUG: Multi-plot trace - {trace_var} already in m/s - no conversion needed")
                                elif original_units in ['km/h', 'km h-1', 'km per h', 'kilometer/h', 'kilometer per hour']:
                                    # Convert from km/h to m/s
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 1000 / 3600
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted {trace_var} from km/h to m/s")
                                elif original_units in ['knots', 'kt', 'kts']:
                                    # Convert from knots to m/s
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 0.514444
                                    logging.debug(f"DEBUG: Multi-plot trace - Converted {trace_var} from knots to m/s")
                                else:
                                    # Default: assume m/s
                                    logging.debug(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in m/s")
                        
                        # Add trace to plot
                            if trace_frequency == 'daily':
                                trace_x_values = trace_df_freq["Date"]
                            else:
                                trace_x_values = trace_df_freq["Year"]
                            trace_y_values = trace_df_freq[trace_var]
                        
                        trace_meta = get_variable_metadata(trace_var)
                        trace_name = f"{get_variable_display_name(trace_var)} ({trace_model}, {trace_frequency}, {trace_ssp}) - Grid {trace_lat_idx},{trace_lon_idx}"
                        
                        fig.add_trace(go.Scatter(
                            x=trace_x_values, 
                            y=trace_y_values,
                            mode='lines+markers',
                            name=trace_name,
                            line=dict(color=trace_meta['color'], width=2, dash='dash'),
                            marker=dict(size=3)
                        ))
                        
                        logging.debug(f"DEBUG: generate_multi_plot - Added trace {trace_idx}: {trace_name}")
                        
                    except Exception as e:
                        logging.debug(f"DEBUG: generate_multi_plot - Error processing trace {trace_idx}: {e}")
                        continue
                
                # Set up plot layout
                fig.update_layout(
                    title=plot_title,
                    xaxis_title=x_label,
                    yaxis_title=f"{get_variable_display_name(plot_var)} ({display_units})",
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        dtick=10,  # Show ticks every 10 years
                        tick0=1950,  # Start ticks at 1950
                        tickmode='linear'  # Linear tick spacing
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    showlegend=True,
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    ),
                    margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                )
                
                # Note: Download legend configuration removed from main display
                # Download legend positioning should be handled separately if needed
                
                logging.debug(f"DEBUG: generate_multi_plot - Successfully created plot {plot_id}")
                return fig
                
            except Exception as e:
                logging.debug(f"DEBUG: generate_multi_plot - Error creating plot: {e}")
                return None
                
        except Exception as e:
            logging.debug(f"DEBUG: generate_multi_plot - Unexpected error: {e}")
            return None

    # Initialize startup sequence
    startup_trigger.set(1)
    
    # Variable Maps page functions - temporarily commented out
    # @output
    # @render.ui
    # def variable_map_controls():
    #     """Render controls for the variable maps page"""
    #     filepaths_map = reactive_netcdf_filepaths.get()
    #     if not filepaths_map:
    #         return ui.div(
    #             ui.p("Loading data...", class_="text-muted"),
    #             class_="text-center"
    #         )
    #     
    #     # Get available variables, models, and scenarios
    #     variables = sorted(list(set([var for var, _, _ in filepaths_map.keys()])))
    #     models = sorted(list(set([model for _, model, _ in filepaths_map.keys()])))
    #     scenarios = sorted(list(set([ssp for _, _, ssp in filepaths_map.keys()])))
    #     
    #     return ui.div(
    #         ui.h5("Select Climate Variable"),
    #         ui.input_select(
    #             "variable_map_var",
    #             "Variable",
    #             choices={var: get_variable_display_name(var) for var in variables},
    #             selected=variables[0] if variables else None
    #         ),
    #         ui.h5("Select Climate Model"),
    #         ui.input_select(
    #             "variable_map_model", 
    #             "Model",
    #             choices={model: model for model in models},
    #             selected=models[0] if models else None
    #         ),
    #         ui.h5("Select SSP Scenario"),
    #         ui.input_select(
    #             "variable_map_ssp",
    #             "Scenario", 
    #             choices={ssp: ssp for ssp in scenarios},
    #             selected=scenarios[0] if scenarios else None
    #         ),
    #         ui.h5("Select Time Period"),
    #         ui.input_slider(
    #             "variable_map_year",
    #             "Year",
    #             min=1950,
    #             max=2100,
    #             value=2050,
    #             step=10
    #         ),
    #         class_="p-3"
    #     )
    # 
    # @output
    # @render_widget
    # def variable_spatial_map():
    #     """Render spatial map for climate variables"""
    #     try:
    #         # Get selections
    #         variable = input.variable_map_var()
    #         model = input.variable_map_model()
    #         ssp = input.variable_map_ssp()
    #         year = input.variable_map_year()
    #         
    #         if not all([variable, model, ssp]):
    #             return create_empty_map("Please select variable, model, and scenario")
    #         
    #         # Get file path
    #         filepaths_map = reactive_netcdf_filepaths.get()
    #         file_key = (variable, model, ssp)
    #         
    #         if file_key not in filepaths_map:
    #             return create_empty_map(f"No data available for {variable}, {model}, {ssp}")
    #         
    #         # Load dataset
    #         cached_datasets = _cached_datasets.get()
    #         ds = cached_datasets.get(file_key)
    #         
    #         if ds is None:
    #             try:
    #                 file_path = filepaths_map[file_key]
    #                 ds = xr.open_dataset(file_path)
    #                 # Cache the dataset
    #                 cached_datasets[file_key] = ds
    #                 _cached_datasets.set(cached_datasets)
    #             except Exception as e:
    #                 return create_empty_map(f"Error loading data: {str(e)}")
    #         
    #         # Extract data for the selected year
    #         try:
    #             if 'time' in ds.coords:
    #                 # Find closest time to selected year
    #                 year_data = ds.sel(time=str(year), method='nearest')
    #             else:
    #                 # If no time dimension, use the entire dataset
    #                 year_data = ds
    #             
    #             # Get the data variable
    #             var_name = variable
    #             if var_name not in year_data.data_vars:
    #                 var_name = list(year_data.data_vars)[0]
    #             
    #             data = year_data[var_name]
    #             
    #             # Convert to numpy arrays
    #             if hasattr(data, 'values'):
    #                 values = data.values
    #             else:
    #                 values = data
    #             
    #             # Get coordinates
    #             if 'lat' in data.coords and 'lon' in data.coords:
    #                 lats = data.lat.values
    #                 lons = data.lon.values
    #             elif 'latitude' in data.coords and 'longitude' in data.coords:
    #                 lats = data.latitude.values
    #                 lons = data.longitude.values
    #             else:
    #                 return create_empty_map("Unable to extract coordinates")
    #             
    #             # Filter to Santa Barbara region
    #             sb_mask = (
    #                 (lats >= SB_LAT_MIN) & (lats <= SB_LAT_MAX) &
    #                 (lons >= SB_LON_MIN) & (lons <= SB_LON_MAX)
    #             )
    #             
    #             if not np.any(sb_mask):
    #                 return create_empty_map("No data available for Santa Barbara region")
    #             
    #             # Create the map
    #             fig = go.Figure()
    #             
    #             # Add heatmap
    #             fig.add_trace(go.Heatmap(
    #                 z=values,
    #                 x=lons,
    #                 y=lats,
    #                 colorscale='Viridis',
    #                 showscale=True,
    #                 colorbar=dict(
    #                     title=get_variable_metadata(variable)['units']
    #                 )
    #             ))
    #             
    #             # Update layout
    #             fig.update_layout(
    #                 title=f"{get_variable_display_name(variable)} - {model} ({ssp}) - {year}",
    #                 mapbox=dict(
    #                     style="open-street-map",
    #                     center=dict(lat=(SB_LAT_MIN + SB_LAT_MAX) / 2, lon=(SB_LON_MIN + SB_LON_MAX) / 2),
    #                     zoom=8
    #                 ),
    #                 height=600,
    #                 margin=dict(l=0, r=0, t=50, b=0)
    #             )
    #             
    #             return fig
    #             
    #         except Exception as e:
    #             return create_empty_map(f"Error processing data: {str(e)}")
    #             
    #     except Exception as e:
    #         return create_empty_map(f"Error creating map: {str(e)}")
    
    # ============================================================================
    # VARIABLE MAP PAGE - SPATIAL CLIMATE CHANGE VISUALIZATION
    # ============================================================================
    
    def _calculate_spatial_changes(variable, model, scenario, start_year, end_year, change_type, common_gridboxes, filepaths_map, cached_datasets):
        """
        VECTORIZED: Calculate spatial change values for all gridboxes between two years.
        Always uses annual frequency. Uses vectorized operations for 100x+ speedup.
        
        Returns: Dict mapping gridbox_id (e.g., "25,10") to {abs_change, pct_change, start_val, end_val, error}
        """
        logging.debug(f"DEBUG: _calculate_spatial_changes called - {variable}, {model}, {scenario}, {start_year}-{end_year}")
        
        # Check cache first
        cache_key = (variable, model, scenario, start_year, end_year, change_type)
        if cache_key in _SPATIAL_CHANGE_CACHE:
            logging.debug(f"DEBUG: âš¡ SPATIAL CHANGE CACHE HIT!")
            return _SPATIAL_CHANGE_CACHE[cache_key]
        
        results = {}
        is_observed = (model == "Observed")
        total_gridboxes = len(common_gridboxes)
        logging.debug(f"DEBUG: VECTORIZED processing of {total_gridboxes} gridboxes for spatial map")
        
        import time
        start_time = time.time()
        
        try:
            # STEP 1: Load dataset ONCE for all gridboxes
            if is_observed:
                ds = load_observed_dataset(variable)
                if ds is None:
                    logging.debug(f"DEBUG: Failed to load observational dataset for {variable}")
                    return {}
                extract_var = [v for v in ds.data_vars.keys() if v != 'spatial_ref'][0]
            else:
                file_key = (variable, model, scenario)
                if file_key not in filepaths_map:
                    logging.debug(f"DEBUG: File not found for {variable}, {model}, {scenario}")
                    return {}
                
                filepath = filepaths_map[file_key]
                ds = cached_datasets.get(filepath)
                if ds is None:
                    logging.debug(f"DEBUG: Dataset not in cache, loading...")
                    ds = xr.open_dataset(filepath)
                    cached_datasets[filepath] = ds
                extract_var = variable
            
            logging.debug(f"DEBUG: Dataset loaded in {time.time() - start_time:.2f}s")
            
            # STEP 2: Batch map observational gridboxes if needed (VECTORIZED)
            gridbox_indices = []  # List of (lat_idx, lon_idx, grid_id)
            
            if is_observed:
                logging.debug(f"DEBUG: Batch mapping {total_gridboxes} projection gridboxes to observational grid...")
                map_start = time.time()
                
                # Get obs grid info once
                lat2d, lon2d, _, _ = _extract_spatial_coords(ds)
                common_obs_gridboxes = _find_common_observational_gridboxes()
                
                if lat2d is not None and lon2d is not None and common_obs_gridboxes:
                    # Collect all projection coordinates
                    proj_coords_list = []
                    grid_ids_list = []
                    
                    for lat_idx, lon_idx in common_gridboxes:
                        grid_id = f"{lat_idx},{lon_idx}"
                        from_coords = _get_coordinates_from_grid_id(grid_id, filepaths_map)
                        if from_coords:
                            proj_coords_list.append(from_coords)
                            grid_ids_list.append(grid_id)
                    
                    # VECTORIZED mapping: process all coordinates at once
                    proj_coords = np.array(proj_coords_list)
                    obs_coords = np.array([(lat, lon) for lat, lon in common_obs_gridboxes if not (np.isnan(lat) or np.isnan(lon))])
                    
                    if len(proj_coords) > 0 and len(obs_coords) > 0:
                        # Compute all pairwise distances at once using broadcasting
                        proj_lats = proj_coords[:, 0:1]  # Shape (n, 1)
                        proj_lons = proj_coords[:, 1:2]
                        obs_lats = obs_coords[:, 0]  # Shape (m,)
                        obs_lons = obs_coords[:, 1]
                        
                        # Euclidean distance
                        distances = np.sqrt((proj_lats - obs_lats)**2 + (proj_lons - obs_lons)**2)
                        
                        # Find nearest obs point for each projection point
                        nearest_obs_indices = np.argmin(distances, axis=1)
                        
                        # Map each projection gridbox to its nearest obs gridbox
                        for i, grid_id in enumerate(grid_ids_list):
                            nearest_obs_lat, nearest_obs_lon = obs_coords[nearest_obs_indices[i]]
                            
                            # Find grid indices for this obs point
                            lat_distances = np.abs(lat2d - nearest_obs_lat)
                            lon_distances = np.abs(lon2d - nearest_obs_lon)
                            min_lat_idx, min_lon_idx = np.unravel_index(
                                np.argmin(lat_distances + lon_distances), lat2d.shape
                            )
                            
                            if 0 <= min_lat_idx < lat2d.shape[0] and 0 <= min_lon_idx < lat2d.shape[1]:
                                gridbox_indices.append((min_lat_idx, min_lon_idx, grid_id))
                
                logging.debug(f"DEBUG: Batch mapping completed in {time.time() - map_start:.2f}s")
            else:
                # Projection data - use indices directly
                for lat_idx, lon_idx in common_gridboxes:
                    grid_id = f"{lat_idx},{lon_idx}"
                    gridbox_indices.append((lat_idx, lon_idx, grid_id))
            
            logging.debug(f"DEBUG: Mapped {len(gridbox_indices)} gridboxes, extracting data vectorized...")
            
            # STEP 3: VECTORIZED extraction - get ALL gridboxes' time series at once
            extract_start = time.time()
            
            lat_indices = np.array([idx[0] for idx in gridbox_indices])
            lon_indices = np.array([idx[1] for idx in gridbox_indices])
            
            # Get time coordinate
            if 'time' in ds.coords:
                time_values = pd.to_datetime(ds['time'].values)
            elif 'Time' in ds.coords:
                time_values = pd.to_datetime(ds['Time'].values)
            elif 'band' in ds.coords:
                # Observational data uses 'band' as time dimension
                band_count = len(ds['band'])
                time_values = pd.date_range('1950-01-01', periods=band_count, freq='D')
            else:
                raise Exception(f"No time coordinate found. Available coords: {list(ds.coords.keys())}")
            
            # VECTORIZED extraction: shape (time, num_gridboxes)
            data_array = ds[extract_var].values[:, lat_indices, lon_indices]
            
            logging.debug(f"DEBUG: Extracted {data_array.shape[1]} gridboxes' time series in {time.time() - extract_start:.2f}s")
            
            # STEP 4: VECTORIZED unit conversions and annual aggregation
            process_start = time.time()
            
            # Apply unit conversions to entire array
            if variable in ['tasmin', 'tasmax']:
                # Convert Kelvin to Fahrenheit for projection data
                data_array = (data_array - 273.15) * 9/5 + 32
            elif variable in ['obs_tmin', 'obs_tmax']:
                # Convert Celsius to Fahrenheit for observational data
                data_array = (data_array * 9/5) + 32
            elif variable in ['pr', 'obs_pr']:
                # Convert kg m-2 s-1 to inches per day (projection)
                # or mm to inches (observational)
                if variable == 'obs_pr':
                    data_array = data_array / 25.4  # mm to inches
                else:
                    data_array = data_array * 86400 / 25.4  # kg m-2 s-1 to inches/day
            
            # Create DataFrame with all gridboxes
            years = pd.to_datetime(time_values).year
            
            # STEP 5: VECTORIZED annual aggregation and change calculation
            for i, (lat_idx, lon_idx, grid_id) in enumerate(gridbox_indices):
                try:
                    gridbox_series = data_array[:, i]
                    
                    # Check for valid data
                    if np.all(np.isnan(gridbox_series)):
                        results[grid_id] = {'error': 'No valid data'}
                        continue
                    
                    # Create DataFrame for this gridbox
                    df = pd.DataFrame({
                        'Year': years,
                        'value': gridbox_series
                    })
                    
                    # Aggregate to annual values
                    if variable in ['pr', 'obs_pr']:
                        # Precipitation: annual sum
                        annual_df = df.groupby('Year')['value'].sum()
                    else:
                        # Temperature, humidity, wind: annual mean
                        annual_df = df.groupby('Year')['value'].mean()
                    
                    # Get values for start and end years
                    if start_year not in annual_df.index or end_year not in annual_df.index:
                        results[grid_id] = {'error': f'Data missing for {start_year} or {end_year}'}
                        continue
                    
                    start_val = float(annual_df[start_year])
                    end_val = float(annual_df[end_year])
                    
                    # Check for NaN values
                    if np.isnan(start_val) or np.isnan(end_val):
                        results[grid_id] = {'error': 'NaN values in data'}
                        continue
                    
                    abs_change = end_val - start_val
                    
                    # Calculate percentage change (handle division by zero)
                    if abs(start_val) < 1e-10:
                        pct_change = 0.0 if abs(abs_change) < 1e-10 else float('inf')
                    else:
                        pct_change = (abs_change / start_val) * 100
                    
                    results[grid_id] = {
                        'abs_change': abs_change,
                        'pct_change': pct_change,
                        'start_val': start_val,
                        'end_val': end_val,
                        'error': None
                    }
                    
                except Exception as e:
                    results[grid_id] = {'error': str(e)}
            
            logging.debug(f"DEBUG: Processed all gridboxes in {time.time() - process_start:.2f}s")
            
        except Exception as e:
            logging.debug(f"DEBUG: Error in vectorized spatial change calculation: {e}")
            import traceback
            traceback.print_exc()
            return {}
        
        # Cache the results
        _SPATIAL_CHANGE_CACHE[cache_key] = results
        total_time = time.time() - start_time
        success_count = sum(1 for d in results.values() if not d.get('error'))
        logging.debug(f"DEBUG: âœ… VECTORIZED processing complete! {success_count}/{total_gridboxes} successful in {total_time:.2f}s")
        
        return results
    
    @output
    @render.ui
    def variable_map_controls():
        """Progressive dropdown system for Variable Map page (3 steps)"""
        logging.debug("DEBUG: variable_map_controls called")
        
        # Get step completion status
        step1 = var_map_step_1_completed.get()
        step2 = var_map_step_2_completed.get()
        step3 = var_map_step_3_completed.get()
        
        logging.debug(f"DEBUG: VarMap steps: {step1}, {step2}, {step3}")
        
        # Get data
        filepaths_map = reactive_netcdf_filepaths.get()
        if not filepaths_map:
            return ui.div(ui.p("Loading data...", style="text-align: center;"))
        
        controls = []
        
        # Step 1: Data Type + Model
        controls.append(ui.tags.h4("Step 1: Select Data Type & Model", class_="mt-3"))
        
        # Data type dropdown
        data_type_choices = {
            "": "Select data type...",
            "Projection": "Climate Model Projections",
            "Observed": "Observational Data (Historical)"
        }
        controls.append(ui.input_select(
            "var_map_data_type",
            "Data Type:",
            choices=data_type_choices,
            selected=var_map_data_type.get() or ""
        ))
        
        # Model dropdown (only show if data type selected)
        current_data_type = var_map_data_type.get()
        if current_data_type:
            if current_data_type == "Observed":
                # Auto-select Observed (no button needed - auto-advances)
                controls.append(ui.div(
                    ui.p("âœ“ Model: Observational Data", style="margin-top: 10px; font-weight: bold; color: green;")
                ))
            else:
                # Show model selector for projections
                model_choices = {"": "Select a model..."}
                models = sorted(set(key[1] for key in filepaths_map.keys() if key[1] != 'Observed'))
                for m in models:
                    model_choices[m] = m
                
                controls.append(ui.input_select(
                    "var_map_model",
                    "Climate Model:",
                    choices=model_choices,
                    selected=var_map_model.get() or ""
                ))
        
        # Step 2: Variable (only show if step 1 complete)
        if step1:
            controls.append(ui.tags.hr())
            controls.append(ui.tags.h4("Step 2: Select Climate Variable", class_="mt-3"))
            
            # Get variables available across all common gridboxes
            grid_feature_ids = reactive_grid_feature_ids.get() or []
            
            if current_data_type == "Observed":
                # Filter observational variables
                all_obs_vars = ['obs_tmin', 'obs_tmax', 'obs_pr']
                # Use loop availability check with properly formatted grid IDs
                common_gridboxes_with_weights = [(gid, 1.0) for gid in grid_feature_ids]
                available_vars = _get_variables_available_in_loop(
                    common_gridboxes_with_weights,
                    filepaths_map,
                    _cached_datasets.get()
                )
                var_choices_dict = {"": "Select a variable..."}
                for v in all_obs_vars:
                    if v in available_vars:
                        display_name = get_variable_metadata(v).get("display_name", v)
                        var_choices_dict[v] = display_name
            else:
                # Projection variables for selected model
                current_model = var_map_model.get()
                if current_model:
                    var_choices_dict = {"": "Select a variable..."}
                    proj_vars = set(key[0] for key in filepaths_map.keys() 
                                  if key[1] == current_model and key[0] not in ['obs_tmin', 'obs_tmax', 'obs_pr'])
                    for v in sorted(proj_vars):
                        display_name = get_variable_metadata(v).get("display_name", v)
                        var_choices_dict[v] = display_name
                else:
                    var_choices_dict = {"": "Select model first..."}
            
            controls.append(ui.input_select(
                "var_map_variable",
                "Climate Variable:",
                choices=var_choices_dict,
                selected=var_map_variable.get() or ""
            ))
        
        # Step 3: Scenario (only show if step 2 complete)
        if step2:
            controls.append(ui.tags.hr())
            controls.append(ui.tags.h4("Step 3: Select Scenario", class_="mt-3"))
            
            if current_data_type == "Observed":
                # Only historical available for observed
                scenario_choices = {
                    "historical": "Historical (1950-2013)"
                }
                controls.append(ui.div(
                    ui.p("Scenario: Historical", style="margin-top: 10px; font-weight: bold;"),
                    ui.input_action_button("var_map_confirm_step3_obs", "Generate Map", class_="btn-success mt-2")
                ))
            else:
                # Filter scenarios based on actual file availability for selected model+variable
                current_model = var_map_model.get()
                current_variable = var_map_variable.get()
                
                # All possible scenarios with display names
                all_scenarios = {
                    "historical": "Historical (1950-2014)",
                    "ssp245": "SSP2-4.5 (Medium emissions)",
                    "ssp370": "SSP3-7.0 (High emissions)",
                    "ssp585": "SSP5-8.5 (Very high emissions)"
                }
                
                # Find available scenarios for this model+variable combination
                available_scenarios = set()
                if current_model and current_variable:
                    for key in filepaths_map.keys():
                        var, model, scenario = key
                        if var == current_variable and model == current_model:
                            available_scenarios.add(scenario)
                
                # Build filtered scenario choices
                scenario_choices = {"": "Select a scenario..."}
                for scenario_key, scenario_label in all_scenarios.items():
                    if scenario_key in available_scenarios:
                        scenario_choices[scenario_key] = scenario_label
                
                # If no scenarios available, show message
                if len(scenario_choices) == 1:  # Only the default "Select..." option
                    scenario_choices = {"": "No scenarios available for this model+variable"}
                
                controls.append(ui.input_select(
                    "var_map_scenario",
                    "Scenario:",
                    choices=scenario_choices,
                    selected=var_map_scenario.get() if var_map_scenario.get() in scenario_choices else ""
                ))
        
        # Reset button (always show if any step is completed)
        if step1 or step2 or step3:
            controls.append(ui.tags.hr())
            controls.append(ui.input_action_button(
                "var_map_reset",
                "ðŸ”„ Reset Selection",
                class_="btn-warning mt-2"
            ))
        
        return ui.div(*controls, class_="variable-map-controls p-3 border rounded")
    
    @output
    @render.ui
    def variable_map_time_controls():
        """Time range sliders and change type toggle"""
        # Only depend on step 3 completion and scenario for re-rendering
        if not var_map_step_3_completed.get():
            return None
        
        # Determine year range based on scenario
        scenario = var_map_scenario.get()
        if scenario == "historical":
            min_year, max_year = 1950, 2014
        else:
            min_year, max_year = 2015, 2100
        
        # Isolate reads of year values to prevent re-rendering when they change
        with reactive.isolate():
            start_year = var_map_start_year.get()
            end_year = var_map_end_year.get()
            change_type = var_map_change_type.get() or "absolute"
        
        # Ensure years are in valid range
        if start_year < min_year:
            start_year = min_year
        if end_year > max_year:
            end_year = max_year
        if end_year <= start_year:
            end_year = start_year + 1
        
        with reactive.isolate():
            variable_name = get_variable_metadata(var_map_variable.get()).get("display_name", "variable")
        
        return ui.div(
            ui.tags.hr(),
            ui.tags.h4("Time Range Selection", class_="mt-3"),
            ui.p(f"Map shows change in annual average {variable_name} from start to end year", 
                 class_="text-muted", style="font-size: 14px;"),
            ui.row(
                ui.column(6,
                    ui.input_slider(
                        "var_map_start_year_slider",
                        "Start Year:",
                        min=min_year,
                        max=max_year-1,
                        value=start_year,
                        step=1,
                        sep=""
                    )
                ),
                ui.column(6,
                    ui.input_slider(
                        "var_map_end_year_slider",
                        "End Year:",
                        min=start_year+1,  # Dynamic: always after start year
                        max=max_year,
                        value=end_year,
                        step=1,
                        sep=""
                    )
                )
            ),
            ui.div(
                ui.p(f"Selected Range: {start_year} - {end_year}", 
                     style="text-align: center; font-weight: bold; font-size: 16px; margin-top: 10px;")
            ),
            ui.tags.hr(),
            ui.tags.h5("Display Type:"),
            ui.input_radio_buttons(
                "var_map_change_type_radio",
                None,
                choices={
                    "absolute": "Absolute Change (e.g., +2.5Â°F)",
                    "percentage": "Percentage Change (e.g., +15%)"
                },
                selected=change_type,
                inline=True
            ),
            class_="time-controls p-3 border rounded mt-3"
        )
    
    @output
    @render.ui
    def variable_spatial_map():
        """Render the spatial map showing climate changes"""
        if not var_map_step_3_completed.get():
            return ui.div()
        
        logging.debug("DEBUG: variable_spatial_map called")
        
        # Get all parameters
        variable = var_map_variable.get()
        model = var_map_model.get() if var_map_data_type.get() == "Projection" else "Observed"
        scenario = var_map_scenario.get()
        start_year = var_map_start_year.get()
        end_year = var_map_end_year.get()
        change_type = var_map_change_type.get()
        
        if not all([variable, model, scenario, start_year, end_year]):
            logging.debug("DEBUG: Missing required parameters for map")
            return ui.div("Please complete all selections to generate the map.", class_="alert alert-info")
        
        logging.debug(f"DEBUG: Generating spatial map for {variable}, {model}, {scenario}, {start_year}-{end_year}")
        
        # Get common gridboxes
        grid_feature_ids = reactive_grid_feature_ids.get() or []
        common_gridboxes = [tuple(map(int, gid.split(','))) for gid in grid_feature_ids]
        filepaths_map = reactive_netcdf_filepaths.get()
        cached_ds = _cached_datasets.get()
        
        # Calculate spatial changes
        results = _calculate_spatial_changes(
            variable, model, scenario, start_year, end_year, 
            change_type, common_gridboxes, filepaths_map, cached_ds
        )
        
        # Prepare data for plotting
        lats, lons, changes, hover_texts = [], [], [], []
        
        # Debug: Check results
        total_results = len(results)
        error_count = sum(1 for d in results.values() if d.get('error'))
        success_count = total_results - error_count
        logging.debug(f"DEBUG: Results - Total: {total_results}, Success: {success_count}, Errors: {error_count}")
        
        # Show sample errors if any
        if error_count > 0:
            sample_errors = [(gid, d['error']) for gid, d in list(results.items())[:5] if d.get('error')]
            logging.debug(f"DEBUG: Sample errors: {sample_errors}")
        
        # Determine unit based on variable (outside loop for consistency)
        if variable in ['tasmin', 'tasmax', 'obs_tmin', 'obs_tmax']:
            unit = "Â°F"
        elif variable in ['pr', 'obs_pr']:
            unit = "inches"
        elif variable == 'wspeed':
            unit = "m/s"
        elif variable in ['huss', 'hursmin', 'hursmax']:
            unit = "%" if 'hurs' in variable else "kg/kg"
        elif variable == 'rsds':
            unit = "W/mÂ²"
        else:
            unit = ""
        
        for grid_id, data in results.items():
            if data.get('error'):
                continue
            
            lat_idx, lon_idx = map(int, grid_id.split(','))
            
            # Get coordinates
            coords = _get_coordinates_from_grid_id(grid_id, filepaths_map)
            if not coords:
                continue
            
            lat, lon = coords
            lats.append(lat)
            lons.append(lon)
            
            # Get change value
            if change_type == "percentage":
                change_val = data['pct_change']
                display_unit = "%"
            else:
                change_val = data['abs_change']
                display_unit = unit
            
            changes.append(change_val)
            
            # Hover text
            hover_text = (
                f"Gridbox: {grid_id}<br>"
                f"Start ({start_year}): {data['start_val']:.2f} {unit}<br>"
                f"End ({end_year}): {data['end_val']:.2f} {unit}<br>"
                f"Change: {change_val:.2f} {display_unit}"
            )
            hover_texts.append(hover_text)
        
        if not changes:
            logging.debug("DEBUG: No valid data for spatial map")
            return ui.div(
                ui.h5("No valid data available", class_="text-warning"),
                ui.p("No gridboxes have sufficient data for the selected year range."),
                class_="alert alert-warning"
            )
        
        # Create plotly figure
        import plotly.graph_objects as go
        
        # Determine color scale range (centered at zero)
        max_abs = max(abs(min(changes)), abs(max(changes)))
        
        # Determine colorbar unit based on change type
        colorbar_unit = "%" if change_type == "percentage" else unit
        
        # CONDITIONAL COLOR SCALE: Reverse for precipitation (blue=increase is positive)
        # For precipitation: more rain is good (blue), less rain is bad (red)
        # For other variables: increase is warm/red, decrease is cool/blue
        is_precipitation = variable in ['pr', 'obs_pr']
        if is_precipitation:
            colorscale = 'RdBu'  # Blue=increase (positive), Red=decrease (negative) for precipitation
            logging.debug(f"DEBUG: Using reversed color scale for precipitation (blue=increase)")
        else:
            colorscale = 'RdBu_r'  # Blue=decrease (negative), Red=increase (positive) for other variables
        
        fig = go.Figure(data=go.Scattergeo(
            lon=lons,
            lat=lats,
            mode='markers',
            marker=dict(
                size=15,
                color=changes,
                colorscale=colorscale,
                cmin=-max_abs,
                cmax=max_abs,
                colorbar=dict(
                    title=f"Change ({colorbar_unit})",
                    thickness=20,
                    len=0.7
                ),
                line=dict(width=0)
            ),
            text=hover_texts,
            hoverinfo='text'
        ))
        
        # Update layout
        variable_name = get_variable_metadata(variable).get("display_name", variable)
        title = f"{variable_name} Change: {start_year} to {end_year}<br>{model} ({scenario})"
        
        # Use proper geographic projection with topography - proper north-up orientation
        fig.update_geos(
            projection_type="natural earth",  # Natural Earth provides good regional view with proper orientation
            center=dict(lat=34.2, lon=-119.8),  # Center on Santa Barbara
            projection_scale=25,  # Zoom level for regional focus
            showland=True,
            landcolor='rgb(243, 243, 243)',
            coastlinecolor='rgb(100, 100, 100)',
            coastlinewidth=1.5,
            showlakes=True,
            lakecolor='rgb(200, 230, 255)',
            showocean=True,
            oceancolor='rgb(230, 245, 255)',
            showcountries=False,
            showsubunits=True,  # Show county/state boundaries
            subunitcolor='rgb(150, 150, 150)',
            subunitwidth=0.8,
            showrivers=False,
            bgcolor='white',
            resolution=50,
            visible=True
        )
        
        fig.update_layout(
            title=title,
            height=700,
            margin=dict(l=20, r=20, t=60, b=20)
        )
        
        # Return as HTML for Shiny rendering
        return ui.HTML(fig.to_html(include_plotlyjs="cdn", config={"responsive": True}))
    
    # Event handlers for Variable Map dropdowns
    @reactive.Effect
    @reactive.event(input.var_map_data_type)
    def _handle_var_map_data_type():
        """Handle data type selection"""
        selected = input.var_map_data_type()
        logging.debug(f"DEBUG: VarMap data type changed to: {selected}")
        
        if selected and selected != "":
            var_map_data_type.set(selected)
            
            # If observational, auto-set model to "Observed"
            if selected == "Observed":
                var_map_model.set("Observed")
                var_map_step_1_completed.set(True)
                logging.debug("DEBUG: VarMap Step 1 completed (Observed)")
        else:
            var_map_data_type.set(None)
            var_map_model.set(None)
            var_map_step_1_completed.set(False)
    
    @reactive.Effect
    @reactive.event(input.var_map_model)
    def _handle_var_map_model():
        """Handle model selection"""
        selected = input.var_map_model()
        logging.debug(f"DEBUG: VarMap model changed to: {selected}")
        
        if selected and selected != "":
            var_map_model.set(selected)
            var_map_step_1_completed.set(True)
            # Reset variable and scenario since different models have different available combinations
            var_map_variable.set(None)
            var_map_scenario.set(None)
            var_map_step_2_completed.set(False)
            var_map_step_3_completed.set(False)
            logging.debug("DEBUG: VarMap Step 1 completed, variable and scenario reset")
        else:
            var_map_model.set(None)
            var_map_step_1_completed.set(False)
    
    @reactive.Effect
    @reactive.event(input.var_map_variable)
    def _handle_var_map_variable():
        """Handle variable selection"""
        selected = input.var_map_variable()
        logging.debug(f"DEBUG: VarMap variable changed to: {selected}")
        
        if selected and selected != "":
            var_map_variable.set(selected)
            var_map_step_2_completed.set(True)
            # Reset scenario since different variables may have different available scenarios
            var_map_scenario.set(None)
            var_map_step_3_completed.set(False)
            logging.debug("DEBUG: VarMap Step 2 completed, scenario reset")
        else:
            var_map_variable.set(None)
            var_map_step_2_completed.set(False)
    
    @reactive.Effect
    @reactive.event(input.var_map_scenario)
    def _handle_var_map_scenario():
        """Handle scenario selection"""
        selected = input.var_map_scenario()
        logging.debug(f"DEBUG: VarMap scenario changed to: {selected}")
        
        if selected and selected != "":
            var_map_scenario.set(selected)
            
            # Automatically set appropriate year ranges based on scenario
            if selected == "historical":
                var_map_start_year.set(1950)
                var_map_end_year.set(2014)
                logging.debug("DEBUG: VarMap years set to historical range: 1950-2014")
            else:
                # Future scenarios (ssp245, ssp370, ssp585)
                var_map_start_year.set(2015)
                var_map_end_year.set(2100)
                logging.debug(f"DEBUG: VarMap years set to future range: 2015-2100 for {selected}")
            
            var_map_step_3_completed.set(True)
            logging.debug("DEBUG: VarMap Step 3 completed")
        else:
            var_map_scenario.set(None)
            var_map_step_3_completed.set(False)
    
    # Simple year slider handlers - update reactive values directly
    @reactive.Effect
    @reactive.event(input.var_map_start_year_slider)
    def _handle_var_map_start_year():
        """Handle start year slider"""
        year = input.var_map_start_year_slider()
        var_map_start_year.set(year)
    
    @reactive.Effect
    @reactive.event(input.var_map_end_year_slider)
    def _handle_var_map_end_year():
        """Handle end year slider"""
        year = input.var_map_end_year_slider()
        var_map_end_year.set(year)
    
    @reactive.Effect
    @reactive.event(input.var_map_change_type_radio)
    def _handle_var_map_change_type():
        """Handle change type radio buttons"""
        selected = input.var_map_change_type_radio()
        var_map_change_type.set(selected)
        logging.debug(f"DEBUG: VarMap change type: {selected}")
    
    @reactive.Effect
    @reactive.event(input.var_map_reset)
    def _handle_var_map_reset():
        """Handle reset button"""
        logging.debug("DEBUG: VarMap reset button clicked")
        
        # Reset all reactive values
        var_map_data_type.set(None)
        var_map_model.set(None)
        var_map_variable.set(None)
        var_map_scenario.set(None)
        var_map_start_year.set(1950)
        var_map_end_year.set(2014)
        var_map_change_type.set("absolute")
        var_map_step_1_completed.set(False)
        var_map_step_2_completed.set(False)
        var_map_step_3_completed.set(False)
        
        # Reset UI
        ui.update_select("var_map_data_type", selected="")
        ui.update_select("var_map_model", selected="")
        ui.update_select("var_map_variable", selected="")
        ui.update_select("var_map_scenario", selected="")
        
        logging.debug("DEBUG: VarMap reset complete")
    
    @reactive.Effect
    @reactive.event(input.var_map_confirm_step3_obs)
    def _handle_var_map_confirm_step3_obs():
        """Handle confirm button for observational data step 3"""
        logging.debug("DEBUG: VarMap Step 3 confirmed for Observed data")
        var_map_scenario.set("historical")
        
        # Set appropriate year range for observational data (1950-2013)
        var_map_start_year.set(1950)
        var_map_end_year.set(2013)
        logging.debug("DEBUG: VarMap years set to observational range: 1950-2013")
        
        var_map_step_3_completed.set(True)

# Configure Shiny for unlimited session duration
os.environ["SHINY_SESSION_TIMEOUT"] = "0"  # 0 = no timeout
os.environ["SHINY_WEBSOCKET_COMPRESSION"] = "true"  # Enable compression for efficiency
# Create the Shiny app instance
app = App(app_ui, server)

# This will replace the end of app.py with properly configured Uvicorn

if __name__ == "__main__":
    import socket
    import subprocess
    import os
    
    def aggressive_cleanup():
        """Kill any processes on port 8080"""
        try:
            result = subprocess.run(
                ['lsof', '-ti', ':8080'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    try:
                        subprocess.run(['kill', '-9', pid.strip()], capture_output=True, timeout=1)
                    except:
                        pass
        except:
            pass
    
    def get_available_port(preferred=8080):
        """Get an available port"""
        for port in range(preferred, preferred + 200):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    s.bind(('0.0.0.0', port))
                    return port
            except:
                continue
        return None
    
    try:
        logging.info("\n" + "=" * 70)
        logging.info("  Santa Barbara Climate Dashboard")
        logging.info("=" * 70 + "\n")
        
        # AGGRESSIVE CLEANUP
        aggressive_cleanup()
        
        # Find available port
        port = get_available_port(8080)
        
        if not port:
            logging.error("âŒ ERROR: No available ports found")
            exit(1)
        
        logging.info(f"ðŸš€ Starting server on port {port}...")
        logging.info(f"ðŸ“ Open your browser to: http://localhost:{port}\n")
        logging.info("Press Ctrl+C to stop\n")
        logging.info("â±ï¸  Note: Large polygon plots (762 gridboxes) may take 3-5 minutes to load\n")
        logging.info("=" * 70 + "\n")
        
        # Import uvicorn and configure with extended timeouts
        import uvicorn
        
        # Run uvicorn with custom config - this WILL apply the timeouts
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info",
            timeout_keep_alive=86400,
            ws_ping_interval=10,
            ws_ping_timeout=86400,
            ws_max_size=300 * 1024 * 1024,
            h11_max_incomplete_event_size=300 * 1024 * 1024,
            limit_concurrency=100,
            backlog=100
        )
        
    except KeyboardInterrupt:
        logging.info("\n\nðŸ‘‹ Server stopped")
    except Exception as e:
        logging.error(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
