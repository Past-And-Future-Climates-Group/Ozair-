from shiny import App, ui, render, reactive, Session
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import numpy as np

# Set Mapbox token for map functionality
# Using a public token for testing - in production, use your own token
px.set_mapbox_access_token("pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw")
import xarray as xr
import os
import re  # For regular expressions to parse filenames
from shinywidgets import output_widget, render_widget
try:
    from scipy.stats import linregress  # Preferred for trendlines
except Exception:  # Fallback if SciPy is unavailable
    linregress = None

# Configuration
DATA_DIR = "."

def compute_trendline(x_values, y_values):
    """Compute linear trendline for given x and y arrays.
    Returns (slope, intercept, x_line, y_line). Skips NaNs and requires >=2 points.
    """
    import numpy as np
    try:
        x_arr = np.asarray(x_values, dtype=float)
        y_arr = np.asarray(y_values, dtype=float)
        mask = np.isfinite(x_arr) & np.isfinite(y_arr)
        x_clean = x_arr[mask]
        y_clean = y_arr[mask]
        if x_clean.size < 2:
            return None
        if linregress is not None:
            res = linregress(x_clean, y_clean)
            slope, intercept = res.slope, res.intercept
        else:
            slope, intercept = np.polyfit(x_clean, y_clean, 1)
        x_line = np.array([x_clean.min(), x_clean.max()])
        y_line = slope * x_line + intercept
        return slope, intercept, x_line, y_line
    except Exception:
        return None
import geojson
import pickle  # For caching grid cells
import hashlib  # For stable hashing
import traceback
from datetime import datetime
import signal
import time

# --- Constants and File Paths ---
def load_netcdf_file_list():
    try:
        with open("netcdf_files.txt", "r") as f:
            files = [line.strip() for line in f if line.strip()]
            # Filter out Livneh files to exclude them from the dashboard
            filtered_files = [f for f in files if "livneh" not in f.lower()]
            print(f"Found {len(files)} NetCDF files in netcdf_files.txt")
            print(f"Filtered out Livneh files, using {len(filtered_files)} files for dashboard")
            return filtered_files
    except FileNotFoundError:
        print("Error: netcdf_files.txt not found")
        return []

NETCDF_FILE_LIST = load_netcdf_file_list()
# Latitude and Longitude bounds for the Santa Barbara region
SB_LAT_MIN, SB_LAT_MAX = 33.39, 35.14
SB_LON_MIN, SB_LON_MAX = -120.77, -118.96

# Cache file for common grid cells
GRID_CACHE_FILE = "grid_cells_cache.pkl"

def save_grid_cells_cache(common_cells, file_list_hash):
    """Save common grid cells to cache file"""
    try:
        cache_data = {
            'common_cells': common_cells,
            'file_list_hash': file_list_hash,
            'timestamp': os.path.getmtime("netcdf_files.txt") if os.path.exists("netcdf_files.txt") else 0
        }
        with open(GRID_CACHE_FILE, 'wb') as f:
            pickle.dump(cache_data, f)
        print(f"Saved {len(common_cells)} grid cells to cache")
    except Exception as e:
        print(f"Error saving grid cache: {e}")

def compute_file_list_hash():
    """Compute hash of current file list for cache validation"""
    # Use hashlib for stable hashing
    file_list_str = '\n'.join(sorted(NETCDF_FILE_LIST))
    return hashlib.md5(file_list_str.encode()).hexdigest()

def load_grid_cells_cache():
    """Load common grid cells from cache file"""
    try:
        if not os.path.exists(GRID_CACHE_FILE):
            print("Cache file does not exist")
            return None, None
        
        with open(GRID_CACHE_FILE, 'rb') as f:
            cache_data = pickle.load(f)
        
        # Only check file list hash for cache validity
        current_hash = compute_file_list_hash()
        cached_hash = cache_data['file_list_hash']
        
        print(f"DEBUG: Current file list hash: {current_hash}")
        print(f"DEBUG: Cached file list hash: {cached_hash}")
        print(f"DEBUG: Hash match: {current_hash == cached_hash}")
        
        if current_hash == cached_hash:
            print(f"Loaded {len(cache_data['common_cells'])} grid cells from cache")
            return cache_data['common_cells'], cache_data['file_list_hash']
        else:
            print("Cache invalid - file list changed")
            return None, None
    except Exception as e:
        print(f"Error loading grid cache: {e}")
        return None, None

# --- Helper Functions for NetCDF Data Parsing and Metadata ---
def parse_filename(filename):
    """
    Parses a NetCDF filename to extract variable, model, and SSP.
    Example: '06083_tasmin_day_TaiESM1_ssp370_r1i1p1f1.nc'
    Handles filenames with '(1)' like '...MPI-ESM1-2-HR_ssp585_r1i1p1f1 (1).nc'
    """
    # Remove the .nc extension and any trailing '(number)'
    base_filename = os.path.splitext(filename)[0]
    base_filename = re.sub(r' \((\d+)\)$', '', base_filename)  # remove (1) if present

    parts = base_filename.split('_')
    # Handle the original file names which don't have the '06083' prefix
    if len(parts) >= 4 and parts[0] in ["tasmin", "tasmax", "pr", "hursmax", "hursmin", "huss", "rsds", "wspeed"]:
        # Assuming format like: var_day_model_ssp_r1i1p1f1
        variable = parts[0]
        model = parts[2]
        ssp = parts[3]
        return variable, model, ssp
    elif len(parts) >= 5:  # For new files like 06083_hursmax_day_GFDL-ESM4_ssp245_r1i1p1f1.nc
        # Assuming format like: 06083_var_day_model_ssp_r1i1p1f1
        variable = parts[1]  # e.g., 'tasmin', 'pr'
        model = parts[3]     # e.g., 'TaiESM1', 'GFDL-ESM4'
        ssp = parts[4]       # e.g., 'ssp370', 'ssp245'
        return variable, model, ssp
    return None, None, None

def get_variable_metadata(variable_name):
    """
    Returns display name, units, and a default color for a given climate variable.
    """
    metadata = {
        # Projected Climate Variables
        "tasmin": {"display_name": "Minimum Temperature Projections", "units": "°F", "color": "blue"},
        "tasmax": {"display_name": "Maximum Temperature Projections", "units": "°F", "color": "red"},
        "pr": {"display_name": "Precipitation Projections", "units": "inches/day", "color": "green"},
        "hursmax": {"display_name": "Maximum Relative Humidity Projections", "units": "%", "color": "purple"},
        "hursmin": {"display_name": "Minimum Relative Humidity Projections", "units": "%", "color": "darkorange"},
        "huss": {"display_name": "Specific Humidity Projections", "units": "kg/kg", "color": "brown"},
        "rsds": {"display_name": "Surface Downwelling Shortwave Radiation Projections", "units": "W/m²", "color": "goldenrod"},
        "wspeed": {"display_name": "Wind Speed Projections", "units": "m/s", "color": "teal"},
        
        # Observed Climate Variables
        "obs_pr": {"display_name": "Observed Precipitation", "units": "inches/day", "color": "darkgreen"},
        "obs_tmax": {"display_name": "Observed Maximum Temperature", "units": "°F", "color": "darkred"},
        "obs_tmin": {"display_name": "Observed Minimum Temperature", "units": "°F", "color": "darkblue"},
    }
    
    result = metadata.get(variable_name, {"display_name": variable_name, "units": "Unknown", "color": "gray"})
    return result

def get_variable_units_for_frequency(variable_name, frequency):
    """
    Returns the appropriate units for a variable based on its aggregation frequency.
    For precipitation, units change when aggregated (daily -> monthly/annual).
    """
    base_metadata = get_variable_metadata(variable_name)
    base_units = base_metadata.get('units', 'Unknown')
    
    # For precipitation variables, units change when aggregated
    if variable_name in ["pr", "obs_pr"]:
        if frequency in ["monthly", "annual"]:
            # Aggregated precipitation: remove "/day" from units
            if base_units.endswith("/day"):
                return base_units.replace("/day", "")
            elif base_units.endswith(" inches/day"):
                return base_units.replace(" inches/day", " inches")
            else:
                return "inches"  # fallback
        else:
            return base_units  # daily frequency keeps original units
    else:
        # All other variables keep the same units regardless of frequency
        return base_units

def get_variable_display_name(variable_name):
    """Get the user-friendly display name for a climate variable"""
    if not variable_name:
        return ""
    
    # Metadata dictionary with display names for variables
    metadata = {
        # Projected Climate Variables
        "tasmin": "Minimum Temperature Projections",
        "tasmax": "Maximum Temperature Projections",
        "pr": "Precipitation Projections",
        "hursmax": "Maximum Relative Humidity Projections",
        "hursmin": "Minimum Relative Humidity Projections",
        "huss": "Specific Humidity Projections",
        "rsds": "Surface Downwelling Shortwave Radiation Projections",
        "wspeed": "Wind Speed Projections",
        
        # Observed Climate Variables
        "obs_pr": "Observed Precipitation",
        "obs_tmax": "Observed Maximum Temperature",
        "obs_tmin": "Observed Minimum Temperature"
    }
    
    # Try to get the display name from the metadata
    display_name = metadata.get(variable_name)
    
    # If no display name found, create a generic one
    if not display_name:
        # Remove 'obs_' prefix if present
        clean_var = variable_name.replace('obs_', '')
        
        # Split the variable name and capitalize words
        words = clean_var.split('_')
        words = [word.capitalize() for word in words]
        
        # Add appropriate suffix
        if variable_name.startswith('obs_'):
            display_name = f"Observed {' '.join(words)}"
        else:
            display_name = f"{' '.join(words)} Projections"
    
    return display_name

def create_variable_choices(available_variables):
    """Create variable choices with display names for dropdowns
    Returns a dict where keys are technical names (values returned) and values are display names (labels shown)
    """
    if not available_variables:
        return {"": "Select a variable..."}
    
    # Create choices as a dictionary with technical names as keys and display names as values
    # This is the correct format for Shiny: {value: label}
    choices = {"": "Select a variable..."}  # Start with blank option
    
    # Group variables by type (projected vs observational)
    proj_vars = [var for var in available_variables if not var.startswith('obs_')]
    obs_vars = [var for var in available_variables if var.startswith('obs_')]
    
    # Add projected variables if present
    if proj_vars:
        for var in sorted(set(proj_vars)):
            display_name = get_variable_display_name(var)
            choices[var] = display_name  # REVERSED: technical name as key, display name as value
    
    # Add observational variables if present
    if obs_vars:
        for var in sorted(set(obs_vars)):
            display_name = get_variable_display_name(var)
            choices[var] = display_name  # REVERSED: technical name as key, display name as value
    
    return choices

def get_variable_from_display_name(display_name, available_variables):
    """Get the technical variable name from a display name"""
    print(f"DEBUG: get_variable_from_display_name - Input display_name: {display_name}")
    print(f"DEBUG: get_variable_from_display_name - Available variables: {available_variables}")
    
    if not display_name:
        return ""
    
    # Mapping of display names to technical variable names
    display_to_var = {
        # Projected Climate Variables
        "Minimum Temperature Projections": "tasmin",
        "Maximum Temperature Projections": "tasmax",
        "Precipitation Projections": "pr",
        "Maximum Relative Humidity Projections": "hursmax",
        "Minimum Relative Humidity Projections": "hursmin",
        "Specific Humidity Projections": "huss",
        "Surface Downwelling Shortwave Radiation Projections": "rsds",
        "Wind Speed Projections": "wspeed",
        
        # Observed Climate Variables
        "Observed Precipitation": "obs_pr",
        "Observed Maximum Temperature": "obs_tmax",
        "Observed Minimum Temperature": "obs_tmin"
    }
    
    # First, try direct mapping
    var = display_to_var.get(display_name)
    print(f"DEBUG: get_variable_from_display_name - Direct mapped var: {var}")
    
    # If found and in available variables, return it
    if var and var in available_variables:
        print(f"DEBUG: get_variable_from_display_name - Returning direct mapped var: {var}")
        return var
    
    # If not found or not in available variables, try a more flexible approach
    for var in available_variables:
        display_var = get_variable_display_name(var)
        print(f"DEBUG: get_variable_from_display_name - Checking var: {var}, display_var: {display_var}")
        if display_var == display_name:
            print(f"DEBUG: get_variable_from_display_name - Returning matched var: {var}")
            return var
    
    # If no match found, return the display name as-is (fallback)
    print(f"DEBUG: get_variable_from_display_name - No match found, returning display_name: {display_name}")
    return display_name

def _are_units_compatible(units1, units2):
    """Check if two units are compatible for plotting together"""
    if not units1 or not units2:
        return False
    
    # Normalize units for comparison
    units1_norm = units1.lower().strip()
    units2_norm = units2.lower().strip()
    
    # Temperature units
    temp_units = ['°c', 'c', 'celsius', '°f', 'f', 'fahrenheit', 'k', 'kelvin']
    if units1_norm in temp_units and units2_norm in temp_units:
        print(f"DEBUG: Compatible temperature units - {units1} vs {units2}")
        return True
    
    # Relative humidity units (percentage)
    relative_humidity_units = ['%', 'percent']
    
    # Specific humidity units (mass ratio)
    specific_humidity_units = ['kg/kg', 'g/kg', 'kg kg-1', 'g g-1', '1']
    
    # Only allow compatibility within the same humidity type
    if units1_norm in relative_humidity_units and units2_norm in relative_humidity_units:
        print(f"DEBUG: Compatible relative humidity units - {units1} vs {units2}")
        return True
    if units1_norm in specific_humidity_units and units2_norm in specific_humidity_units:
        print(f"DEBUG: Compatible specific humidity units - {units1} vs {units2}")
        return True
    
    # Precipitation units
    precip_units = ['mm/day', 'mm day-1', 'mm d-1', 'kg m-2 s-1', 'mm/s', 'inches/day', 'inches']
    if units1_norm in precip_units and units2_norm in precip_units:
        print(f"DEBUG: Compatible precipitation units - {units1} vs {units2}")
        return True
    
    # Radiation units
    radiation_units = ['w/m2', 'w m-2', 'w/m²', 'w m-²', 'wm-2', 'wm-²', 'W m-2']
    if units1_norm in radiation_units and units2_norm in radiation_units:
        print(f"DEBUG: Compatible radiation units - {units1} vs {units2}")
        return True
    
    # Wind speed units
    wind_units = ['m/s', 'm s-1', 'ms-1', 'km/h', 'km h-1', 'mph']
    if units1_norm in wind_units and units2_norm in wind_units:
        print(f"DEBUG: Compatible wind speed units - {units1} vs {units2}")
        return True
    
    # Pressure units
    pressure_units = ['pa', 'hpa', 'mbar', 'bar', 'millibar']
    if units1_norm in pressure_units and units2_norm in pressure_units:
        print(f"DEBUG: Compatible pressure units - {units1} vs {units2}")
        return True
    
    print(f"DEBUG: Incompatible units - {units1} vs {units2}")
    return False

def check_unit_compatibility(var1, var2, filepaths_map=None, freq1=None, freq2=None):
    """Check if two variables have compatible units for plotting together using standardized units
    
    Args:
        var1: First variable name
        var2: Second variable name
        filepaths_map: Optional map of file paths (ignored - using standardized units)
        freq1: Optional frequency for var1 ('daily', 'monthly', 'annual')
        freq2: Optional frequency for var2 ('daily', 'monthly', 'annual')
    """
    if not var1 or not var2:
        print(f"DEBUG: check_unit_compatibility - One or both variables are None: var1={var1}, var2={var2}")
        return False
    
    # For precipitation, frequency compatibility:
    # - daily: inches/day (rate) - cannot mix with monthly/annual
    # - monthly: inches (total) - CAN mix with annual
    # - annual: inches (total) - CAN mix with monthly
    if var1 in ["pr", "obs_pr"] and var2 in ["pr", "obs_pr"]:
        if freq1 and freq2:
            # Daily data (rate) cannot be mixed with aggregated data (totals)
            if freq1 == 'daily' and freq2 in ['monthly', 'annual']:
                print(f"DEBUG: Incompatible - daily precipitation (inches/day) cannot mix with {freq2} precipitation (inches)")
                return False
            if freq2 == 'daily' and freq1 in ['monthly', 'annual']:
                print(f"DEBUG: Incompatible - daily precipitation (inches/day) cannot mix with {freq1} precipitation (inches)")
                return False
            # Monthly and annual can be plotted together (both are totals in inches)
            print(f"DEBUG: Compatible - precipitation variables with frequencies {freq1} and {freq2}")
        else:
            print(f"DEBUG: Compatible - both precipitation variables (frequency check skipped)")
        return True
    
    # Same variable with same or unspecified frequency is always compatible
    if var1 == var2:
        print(f"DEBUG: Compatible - same variable: {var1}")
        return True
    
    # Use standardized units from variable metadata instead of file units
    print("DEBUG: Checking compatibility using standardized units")
    metadata1 = get_variable_metadata(var1)
    metadata2 = get_variable_metadata(var2)
    
    units1 = metadata1.get('units', 'Unknown')
    units2 = metadata2.get('units', 'Unknown')
    
    print(f"DEBUG: Standardized units - {var1}: {units1}, {var2}: {units2}")
    
    # Same units can be plotted together
    if units1 == units2:
        print(f"DEBUG: Compatible - same standardized units: {units1}")
        return True
    
    # Check for unit compatibility (e.g., °C vs °F, kg/kg vs g/kg)
    if _are_units_compatible(units1, units2):
        print(f"DEBUG: Compatible - compatible standardized units: {units1} vs {units2}")
        return True
    
    # If units are not compatible, they cannot be plotted together
    print(f"DEBUG: Incompatible - different standardized units: {units1} vs {units2}")
    return False

def _get_actual_units_from_files(variable, filepaths_map):
    """Extract actual units from NetCDF files for a given variable"""
    # Handle observed variables with known target units
    if variable in ("obs_tmin", "obs_tmax"):
        return "°F"
    if variable == "obs_pr":
        return "inches/day"

    if not filepaths_map:
        return None
    
    # Find a file with this variable
    sample_file = None
    for (var, model, ssp), filepath in filepaths_map.items():
        if var == variable:
            sample_file = filepath
            break
    
    if not sample_file:
        print(f"DEBUG: No file found for variable {variable}")
        return None
    
    try:
        # Open the file and check the variable's units attribute
        with xr.open_dataset(sample_file, decode_times=False) as ds:
            if variable in ds.data_vars:
                var_data = ds[variable]
                if hasattr(var_data, 'attrs') and 'units' in var_data.attrs:
                    units = var_data.attrs['units']
                    print(f"DEBUG: Found units for {variable}: {units}")
                    return units
                else:
                    print(f"DEBUG: No units attribute found for {variable}")
                    return None
            else:
                print(f"DEBUG: Variable {variable} not found in dataset")
                return None
    except Exception as e:
        print(f"DEBUG: Error reading units from {sample_file}: {e}")
        return None

def _fallback_unit_compatibility(var1, var2):
    """Fallback compatibility check using logical grouping when actual units can't be determined"""
    print(f"DEBUG: Fallback compatibility check for {var1} vs {var2}")
    
    # Observed vs projection temperature variables are compatible
    if ((var1 in ['obs_tmax', 'obs_tmin'] and var2 in ['tasmax', 'tasmin']) or
        (var2 in ['obs_tmax', 'obs_tmin'] and var1 in ['tasmax', 'tasmin'])):
        print(f"DEBUG: Compatible - observed temperature vs projection temperature")
        return True
    
    # Observed precipitation vs projection precipitation are compatible
    if ((var1 == 'obs_pr' and var2 == 'pr') or (var2 == 'obs_pr' and var1 == 'pr')):
        print(f"DEBUG: Compatible - observed precipitation vs projection precipitation")
        return True
    
    # Temperature variables can be plotted together
    if var1 in ['tasmax', 'tasmin'] and var2 in ['tasmax', 'tasmin']:
        print(f"DEBUG: Compatible - both temperature variables")
        return True
    
    # Relative humidity variables can be plotted together
    if var1 in ['hursmax', 'hursmin'] and var2 in ['hursmax', 'hursmin']:
        print(f"DEBUG: Compatible - both relative humidity variables")
        return True
    
    # Specific humidity variables can be plotted together (but not with relative humidity)
    if var1 in ['huss'] and var2 in ['huss']:
        print(f"DEBUG: Compatible - both specific humidity variables")
        return True
    
    # Precipitation and radiation are usually incompatible with others
    if var1 in ['pr', 'rsds'] or var2 in ['pr', 'rsds']:
        if var1 != var2:
            print(f"DEBUG: Not compatible - precipitation/radiation with other variables")
            return False
    
    # Wind speed is usually incompatible with others
    if var1 in ['wspeed'] or var2 in ['wspeed']:
        if var1 != var2:
            print(f"DEBUG: Not compatible - wind speed with other variables")
            return False
    
    # For other combinations, treat as incompatible (no permissive fallback)
    print(f"DEBUG: Not compatible - different variable groups")
    return False

def get_grid_in_bounds(ds):
    lats = ds['lat'].values
    lons = ds['lon'].values
    cells = []
    lon_min, lon_max = SB_LON_MIN, SB_LON_MAX
    if np.any(lons > 180) and SB_LON_MIN < 0:
        lon_min += 360
        lon_max += 360
    
    print(f"DEBUG: Santa Barbara bounds: lat {SB_LAT_MIN}-{SB_LAT_MAX}, lon {SB_LON_MIN}-{SB_LON_MAX}")
    print(f"DEBUG: Dataset bounds: lat {lats.min():.3f}-{lats.max():.3f}, lon {lons.min():.3f}-{lons.max():.3f}")
    
    for lat_idx, lat in enumerate(lats):
        for lon_idx, lon in enumerate(lons):
            if (SB_LAT_MIN <= lat <= SB_LAT_MAX and
                lon_min <= lon <= lon_max):
                cells.append((lat_idx, lon_idx))
    
    print(f"DEBUG: Found {len(cells)} grid cells within Santa Barbara bounds")
    return cells

def find_common_valid_gridcells(filepaths):
    if not filepaths:
        print("DEBUG: No filepaths provided to find_common_valid_gridcells")
        return set()
    
    print(f"DEBUG: Processing {len(filepaths)} files to find common valid grid cells")
    
    try:
        with xr.open_dataset(filepaths[0], engine="netcdf4", decode_times=False) as ref_ds:
            initial_cells = get_grid_in_bounds(ref_ds)
            common_cells = set(initial_cells)
            print(f"DEBUG: Initial grid cells from bounds: {len(initial_cells)}")
    except Exception as e:
        print(f"DEBUG: Error opening reference file {filepaths[0]}: {e}")
        return set()
    
    processed_files = 0
    for filepath in filepaths[1:]:
        if not common_cells:
            print("DEBUG: No common cells remaining, stopping")
            break
            
        processed_files += 1
        if processed_files % 2 == 0:  # More frequent progress updates
            print(f"DEBUG: Processed {processed_files}/{len(filepaths)} files, {len(common_cells)} cells remaining")
            
        try:
            with xr.open_dataset(filepath, engine="netcdf4", decode_times=False) as ds:
                var = list(ds.data_vars.keys())[0]
                # More efficient: check if the variable has any non-null data at all first
                if ds[var].isnull().all():
                    print(f"DEBUG: File {filepath} has no valid data, skipping")
                    continue
                    
                mask = ~ds[var].isnull().all(dim='time')
                valid_cells = set()
                
                # Only check cells that are still in common_cells
                for lat_idx, lon_idx in common_cells:
                    if (lat_idx < mask.sizes['lat'] and
                        lon_idx < mask.sizes['lon'] and
                        mask.isel(lat=lat_idx, lon=lon_idx).item()):
                        valid_cells.add((lat_idx, lon_idx))
                
                common_cells = common_cells.intersection(valid_cells)
                
        except Exception as e:
            print(f"DEBUG: Error processing {filepath}: {e}")
            # Continue processing other files instead of returning empty set
            continue
    
    result = sorted(common_cells)
    print(f"DEBUG: Final result: {len(result)} common valid grid cells")
    return result

# --- Pre-computation and Data Loading Functions ---
# Cache for observational gridbox mappings to avoid repeated calculations
_obs_gridbox_cache = {}
# Cache for map data to speed up loading
_map_data_cache = None


def load_observed_dataset(variable):
    """Load observed dataset for a given variable"""
    try:
        if variable == 'obs_tmax':
            obs_fp = "livneh_day_Tmax_clip_15Oct2014.1950-2013.nc"
        elif variable == 'obs_tmin':
            obs_fp = "livneh_day_Tmin_clip_15Oct2014.1950-2013.nc"
        elif variable == 'obs_pr':
            obs_fp = "livneh_day_Prec_clip_15Oct2014.1950-2013.nc"
        else:
            print(f"DEBUG: Unknown observed variable: {variable}")
            return None
        
        full_path = os.path.join(DATA_DIR, obs_fp)
        if not os.path.exists(full_path):
            print(f"DEBUG: Observed file not found: {full_path}")
            return None
        
        with xr.open_dataset(full_path, engine="netcdf4") as temp_ds:
            ds = temp_ds.load()
        return ds
    except Exception as e:
        print(f"DEBUG: Error loading observed dataset for {variable}: {e}")
        return None

def _find_common_observational_gridboxes():
    """Find common gridboxes across all observational datasets"""
    try:
        # Load all observational datasets
        obs_datasets = {}
        for var in ['obs_tmax', 'obs_tmin', 'obs_pr']:
            try:
                ds = load_observed_dataset(var)
                if ds is not None:
                    obs_datasets[var] = ds
            except Exception as e:
                print(f"DEBUG: Error loading observed dataset for {var}: {e}")
        
        if not obs_datasets:
            print("DEBUG: Could not load any observational datasets")
            return None
        
        # Extract spatial coordinates from the first dataset as reference
        first_var = list(obs_datasets.keys())[0]
        first_ds = obs_datasets[first_var]
        lat2d, lon2d, _, _ = _extract_spatial_coords(first_ds)
        
        if lat2d is None or lon2d is None:
            print("DEBUG: Could not extract spatial coordinates from observational dataset")
            return None
        
        # Find common valid gridboxes across all datasets
        common_gridboxes = []
        for i in range(lat2d.shape[0]):
            for j in range(lat2d.shape[1]):
                lat_val = lat2d[i, j]
                lon_val = lon2d[i, j]
                
                # Check if this gridbox is valid in all datasets
                is_valid = True
                for var, ds in obs_datasets.items():
                    try:
                        ds_lat2d, ds_lon2d, _, _ = _extract_spatial_coords(ds)
                        if ds_lat2d is None or ds_lon2d is None:
                            is_valid = False
                            break
                        
                        # Check if this lat/lon exists in this dataset
                        found = False
                        for di in range(ds_lat2d.shape[0]):
                            for dj in range(ds_lon2d.shape[1]):
                                if (abs(ds_lat2d[di, dj] - lat_val) < 0.001 and 
                                    abs(ds_lon2d[di, dj] - lon_val) < 0.001):
                                    found = True
                                    break
                            if found:
                                break
                        
                        if not found:
                            is_valid = False
                            break
                    except Exception as e:
                        print(f"DEBUG: Error checking gridbox for {var}: {e}")
                        is_valid = False
                        break
                
                # Add to common gridboxes if valid in all datasets
                if is_valid:
                    common_gridboxes.append((lat_val, lon_val))
                
        print(f"DEBUG: Found {len(common_gridboxes)} common observational gridboxes")
        return common_gridboxes
        
    except Exception as e:
        print(f"DEBUG: Error finding common observational gridboxes: {e}")
        return None

def _find_most_encompassing_obs_gridbox(projection_lat, projection_lon, obs_ds):
    """
    Find the observational gridbox that most encompasses the given projection gridbox.
    Only considers common gridboxes that exist in all 3 observational NetCDF files.
    Returns the lat_idx, lon_idx of the most encompassing observational gridbox.
    """
    try:
        # Create cache key based on coordinates and dataset info
        cache_key = (round(projection_lat, 4), round(projection_lon, 4), id(obs_ds))
        
        # Check cache first
        if cache_key in _obs_gridbox_cache:
            cached_result = _obs_gridbox_cache[cache_key]
            print(f"DEBUG: Using cached mapping for ({projection_lat:.4f}, {projection_lon:.4f}) -> ({cached_result[0]}, {cached_result[1]})")
            return cached_result
        
        # Get common observational gridboxes
        common_obs_gridboxes = _find_common_observational_gridboxes()
        if not common_obs_gridboxes:
            print(f"DEBUG: No common observational gridboxes found")
            return None, None
        
        # Extract spatial coordinates from observational dataset
        lat2d, lon2d, _, _ = _extract_spatial_coords(obs_ds)
        if lat2d is None or lon2d is None:
            print(f"DEBUG: Could not extract spatial coordinates from observational dataset")
            return None, None
        
        # Find the most encompassing observational gridbox from common ones
        best_lat_idx, best_lon_idx = None, None
        max_encompassment = -1
        
        print(f"DEBUG: Checking {len(common_obs_gridboxes)} common observational gridboxes for projection point ({projection_lat:.4f}, {projection_lon:.4f})")
        print(f"DEBUG: Sample obs gridbox indices: {list(common_obs_gridboxes[:5])}")
        
        valid_obs_count = 0
        bounds_failures = 0
        nan_failures = 0
        
        for obs_coords in common_obs_gridboxes:
            # common_obs_gridboxes contains coordinate pairs (lat, lon), not indices
            if len(obs_coords) != 2:
                bounds_failures += 1
                continue
            
            obs_lat, obs_lon = obs_coords
            
            # Skip invalid coordinates
            if np.isnan(obs_lat) or np.isnan(obs_lon):
                nan_failures += 1
                continue
                
            # Find the array indices that correspond to these coordinates
            # We need to find the closest lat/lon indices in the coordinate arrays
            lat_distances = np.abs(lat2d - obs_lat)
            lon_distances = np.abs(lon2d - obs_lon)
            
            # Find the minimum distance indices
            min_lat_idx, min_lon_idx = np.unravel_index(
                np.argmin(lat_distances + lon_distances), 
                lat2d.shape
            )
            
            # Verify the indices are within bounds
            if not (0 <= min_lat_idx < lat2d.shape[0] and 0 <= min_lon_idx < lat2d.shape[1]):
                bounds_failures += 1
                continue
                
            valid_obs_count += 1
            
            # Calculate how much this observational gridbox encompasses the projection point
            # Use a more robust distance-based approach that accounts for the Earth's curvature
            lat_diff = abs(obs_lat - projection_lat)
            lon_diff = abs(obs_lon - projection_lon)
            
            # Calculate distance in degrees (approximate)
            distance = np.sqrt(lat_diff**2 + lon_diff**2)
            
            # Calculate encompassment score (lower distance = higher encompassment)
            # Use a more generous scoring that allows for reasonable distances
            # Score of 1.0 for exact match, decreasing with distance
            encompassment_score = 1.0 / (1.0 + distance)
            
            # Additional bonus for very close matches (within 0.1 degrees)
            if distance < 0.1:
                encompassment_score *= 1.5
            
            if encompassment_score > max_encompassment:
                max_encompassment = encompassment_score
                best_lat_idx, best_lon_idx = min_lat_idx, min_lon_idx
        
        print(f"DEBUG: Processed {valid_obs_count} valid observational gridboxes, best score: {max_encompassment:.4f}")
        print(f"DEBUG: Bounds failures: {bounds_failures}, NaN failures: {nan_failures}")
        print(f"DEBUG: lat2d shape: {lat2d.shape}, lon2d shape: {lon2d.shape}")
        print(f"DEBUG: Projection point: ({projection_lat:.4f}, {projection_lon:.4f})")
        if best_lat_idx is not None and best_lon_idx is not None:
            print(f"DEBUG: Best match found at obs indices: ({best_lat_idx}, {best_lon_idx})")
        
        # Apply minimum encompassment threshold (corresponds to ~0.5 degree distance)
        # Reduced from 0.1 to 0.05 to allow more gridboxes to have observational data
        min_encompassment_threshold = 0.05
        
        if best_lat_idx is None or best_lon_idx is None or max_encompassment < min_encompassment_threshold:
            print(f"DEBUG: No valid observational gridbox found for projection point ({projection_lat:.4f}, {projection_lon:.4f}) - best score: {max_encompassment:.4f}, threshold: {min_encompassment_threshold}")
            return None, None
        
        # Get the actual coordinates of the selected observational gridbox
        obs_lat = lat2d[best_lat_idx, best_lon_idx]
        obs_lon = lon2d[best_lat_idx, best_lon_idx]
        
        print(f"DEBUG: Mapped projection ({projection_lat:.4f}, {projection_lon:.4f}) to most encompassing obs gridbox ({best_lat_idx}, {best_lon_idx}) at ({obs_lat:.4f}, {obs_lon:.4f}) - encompassment score: {max_encompassment:.4f}")
        
        # Validate the mapping by checking if the selected gridbox actually has data
        try:
            # Try to extract a small sample to verify data exists
            test_df = extract_timeseries(obs_ds, best_lat_idx, best_lon_idx, 'obs_tmax')
            if test_df is not None and not test_df.empty and not test_df['obs_tmax'].isna().all():
                print(f"DEBUG: ✅ Validation passed - selected obs gridbox has valid data")
            else:
                print(f"DEBUG: ⚠️  Validation warning - selected obs gridbox may not have valid data")
        except Exception as e:
            print(f"DEBUG: ⚠️  Validation error - could not verify data at selected obs gridbox: {e}")
        
        # Cache the result
        result = (best_lat_idx, best_lon_idx)
        _obs_gridbox_cache[cache_key] = result
        
        return result
        
    except Exception as e:
        print(f"DEBUG: Error finding most encompassing obs gridbox: {e}")
        return None, None
def _get_coordinates_from_grid_id(grid_id, filepaths_map):
    """
    Convert grid ID (lat_idx,lon_idx) to actual geographic coordinates.
    Returns (lat, lon) or (None, None) if conversion fails.
    """
    try:
        # Parse grid indices
        if ',' in grid_id:
            lat_str, lon_str = grid_id.split(',')
            lat_idx = int(lat_str)
            lon_idx = int(lon_str)
        else:
            return None, None
        
        # Use the first available file to get coordinate mapping
        if not filepaths_map:
            return None, None
            
        sample_file = next(iter(filepaths_map.values()))
        with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
            if 'lat' not in ds.coords or 'lon' not in ds.coords:
                return None, None
            
            # Check if indices are within bounds
            if not (0 <= lat_idx < ds['lat'].size and 0 <= lon_idx < ds['lon'].size):
                return None, None
            
            # Get actual geographic coordinates
            lat = float(ds['lat'].values[lat_idx])
            lon = float(ds['lon'].values[lon_idx])
            return lat, lon
    except Exception as e:
        print(f"DEBUG: Error converting grid ID to coordinates: {e}")
        return None, None
def _check_observational_data_availability(grid_id, variable, lat=None, lon=None):
    """
    Check if valid observational data exists for a given gridbox and variable.
    Returns True if valid data exists, False otherwise.
    
    Args:
        grid_id: Grid ID in format "lat_idx,lon_idx"
        variable: Variable name (obs_tmin, obs_tmax, obs_pr)
        lat: Optional latitude coordinate (if not provided, will be calculated from grid_id)
        lon: Optional longitude coordinate (if not provided, will be calculated from grid_id)
    """
    try:
        if variable not in ["obs_tmin", "obs_tmax", "obs_pr"]:
            return False
        
        # If coordinates are not provided, we can't proceed
        if lat is None or lon is None:
            print(f"DEBUG: No coordinates provided for observational data availability check")
            return False
        
        # Load the observational dataset
        ds = load_observed_dataset(variable)
        if ds is None:
            print(f"DEBUG: Could not load observed dataset for {variable}")
            return False
        
        # Try to find a valid observational gridbox for this projection point
        obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(lat, lon, ds)
        
        if obs_lat_idx is None or obs_lon_idx is None:
            print(f"DEBUG: No valid observational gridbox found for {variable} at grid {grid_id}")
            return False
        
        # Check if the data actually exists at this location
        try:
            # Try to extract a small sample of data to verify it exists
            df_sample = extract_timeseries(ds, obs_lat_idx, obs_lon_idx, variable)
            if df_sample is None or df_sample.empty:
                print(f"DEBUG: No data available for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
                return False
            
            # Check if the data has valid values (not all NaN)
            if df_sample[variable].isna().all():
                print(f"DEBUG: All data is NaN for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
                return False
            
            print(f"DEBUG: Valid observational data confirmed for {variable} at grid {grid_id} (obs coords: {obs_lat_idx}, {obs_lon_idx})")
            return True
            
        except Exception as e:
            print(f"DEBUG: Error checking data availability for {variable} at grid {grid_id}: {e}")
            return False
            
    except Exception as e:
        print(f"DEBUG: Error in _check_observational_data_availability: {e}")
        return False

def _precompute_map_data():
    """
    Pre-compute map data for instant loading using static file paths.
    Returns cached map data including grid cells and GeoJSON.
    """
    global _map_data_cache
    
    if _map_data_cache is not None:
        return _map_data_cache
    
    try:
        print("DEBUG: Pre-computing map data for instant loading...")
        
        # Use static file paths instead of reactive values for instant loading
        filepaths_list = []
        for filename in NETCDF_FILE_LIST:
            if os.path.exists(filename):
                filepaths_list.append(filename)
        
        if not filepaths_list:
            print("DEBUG: No files available for map pre-computation")
            return None
        
        # Check cache for grid cells
        current_hash = hashlib.md5(str(sorted(filepaths_list)).encode()).hexdigest()
        common_cells, cached_hash = load_grid_cells_cache()
        
        if common_cells is None or current_hash != cached_hash:
            print("DEBUG: Computing common grid cells for instant map loading...")
            # Use first 3 files for maximum efficiency - all files have identical grid structure
            sample_files = filepaths_list[:3]
            common_cells = find_common_valid_gridcells(sample_files)
            
            if common_cells:
                save_grid_cells_cache(common_cells, current_hash)
                print(f"DEBUG: Pre-computed {len(common_cells)} common grid cells")
            else:
                print("DEBUG: No common grid cells found during pre-computation")
                return None
        else:
            print(f"DEBUG: Using cached grid cells for instant map loading: {len(common_cells)} cells")
        
        # Create GeoJSON using first available file
        sample_file = filepaths_list[0]
        try:
            with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                
                _map_data_cache = {
                    'common_cells': common_cells,
                    'geojson_data': geojson_data,
                    'feature_ids': feature_ids,
                    'grid_feature_ids': [f"{lat},{lon}" for lat, lon in common_cells]
                }
                
                print("DEBUG: Instant map data pre-computation completed")
                return _map_data_cache
                
        except Exception as e:
            print(f"DEBUG: Error creating GeoJSON during pre-computation: {e}")
            return None
            
    except Exception as e:
        print(f"DEBUG: Error in instant map data pre-computation: {e}")
        return None

def _precompute_obs_gridbox_mappings(common_grid_indices, obs_ds):
    """
    Pre-compute mappings from all common projection gridboxes to their most encompassing observational gridboxes.
    This improves performance by avoiding repeated calculations.
    """
    try:
        print(f"DEBUG: Pre-computing observational gridbox mappings for {len(common_grid_indices)} common gridboxes...")
        
        # Extract spatial coordinates from projection dataset (for common gridboxes)
        # We need to get the coordinates of the common gridboxes
        # This is a simplified version - in practice, you'd need the projection dataset coordinates
        
        # For now, we'll let the mappings be computed on-demand with caching
        # This function serves as a placeholder for future optimization
        print(f"DEBUG: Observational gridbox mapping pre-computation completed (using on-demand caching)")
        
    except Exception as e:
        print(f"DEBUG: Error in pre-computing obs gridbox mappings: {e}")

def _extract_spatial_coords(ds):
    """Return (lat2d, lon2d, lat_dim, lon_dim) from a dataset that may use
    lat/lon (1D or 2D) or y/x. Longitudes are adjusted to -180..180 if needed.
    """
    import numpy as np

    # Try common lat/lon names (case-insensitive variants too)
    lat_name_candidates = ["lat", "latitude", "Lat", "Latitude"]
    lon_name_candidates = ["lon", "longitude", "Lon", "Longitude"]

    lat_var = None
    lon_var = None
    for ln in lat_name_candidates:
        if ln in ds:
            lat_var = ds[ln]
            break
        if ln in ds.coords:
            lat_var = ds.coords[ln]
            break
    for ln in lon_name_candidates:
        if ln in ds:
            lon_var = ds[ln]
            break
        if ln in ds.coords:
            lon_var = ds.coords[ln]
            break

    # If lat/lon not found, try y/x (assume geographic degrees if ranges match)
    if lat_var is None or lon_var is None:
        if "y" in ds.coords and "x" in ds.coords:
            y = ds.coords["y"].values
            x = ds.coords["x"].values
            # Heuristic: treat y/x as degrees if plausible ranges
            if y.ndim == 1 and x.ndim == 1:
                lat2d, lon2d = np.meshgrid(y, x, indexing="ij")
                # Adjust lon if 0..360
                if np.nanmax(lon2d) > 180:
                    lon2d = np.where(lon2d > 180, lon2d - 360, lon2d)
                return lat2d, lon2d, "y", "x"

    if lat_var is None or lon_var is None:
        # As a last resort, search variables by CF attributes
        try:
            for name, var in ds.variables.items():
                std = str(getattr(var, 'standard_name', '')).lower()
                units = str(getattr(var, 'units', '')).lower()
                if lat_var is None and (std == 'latitude' or 'degree' in units and 'north' in units):
                    lat_var = var
                if lon_var is None and (std == 'longitude' or 'degree' in units and 'east' in units):
                    lon_var = var
                if lat_var is not None and lon_var is not None:
                    break
        except Exception:
            pass
        if lat_var is None or lon_var is None:
            return None, None, None, None

    lat_vals = lat_var.values
    lon_vals = lon_var.values

    # Adjust 0..360 longitudes to -180..180
    if np.nanmax(lon_vals) > 180:
        lon_vals = np.where(lon_vals > 180, lon_vals - 360, lon_vals)

    # 1D lat/lon → broadcast to 2D
    if lat_vals.ndim == 1 and lon_vals.ndim == 1:
        lat_dim = lat_var.dims[0]
        lon_dim = lon_var.dims[0]
        lat2d, lon2d = np.meshgrid(lat_vals, lon_vals, indexing="ij")
        return lat2d, lon2d, lat_dim, lon_dim

    # 2D lat/lon
    if lat_vals.ndim == 2 and lon_vals.ndim == 2:
        lat_dim, lon_dim = lat_var.dims
        return lat_vals, lon_vals, lat_dim, lon_dim

    return None, None, None, None

def _open_dataset_fallback(path):
    """Open a NetCDF with multiple engine fallbacks. Returns xr.Dataset or None."""
    for eng in ("h5netcdf", "netcdf4", "scipy", None):
        try:
            return xr.open_dataset(path, engine=eng, decode_times=False)
        except Exception:
            continue
    return None


def _discover_obs_path(candidate_names):
    """Return first existing path matching any of candidate file basenames, searching workspace recursively."""
    try:
        for name in candidate_names:
            direct = os.path.join(os.getcwd(), name)
            if os.path.exists(direct):
                return direct
        import glob
        for name in candidate_names:
            matches = glob.glob(os.path.join(os.getcwd(), "**", name), recursive=True)
            if matches:
                return matches[0]
    except Exception:
        pass
    return None


def _extract_all_points_from_file(file_path):
    """Open a NetCDF file and return a list of (lat, lon) for all coordinate points.
    Uses multiple engine fallbacks and supports lat/lon or y/x (1D or 2D)."""
    try:
        ds = _open_dataset_fallback(file_path)
        if ds is None:
            return []
        lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
        if lat2d is None or lon2d is None:
            ds.close()
            return []
        # Build full list of points
        points = []
        try:
            total_i = lat2d.shape[0]
            total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
            for i in range(total_i):
                for j in range(total_j):
                    points.append((float(lat2d[i, j]), float(lon2d[i, j])))
        except Exception:
            pass
        ds.close()
        return points
    except Exception:
        return []


def _extract_nonzero_points_from_file(file_path):
    """Return (lat, lon) points where the data is non-zero for at least one time step.
    Supports time dim named 'time' or 'band'. Falls back to all points if no time dim.
    """
    try:
        ds = _open_dataset_fallback(file_path)
        if ds is None:
            return []
        # Choose primary data variable (exclude spatial_ref)
        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
        if not data_vars:
            ds.close(); return []
        var = data_vars[0]
        lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
        if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
            ds.close(); return []
        dims = ds[var].dims
        time_dim = 'time' if 'time' in dims else ('band' if 'band' in dims else None)
        points = []
        if time_dim and lat_dim in dims and lon_dim in dims:
            data = ds[var]
            data = data.where(np.isfinite(data)).fillna(0)
            mask = (data != 0).any(dim=time_dim)
            n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
            for i in range(n_i):
                for j in range(n_j):
                    if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                        points.append((float(lat2d[i, j]), float(lon2d[i, j])))
        else:
            # No time dim; include all coordinates
            try:
                total_i = lat2d.shape[0]
                total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
                for i in range(total_i):
                    for j in range(total_j):
                        points.append((float(lat2d[i, j]), float(lon2d[i, j])))
            except Exception:
                pass
        ds.close()
        return points
    except Exception:
        return []

def create_grid_geojson(common_valid_grid_indices, ds_for_coords):
    """
    Creates a GeoJSON FeatureCollection of grid cells based on a pre-computed
    set of common valid (lat_idx, lon_idx) pairs.
    """
    if not common_valid_grid_indices or ds_for_coords is None:
        print("Error: No common valid grid indices or coordinate dataset for GeoJSON creation.")
        return None, []

    print(f"Generating GeoJSON features for {len(common_valid_grid_indices)} common valid grid cells...")

    lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds_for_coords)
    if lat2d is None or lon2d is None:
        print("Error: Unable to extract spatial coordinates for GeoJSON creation.")
        return None, []

    # Estimate deltas from medians of neighbor diffs (fallback to small values)
    try:
        # Differences along each axis, ignore NaNs
        lat_diff_rows = np.nanmedian(np.abs(np.diff(lat2d, axis=0))) if lat2d.shape[0] > 1 else np.nan
        lon_diff_cols = np.nanmedian(np.abs(np.diff(lon2d, axis=1))) if lon2d.shape[1] > 1 else np.nan
        # Use the smaller of the two deltas to ensure square cells
        if np.isfinite(lat_diff_rows) and lat_diff_rows > 0 and np.isfinite(lon_diff_cols) and lon_diff_cols > 0:
            # Use the smaller delta to ensure square appearance
            delta_lat = min(lat_diff_rows, lon_diff_cols) / 2.0
            delta_lon = delta_lat  # Make them equal for square cells
        else:
            delta_lat = delta_lon = 0.05  # Fallback to equal values
    except Exception:
        delta_lat, delta_lon = 0.05, 0.05

    features = []
    feature_ids = []

    for lat_idx, lon_idx in common_valid_grid_indices:
        # Ensure indices are within bounds of the coordinate dataset
        if not (0 <= lat_idx < lat2d.shape[0] and 0 <= lon_idx < lon2d.shape[1]):
            print(f"Warning: Skipping out-of-bounds index ({lat_idx}, {lon_idx}) during GeoJSON creation.")
            continue

        lat = float(lat2d[lat_idx, lon_idx])
        lon = float(lon2d[lat_idx, lon_idx])

        display_lon_for_geojson = lon

        # Add small gap between cells to prevent visual overlap
        gap_factor = 0.95  # 5% gap between cells
        polygon = geojson.Polygon([[(display_lon_for_geojson - delta_lon * gap_factor, lat - delta_lat * gap_factor),
                                    (display_lon_for_geojson + delta_lon * gap_factor, lat - delta_lat * gap_factor),
                                    (display_lon_for_geojson + delta_lon * gap_factor, lat + delta_lat * gap_factor),
                                    (display_lon_for_geojson - delta_lon * gap_factor, lat + delta_lat * gap_factor),
                                    (display_lon_for_geojson - delta_lon * gap_factor, lat - delta_lat * gap_factor)]])
    
        feature_id = f"{lat_idx},{lon_idx}"
        properties = {"lat": lat, "lon": lon, "display_lat": f"{lat:.3f}", "display_lon": f"{display_lon_for_geojson:.3f}"}
        feature = geojson.Feature(geometry=polygon, id=feature_id, properties=properties)

        features.append(feature)
        feature_ids.append(feature_id)

    print(f"Finished GeoJSON generation. Total features created: {len(features)}.")
    return geojson.FeatureCollection(features), feature_ids

# --- Load CSV Data ---
def load_csv_minimum_temperature_data():
    """
    Loads minimum temperature data from a CSV file.
    """
    try:
        possible_filenames = ['chart.csv', 'chart (1).csv', 'chart1.csv']
        found_path = None
        for root, _, files in os.walk("."):
            for f in files:
                if f in possible_filenames:
                    found_path = os.path.join(root, f)
                    break
            if found_path:
                break
        if not found_path:
            raise FileNotFoundError("No CSV file matching expected names found.")

        try:
            df = pd.read_csv(found_path, skiprows=8, na_values=['', 'nan', 'NaN'])
        except Exception:
            df = pd.read_csv(found_path, na_values=['', 'nan', 'NaN'])

        df.rename(columns=lambda x: x.strip(), inplace=True)
        df["year"] = pd.to_numeric(df.iloc[:, 0], errors="coerce")
        df.dropna(subset=["year"], inplace=True)
        df["year"] = df["year"].astype(int)
    
        column_mapping = {
            "Observed": "Observed",
            "Modeled RCP 8.5 Range Min": "Modeled RCP 8.5 Range Min",
            "Modeled RCP 8.5 Range Max": "Modeled RCP 8.5 Range Max",
            "CanESM2 (Average)": "CanESM2 (Average)",
            "CNRM-CM5 (Cool/Wet)": "CNRM-CM5 (Cool/Wet)",
            "HadGEM2-ES (Warm/Dry)": "HadGEM2-ES (Warm/Dry)",
            "MIROC5 (Complement)": "MIROC5 (Complement)"
        }
    
        df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns}, inplace=True)

        return df
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return None

# --- Utility Functions for NetCDF Data ---

def calculate_mad(trace1_data, trace2_data, trace1_freq, trace2_freq, trace1_name, trace2_name):
    """
    Calculate Mean Absolute Difference (MAD) between two traces.
    
    Args:
        trace1_data: DataFrame with 'Date' and variable columns
        trace2_data: DataFrame with 'Date' and variable columns  
        trace1_freq: Frequency of trace1 ('daily', 'monthly', 'annual')
        trace2_freq: Frequency of trace2 ('daily', 'monthly', 'annual')
        trace1_name: Display name for trace1
        trace2_name: Display name for trace2
    
    Returns:
        dict with 'mad', 'categorization', 'common_years', 'n_points', 'error' keys
    """
    try:
        # Check if frequencies match
        if trace1_freq != trace2_freq:
            return {
                'mad': None,
                'categorization': 'Cannot compare - different frequencies',
                'common_years': None,
                'n_points': 0,
                'error': f'Frequencies do not match: {trace1_freq} vs {trace2_freq}'
            }
        
        # Get variable names from dataframes
        trace1_var = [col for col in trace1_data.columns if col != 'Date'][0]
        trace2_var = [col for col in trace2_data.columns if col != 'Date'][0]
        
        # Ensure we have Year column for all data
        if 'Year' not in trace1_data.columns:
            trace1_data['Year'] = trace1_data['Date'].dt.year
        if 'Year' not in trace2_data.columns:
            trace2_data['Year'] = trace2_data['Date'].dt.year
        
        # For daily and monthly data, match by exact date instead of just year
        if trace1_freq == 'daily' or trace1_freq == 'monthly':
            # Match by exact date for daily/monthly comparisons
            trace1_dates = set(pd.to_datetime(trace1_data['Date']).dt.date)
            trace2_dates = set(pd.to_datetime(trace2_data['Date']).dt.date)
            common_dates = sorted(trace1_dates.intersection(trace2_dates))
            
            print(f"DEBUG: MAD calculation - Trace1 has {len(trace1_dates)} {trace1_freq} dates")
            print(f"DEBUG: MAD calculation - Trace2 has {len(trace2_dates)} {trace2_freq} dates")
            print(f"DEBUG: MAD calculation - Common dates: {len(common_dates)}")
            
            if len(common_dates) == 0:
                return {
                    'mad': None,
                    'categorization': 'Cannot compare - no common time period',
                    'common_years': None,
                    'n_points': 0,
                    'error': 'No overlapping dates found'
                }
            
            # Filter to common dates and merge on date
            trace1_data['Date_normalized'] = pd.to_datetime(trace1_data['Date']).dt.date
            trace2_data['Date_normalized'] = pd.to_datetime(trace2_data['Date']).dt.date
            
            trace1_filtered = trace1_data[trace1_data['Date_normalized'].isin(common_dates)].sort_values('Date')
            trace2_filtered = trace2_data[trace2_data['Date_normalized'].isin(common_dates)].sort_values('Date')
            
            # Get year range for display
            common_years = f"{min(common_dates).year}-{max(common_dates).year}"
            
        else:
            # For annual data, match by year
            trace1_years = set(trace1_data['Year'].dropna())
            trace2_years = set(trace2_data['Year'].dropna())
            common_years_set = sorted(trace1_years.intersection(trace2_years))
            
            # Debug output for time ranges
            print(f"DEBUG: MAD calculation - Trace1 years: {sorted(trace1_years)}")
            print(f"DEBUG: MAD calculation - Trace2 years: {sorted(trace2_years)}")
            print(f"DEBUG: MAD calculation - Common years: {common_years_set}")
            
            if len(common_years_set) == 0:
                return {
                    'mad': None,
                    'categorization': 'Cannot compare - no common time period',
                    'common_years': None,
                    'n_points': 0,
                    'error': 'No overlapping years found'
                }
            
            # Filter to common years and get values
            trace1_filtered = trace1_data[trace1_data['Year'].isin(common_years_set)].sort_values('Year')
            trace2_filtered = trace2_data[trace2_data['Year'].isin(common_years_set)].sort_values('Year')
            common_years = f"{min(common_years_set)}-{max(common_years_set)}"
        # Ensure we have the same number of points
        if len(trace1_filtered) != len(trace2_filtered):
            return {
                'mad': None,
                'categorization': 'Cannot compare - different number of data points',
                'common_years': common_years,
                'n_points': 0,
                'error': f'Different number of points: {len(trace1_filtered)} vs {len(trace2_filtered)}'
            }
        
        # Get values for comparison
        values1 = trace1_filtered[trace1_var].values
        values2 = trace2_filtered[trace2_var].values
        
        # Remove any NaN values
        valid_mask = ~(np.isnan(values1) | np.isnan(values2))
        if not np.any(valid_mask):
            return {
                'mad': None,
                'categorization': 'Cannot compare - no valid data',
                'common_years': common_years,
                'n_points': 0,
                'error': 'No valid (non-NaN) data points found'
            }
        
        values1_valid = values1[valid_mask]
        values2_valid = values2[valid_mask]
        
        # Check if we have any data points for MAD calculation
        if len(values1_valid) < 2:
            return {
                'mad': None,
                'categorization': 'Insufficient data - need at least 2 data points',
                'common_years': f"{min(common_years)}-{max(common_years)}",
                'n_points': len(values1_valid),
                'error': f'Only {len(values1_valid)} data points available (minimum 2 required)'
            }
        
        # Calculate MAD
        mad = np.mean(np.abs(values1_valid - values2_valid))
        
        # Debug output for limited data cases
        if len(values1_valid) <= 5:
            print(f"DEBUG: MAD Debug - {len(values1_valid)} data points")
            print(f"DEBUG: Values1: {values1_valid}")
            print(f"DEBUG: Values2: {values2_valid}")
            print(f"DEBUG: Differences: {np.abs(values1_valid - values2_valid)}")
            print(f"DEBUG: Calculated MAD: {mad}")
            print(f"DEBUG: Common years: {common_years}")
        
        # Add warning for limited data
        warning = ""
        if len(values1_valid) < 5:
            warning = f"⚠️ Limited data ({len(values1_valid)} points) - results may be less reliable"
        
        # Categorize based on MAD value and variable type
        # Determine variable type for appropriate thresholds
        if trace1_var in ['tasmin', 'tasmax', 'obs_tmin', 'obs_tmax']:
            # Temperature variables (in Fahrenheit)
            if mad < 2.0:
                categorization = 'Excellent match'
            elif mad < 5.0:
                categorization = 'Good match'
            elif mad < 10.0:
                categorization = 'Fair match'
            else:
                categorization = 'Poor match'
        elif trace1_var in ['pr', 'obs_pr']:
            # Precipitation variables (in inches)
            if mad < 0.5:
                categorization = 'Excellent match'
            elif mad < 1.0:
                categorization = 'Good match'
            elif mad < 2.0:
                categorization = 'Fair match'
            else:
                categorization = 'Poor match'
        else:
            # Other variables - use relative thresholds
            # Calculate relative MAD as percentage of mean
            mean_val = np.mean([np.mean(values1_valid), np.mean(values2_valid)])
            if mean_val != 0:
                relative_mad = (mad / mean_val) * 100
                if relative_mad < 5:
                    categorization = 'Excellent match'
                elif relative_mad < 15:
                    categorization = 'Good match'
                elif relative_mad < 30:
                    categorization = 'Fair match'
                else:
                    categorization = 'Poor match'
            else:
                categorization = 'Cannot categorize'
        
        return {
            'mad': round(mad, 4),
            'categorization': categorization,
            'common_years': common_years,  # Already formatted as string in the logic above
            'n_points': len(values1_valid),
            'error': None,
            'warning': warning
        }
        
    except Exception as e:
        return {
            'mad': None,
            'categorization': 'Error in calculation',
            'common_years': None,
            'n_points': 0,
            'error': str(e)
        }

def extract_timeseries(ds, lat_idx, lon_idx, variable_name=None):
    """
    Extracts a time series for a specific grid point from an xarray Dataset.
    If the specified point has no data, tries to find a nearby valid point.
    """
    if ds is None:
        return pd.DataFrame({"Date": [], "No Data": []})
    
    # Adapt datasets that use 'band','x','y' instead of 'time','lat','lon' (e.g., observed Livneh NetCDFs)
    try:
        has_band = ('band' in getattr(ds, 'dims', {})) or ('band' in getattr(ds, 'coords', {}))
        if "time" not in ds.coords and has_band:
            # Synthesize daily time from 1950-01-01 for the number of bands
            num_bands = ds.dims.get('band', ds['band'].size if 'band' in ds.coords else None)
            if num_bands is not None and num_bands > 0:
                start_date = pd.Timestamp('1950-01-01')
                times = pd.date_range(start=start_date, periods=num_bands, freq='D')
                if 'band' in ds.dims:
                    ds = ds.rename({'band': 'time'})
                # Assign synthesized time coordinate
                ds = ds.assign_coords(time=("time", times))
        # Normalize spatial dims
        if 'lat' not in ds.coords and 'y' in getattr(ds, 'dims', {}):
            ds = ds.rename({'y': 'lat'})
        if 'lon' not in ds.coords and 'x' in getattr(ds, 'dims', {}):
            ds = ds.rename({'x': 'lon'})
    except Exception as adapt_err:
        print(f"DEBUG: extract_timeseries - adaptation error: {adapt_err}")
    
    if "time" not in ds.coords:
        return pd.DataFrame({"Date": [], "No Data": []})

    times = pd.to_datetime(ds["time"].values)

    # Map observed variable names to actual dataset variable names
    if variable_name and variable_name.startswith('obs_'):
        # For observed variables, find the actual data variable (not spatial_ref)
        data_vars = [var for var in ds.data_vars.keys() if var != 'spatial_ref']
        if data_vars:
            var_to_extract = data_vars[0]  # Use the first non-spatial_ref variable
        else:
            var_to_extract = list(ds.data_vars.keys())[0]
    else:
        var_to_extract = variable_name if variable_name and variable_name in ds.data_vars else list(ds.data_vars.keys())[0]

    if var_to_extract not in ds.data_vars:
        return pd.DataFrame({"Date": [], "No Data": []})

    # Try the specified point first
    try:
        values = ds[var_to_extract].isel(time=slice(None), lat=lat_idx, lon=lon_idx).values
        
        # Check if the data is valid (not all NaN)
        if not np.isnan(values).all():
            # Use the original variable_name for the column, not the extracted variable name
            column_name = variable_name if variable_name else var_to_extract
            df = pd.DataFrame({"Date": times, column_name: values})
            return df
    except IndexError as e:
        print(f"Error: Lat/Lon indices {lat_idx},{lon_idx} out of bounds for variable {var_to_extract}.")
    except Exception as e:
        print(f"An unexpected error occurred during time series extraction: {e}")

    # If the specified point has no data, search for a valid point
    try:
        # Get the data array
        data_array = ds[var_to_extract]
        
        # Find valid data points (non-NaN)
        valid_mask = ~data_array.isnull().all(dim='time')
        
        # Get valid indices
        valid_lat_indices = valid_mask.lat.values
        valid_lon_indices = valid_mask.lon.values
        
        # Find the closest valid point to the requested location
        min_distance = float('inf')
        best_lat_idx = None
        best_lon_idx = None
        
        for i in range(len(valid_lat_indices)):
            for j in range(len(valid_lon_indices)):
                if valid_mask.isel(lat=i, lon=j).item():
                    # Calculate distance to requested point
                    distance = abs(i - lat_idx) + abs(j - lon_idx)
                    if distance < min_distance:
                        min_distance = distance
                        best_lat_idx = i
                        best_lon_idx = j
        
        if best_lat_idx is not None and best_lon_idx is not None:
            values = data_array.isel(time=slice(None), lat=best_lat_idx, lon=best_lon_idx).values
            
            if not np.isnan(values).all():
                # Use the original variable_name for the column, not the extracted variable name
                column_name = variable_name if variable_name else var_to_extract
                df = pd.DataFrame({"Date": times, column_name: values})
                return df
            
    except Exception as e:
        print(f"DEBUG: Error searching for valid data points: {e}")

    # If all else fails, return empty DataFrame
    return pd.DataFrame({"Date": [], "No Data": []})
# --- UI Definition ---
app_ui = ui.page_fluid(
    ui.tags.head(
        ui.tags.meta(name="cache-control", content="no-cache, no-store, must-revalidate"),
        ui.tags.meta(name="pragma", content="no-cache"),
        ui.tags.meta(name="expires", content="0"),
        ui.tags.style("""
            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&display=swap');
            body { font-family: 'Inter', sans-serif; background-color: #f8f9fa; color: #333; overflow-x: auto; overflow-y: auto; }
            .custom-card { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border-radius: 15px; padding: 20px; margin: 10px 0; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); }
            .metric-card { background: white; border-radius: 10px; padding: 15px; margin: 10px 0; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); border-left: 4px solid #667eea; }
            .text-center { text-align: center; }
            .text-muted { color: #6c757d; }
            .container { width: 100% !important; max-width: none !important; margin: 0 auto; padding: 20px; background-color: #fff; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.05); margin-top: 20px; margin-bottom: 20px; }
            .row { width: 100% !important; margin: 0 !important; }
            .col-12, .col-lg-3, .col-lg-9 { padding-left: 10px; padding-right: 10px; }
            /* Override Bootstrap container constraints */
            .container-fluid { width: 100% !important; max-width: none !important; }
            /* Ensure full width for all main content */
            .main-content { width: 100% !important; max-width: none !important; }
            /* Force full width layout */
            html, body { width: 100% !important; margin: 0 !important; padding: 0 !important; }
            .shiny-app-container { width: 100% !important; max-width: none !important; }
            /* Override any remaining Bootstrap constraints */
            .container, .container-fluid, .container-sm, .container-md, .container-lg, .container-xl, .container-xxl {
                width: 100% !important;
                max-width: none !important;
            }
            /* Make Plotly widgets responsive */
            .js-plotly-plot, .plotly, .plotly-graph-div {
                width: 100% !important;
                max-width: 100% !important;
                height: auto !important;
                min-height: 500px !important;
            }
            /* Ensure map and plot containers are responsive */
            .map-container, .plot-container {
                width: 100% !important;
                max-width: 100% !important;
                overflow: visible !important;
            }
            /* MAD table responsive styling */
            .mad-table-container {
                width: 100% !important;
                max-width: none !important;
                overflow-x: auto !important;
                overflow-y: auto !important;
            }
            .mad-table-container .js-plotly-plot {
                width: 100% !important;
                max-width: none !important;
            }
            /* Ensure all UI components are responsive */
            .shiny-input-container, .form-group, .form-control {
                width: 100% !important;
                max-width: none !important;
            }
            /* Progressive dropdown responsive behavior */
            .progressive-dropdown-container {
                width: 100% !important;
                max-width: none !important;
            }
            /* Ensure all containers use full width */
            .container-fluid, .container {
                width: 100% !important;
                max-width: none !important;
            }
            h2 { font-size: 2.5rem; margin-bottom: 20px; color: #2c3e50; text-shadow: 1px 1px 2px rgba(0,0,0,0.1); }
            h3 { font-size: 2rem; margin-top: 20px; margin-bottom: 15px; color: #34495e; }
            h4 { font-size: 1.5rem; margin-top: 15px; margin-bottom: 10px; color: #34495e; }
            ul { list-style-type: disc; margin-left: 20px; }
            li { margin-bottom: 5px; }
            hr { border-top: 1px solid #eee; margin: 20px 0; }
            .shiny-input-container { margin-bottom: 15px; }
            .js-plotly-plot { margin-top: 20px; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; }
            .loading-message {
                text-align: center;
                font-size: 1.2em;
                color: #555;
                margin-top: 50px;
                padding: 20px;
                border: 1px solid #ddd;
                border-radius: 8px;
                background-color: #f0f0f0;
                box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            }
            #multi_plot_options {
                display: none;
                background-color: #f8f9fa;
                border: 1px solid #dee2e6;
                border-radius: 8px;
                padding: 15px;
                margin-top: 10px;
            }
            .multi-plot-section {
                border-left: 3px solid #667eea;
                padding-left: 15px;
                margin: 10px 0;
            }
            /* Hide Shiny client errors that are non-critical */
            .shiny-output-error-validation {
                display: none !important;
            }
            .shiny-output-error:has(.shiny-error-silent) {
                display: none !important;
            }
            /* Aggressively hide error modals and notifications */
            .modal:has(.shiny-notification-error),
            .shiny-modal:has(.shiny-notification-error),
            [role="dialog"]:has(.shiny-notification-error) {
                display: none !important;
                opacity: 0 !important;
                pointer-events: none !important;
            }
            .modal-backdrop {
                display: none !important;
            }
            #shiny-notification-panel .shiny-notification-error {
                display: none !important;
            }
            /* Hide specific error messages */
            .shiny-notification-error:has([class*="Cannot set properties"]),
            .shiny-notification:has([class*="Cannot set properties"]),
            .shiny-notification-error:has([class*="Widget is not attached"]),
            .shiny-notification:has([class*="Widget is not attached"]),
            .shiny-notification-error:has([class*="model cleanup"]),
            .shiny-notification:has([class*="model cleanup"]) {
                display: none !important;
            }
            /* Hide all widget-related client errors */
            div[role="status"]:has([class*="Shiny Client Errors"]),
            div:has(> div:has([class*="Shiny Client Errors"])),
            .shiny-notification-error,
            .shiny-notification:has([class*="Shiny Client Errors"]),
            div[class*="shiny-notification"]:has([class*="error"]),
            .modal-content:has([class*="error"]) {
                display: none !important;
            }
            
            /* Hide any error dialogs that might appear */
            .modal:has([class*="error"]),
            .modal:has([class*="Shiny Client Errors"]) {
                display: none !important;
            }
        """),
        ui.tags.script("""
            // Comprehensive widget error suppression
            // Override window.onerror to catch errors at the earliest stage
            (function() {
                const originalOnError = window.onerror;
                
                window.onerror = function(message, source, lineno, colno, error) {
                    const msgStr = String(message);
                    if (msgStr.includes('Widget is not attached') ||
                        msgStr.includes('model cleanup') ||
                        msgStr.includes('Cannot set properties of undefined') ||
                        msgStr.includes('Cannot read properties of undefined') ||
                        msgStr.includes('setting') ||
                        msgStr.includes('[anywidget]') ||
                        msgStr.includes('Runtime not found') ||
                        msgStr.includes('Could not create a view for model')) {
                        console.log('[Suppressed] Non-critical widget error:', msgStr);
                        return true; // Prevent default error handling
                    }
                    if (originalOnError) {
                        return originalOnError(message, source, lineno, colno, error);
                    }
                    return false;
                };
            })();
            
            // Also use addEventListener as backup
            window.addEventListener('error', function(event) {
                if (event.message && (
                    event.message.includes('Widget is not attached') ||
                    event.message.includes('model cleanup') ||
                    event.message.includes('Cannot set properties of undefined') ||
                    event.message.includes('Cannot read properties of undefined') ||
                    event.message.includes('setting') ||
                    event.message.includes('[anywidget]') ||
                    event.message.includes('Runtime not found') ||
                    event.message.includes('Could not create a view for model')
                )) {
                    console.log('[Suppressed] Non-critical widget error (listener):', event.message);
                    event.stopImmediatePropagation();
                    event.stopPropagation();
                    event.preventDefault();
                    return false;
                }
            }, true);
            
            // Override Shiny's error handling to prevent error dialogs
            // Wait for Shiny to be available, then override
            function overrideShinyErrors() {
                if (window.Shiny) {
                    var originalError = window.Shiny.onError;
                    window.Shiny.onError = function(error) {
                        // Convert error to string for checking
                        var errorStr = error ? (error.message || String(error)) : '';
                        
                        if (errorStr.includes('Cannot set properties of undefined') ||
                            errorStr.includes('Cannot read properties of undefined') ||
                            errorStr.includes('setting') ||
                            errorStr.includes('Widget is not attached') ||
                            errorStr.includes('model cleanup') ||
                            errorStr.includes('[anywidget]') ||
                            errorStr.includes('Runtime not found') ||
                            errorStr.includes('Could not create a view for model')) {
                            console.log('[Suppressed] Shiny error:', errorStr);
                            return false; // Suppress the error
                        }
                        if (originalError) {
                            return originalError(error);
                        }
                    };
                    console.log('[Info] Shiny error handler overridden');
                } else {
                    // Retry after a short delay if Shiny isn't loaded yet
                    setTimeout(overrideShinyErrors, 100);
                }
            }
            overrideShinyErrors();
            
            // Intercept and suppress error dialogs at the DOM level
            var observer = new MutationObserver(function(mutations) {
                mutations.forEach(function(mutation) {
                    if (mutation.type === 'childList') {
                        mutation.addedNodes.forEach(function(node) {
                            if (node.nodeType === 1) { // Element node
                                // FIXED: Convert className to string to handle SVG elements
                                var className = String(node.className || '');
                                if (className && (
                                    className.includes('shiny-notification') ||
                                    className.includes('modal') ||
                                    node.textContent && (
                                        node.textContent.includes('Shiny Client Errors') ||
                                        node.textContent.includes('Cannot set properties of undefined') ||
                                        node.textContent.includes('Cannot read properties of undefined')
                                    )
                                )) {
                                    console.log('Suppressing error dialog:', node);
                                    node.style.display = 'none';
                                    if (node.parentNode) {
                                        node.parentNode.removeChild(node);
                                    }
                                }
                            }
                        });
                    }
                });
            });
            
            // Start observing
            observer.observe(document.body, {
                childList: true,
                subtree: true
            });
            
            // Also suppress promise rejection errors related to widgets
            window.addEventListener('unhandledrejection', function(event) {
                if (event.reason && event.reason.message && (
                    event.reason.message.includes('Widget is not attached') ||
                    event.reason.message.includes('model cleanup') ||
                    event.reason.message.includes('[anywidget]')
                )) {
                    console.log('Suppressed non-critical widget promise rejection:', event.reason.message);
                    event.preventDefault();
                    return false;
                }
            });
        """),
        ui.tags.style("""
            /* Legend scrolling styles for Plotly plots - positioned below plot */
            /* FIXED: Legend at bottom of plot, scrollable horizontally, fixed height */
            .js-plotly-plot .legend {
                max-height: 60px !important;
                max-width: 90% !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                position: relative !important;
                z-index: 1000 !important;
                margin: 10px auto !important;
                padding: 5px 10px !important;
                /* Scrollbar styling */
                scrollbar-width: thin !important;
                scrollbar-color: rgba(0,0,0,0.3) rgba(0,0,0,0.1) !important;
            }

            /* Webkit scrollbar styling for legend */
            .js-plotly-plot .legend::-webkit-scrollbar {
                height: 6px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-track {
                background: rgba(0,0,0,0.1) !important;
                border-radius: 3px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-thumb {
                background: rgba(0,0,0,0.3) !important;
                border-radius: 3px !important;
            }

            .js-plotly-plot .legend::-webkit-scrollbar-thumb:hover {
                background: rgba(0,0,0,0.5) !important;
            }

            .js-plotly-plot .legend .legendscrollbox {
                max-height: 55px !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                width: 100% !important;
            }

            /* Ensure legend items are properly sized and scrollable horizontally */
            .js-plotly-plot .legend .traces {
                max-height: 55px !important;
                overflow-y: hidden !important;
                overflow-x: auto !important;
                white-space: nowrap !important;
                display: flex !important;
                flex-direction: row !important;
                gap: 15px !important;
            }

            /* Make legend items more compact for horizontal layout */
            .js-plotly-plot .legend .legendtext {
                font-size: 9px !important;
                line-height: 1.2 !important;
                white-space: nowrap !important;
            }
            
            /* Ensure legend items are horizontally aligned */
            .js-plotly-plot .legend .legendtoggle {
                display: inline-flex !important;
                align-items: center !important;
                margin-right: 10px !important;
            }
            
            /* Plot container dimensions - now scrollable */
            .plot-container {
                width: 100% !important;
                min-height: 500px !important;
                overflow: visible !important;
                position: relative !important;
            }
            
            /* Plotly plots - allow larger sizes */
            .plot-container .js-plotly-plot {
                width: 100% !important;
                min-height: 500px !important;
                overflow: visible !important;
            }
        """),
        ui.tags.style("""
            /* Progressive dropdown styles */
            .dropdown-step {
                background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
                border: 2px solid #dee2e6;
                border-radius: 10px;
                padding: 20px;
                margin: 15px 0;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                transition: all 0.3s ease;
                opacity: 0.3;
                pointer-events: none;
            }
            .dropdown-step.active {
                border-color: #667eea;
                background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
                box-shadow: 0 4px 8px rgba(102, 126, 234, 0.2);
                opacity: 1;
                pointer-events: auto;
            }
            .dropdown-step.completed {
                border-color: #28a745;
                background: linear-gradient(135deg, #f8fff9 0%, #e8f5e8 100%);
                opacity: 1;
                pointer-events: auto;
            }
            .step-explanation {
                font-size: 0.9em;
                color: #6c757d;
                margin-top: 8px;
                font-style: italic;
            }
            .step-title {
                font-weight: 600;
                color: #495057;
                margin-bottom: 10px;
            }
            .progressive-dropdown-container {
                width: 100%;
                margin: 0 auto;
            }
            /* Plotly plot width fixes */
            .plotly-graph-div {
                width: 100% !important;
                max-width: none !important;
                overflow: visible !important;
            }
            .plotly .main-svg {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxis {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxislayer-above {
                width: 100% !important;
                max-width: none !important;
            }
            .plotly .xaxislayer-below {
                width: 100% !important;
                max-width: none !important;
            }
            .js-plotly-plot {
                width: 100% !important;
                max-width: none !important;
                overflow: visible !important;
            }
            /* Animation for completion */
            .pulse-animation {
                animation: pulse 1s ease-in-out;
            }
            @keyframes pulse {
                0% { transform: scale(1); }
                50% { transform: scale(1.02); }
                100% { transform: scale(1); }
            }
            .animation-done {
                transition: all 0.3s ease;
            }
            /* Ensure button centering is maintained */
            .text-center {
                text-align: center !important;
            }
            /* Specific styling for the reset selection button container */
            .dropdown-step .text-center {
                display: flex !important;
                justify-content: center !important;
                align-items: center !important;
                width: 100% !important;
            }
            /* Ensure the button itself is centered */
            .dropdown-step .btn {
                margin: 0 auto !important;
                display: block !important;
            }
        """),
        ui.tags.script("""
            // AGGRESSIVE error suppression for Shiny dynamic UI updates
            (function() {
                // Override console.error to suppress Shiny-related errors
                var originalError = console.error;
                console.error = function() {
                    var args = Array.from(arguments);
                    var message = args.join(' ');
                    // Suppress specific Shiny errors
                    if (message.includes('Cannot set properties of undefined') ||
                        message.includes('Cannot read properties of undefined') ||
                        message.includes('SilentException') ||
                        message.includes('setting \'')) {
                        console.log('Suppressed Shiny error:', message);
                        return;
                    }
                    originalError.apply(console, arguments);
                };
                
                // Suppress global errors
                window.addEventListener('error', function(event) {
                    if (event.error && event.error.message) {
                        var msg = event.error.message;
                        if (msg.includes('Cannot set properties of undefined') ||
                            msg.includes('Cannot read properties of undefined') ||
                            msg.includes('setting \'') ||
                            msg.includes('SilentException')) {
                            event.preventDefault();
                            event.stopPropagation();
                            event.stopImmediatePropagation();
                            console.log('Suppressed error:', msg);
                            return false;
                        }
                    }
                }, true);
                
                // Suppress promise rejections
                window.addEventListener('unhandledrejection', function(event) {
                    if (event.reason) {
                        var msg = (event.reason.message || event.reason || '').toString();
                        if (msg.includes('Cannot set properties of undefined') ||
                            msg.includes('Cannot read properties of undefined') ||
                            msg.includes('setting \'') ||
                            msg.includes('SilentException')) {
                            event.preventDefault();
                            event.stopPropagation();
                            console.log('Suppressed promise rejection:', msg);
                            return false;
                        }
                    }
                });
                
                // Aggressively hide Shiny error notifications and modal dialogs
                function hideErrorNotifications() {
                    // Hide error notifications
                    var notifs = document.querySelectorAll(
                        '.shiny-notification-error, ' +
                        '.shiny-output-error, ' +
                        '.shiny-output-error-validation, ' +
                        '[id*="shiny-notification"]'
                    );
                    notifs.forEach(function(el) {
                        var text = (el.textContent || el.innerText || '').toLowerCase();
                        if (text.includes('cannot set properties') ||
                            text.includes('cannot read properties') ||
                            text.includes('undefined') ||
                            text.includes('setting') ||
                            text.includes('silentexception')) {
                            el.remove();
                        }
                    });
                    
                    // Also check for modals/dialogs
                    var modals = document.querySelectorAll('.modal, .shiny-modal, [role="dialog"]');
                    modals.forEach(function(modal) {
                        var text = (modal.textContent || modal.innerText || '').toLowerCase();
                        if (text.includes('shiny client errors') ||
                            text.includes('cannot set properties') ||
                            text.includes('error on client')) {
                            modal.remove();
                            // Also remove backdrop if present
                            var backdrop = document.querySelector('.modal-backdrop');
                            if (backdrop) backdrop.remove();
                        }
                    });
                }
                
                // Run immediately and then every 50ms
                hideErrorNotifications();
                setInterval(hideErrorNotifications, 50);
                
                // Also run on DOM changes (catches newly added error elements)
                if (typeof MutationObserver !== 'undefined') {
                    var observer = new MutationObserver(hideErrorNotifications);
                    observer.observe(document.body, {
                        childList: true,
                        subtree: true
                    });
                }
            })();
        """),
        ui.tags.link(rel="stylesheet", href="https://api.mapbox.com/mapbox-gl-js/v2.15.0/mapbox-gl.css"),
    ),
    ui.h2("🌊 Santa Barbara Climate Data Dashboard", class_="text-center"),
    ui.div(ui.p("📊 Data Source: Cal-Adapt CSV & NetCDF", class_="text-center text-muted"), class_="data-source"),
    ui.navset_tab(
        ui.nav_panel("🏠 Welcome", ui.div(ui.h3("Welcome!", class_="text-center"), ui.p("Explore Santa Barbara's climate using Cal-Adapt data.", class_="text-center"), class_="container")),
        ui.nav_panel("📖 User Guide",
            ui.div(
                ui.h3("How to Use This Dashboard"),
                ui.h4("Getting Started"),
                ui.p("This dashboard provides access to climate data for the Santa Barbara region. Follow these steps to explore the data:"),
                ui.tags.ol([
                    ui.tags.li("Navigate to the 'Gridbox Map' tab to explore NetCDF data."),
                    ui.tags.li("On the map, click any grid cell (puzzle piece) to select it."),
                    ui.tags.li("The selected cell will be highlighted with a red border, and the climate plots below will update automatically."),
                    ui.tags.li("Alternatively, use the 'Select Gridbox' dropdown to choose a grid cell."),
                    ui.tags.li("Use the 'Climate Variable', 'Climate Model', and 'SSP Scenario' dropdowns to change which data is plotted for the selected grid cell."),
                    ui.tags.li("Use the 'Variable Maps' tab to explore spatial distributions of climate variables."),
                ]),
                ui.h4("Features"),
                ui.tags.ul([
                    ui.tags.li("Time series plots with trend analysis"),
                    ui.tags.li("Mean Absolute Difference (MAD) analysis between different climate models"),
                    ui.tags.li("Spatial maps showing geographic patterns"),
                    ui.tags.li("Interactive controls for data selection"),
                    ui.tags.li("Support for multiple climate variables, models, and scenarios"),
                ]),
                class_="container"
            )
        ),
        ui.nav_panel("📚 Glossary",
            ui.div(
                ui.h3("Climate Data Glossary"),
                ui.h4("Data Sources"),
                ui.p("Cal-Adapt: California's climate planning platform, providing climate change research and data."),
                ui.p("NetCDF: Network Common Data Form, a set of interfaces for array-oriented data."),
                ui.h4("Climate Models"),
                ui.p("CMIP6 Models: Climate Model Intercomparison Project Phase 6 models (e.g., GFDL-ESM4, TaiESM1, etc.)"),
                ui.h4("Scenarios"),
                ui.p("SSP: Shared Socioeconomic Pathway. SSP245 (intermediate), SSP370 (medium-high), SSP585 (high emissions)."),
                ui.h4("Climate Variables"),
                ui.tags.ul([
                    ui.tags.li("tasmin / tasmax: Daily minimum/maximum near-surface air temperature."),
                    ui.tags.li("pr: Precipitation."),
                    ui.tags.li("hursmax / hursmin: Maximum/Minimum relative humidity."),
                    ui.tags.li("huss: Specific humidity."),
                    ui.tags.li("rsds: Surface Downwelling Shortwave Radiation."),
                    ui.tags.li("wspeed: Wind Speed."),
                ]),
                ui.h4("Analysis Methods"),
                ui.p("Mean Absolute Difference (MAD): A statistical measure comparing the differences between two datasets, commonly used to assess model agreement."),
                class_="container"
            )
        ),
        ui.nav_panel("ℹ️ About",
            ui.div(
                ui.h3("About This Dashboard"),
                ui.h4("Purpose"),
                ui.p("This dashboard provides interactive access to climate data for the Santa Barbara region, allowing users to explore climate projections, compare different models and scenarios, and analyze spatial and temporal patterns."),
                ui.h4("Data Coverage"),
                ui.p("The dashboard includes climate projections from multiple CMIP6 models under different SSP scenarios, covering various climate variables such as temperature, precipitation, humidity, and more."),
                ui.h4("Technical Details"),
                ui.tags.ul([
                    ui.tags.li("Built with Python Shiny framework"),
                    ui.tags.li("Data visualization using Plotly"),
                    ui.tags.li("Climate data from Cal-Adapt"),
                    ui.tags.li("Interactive maps and time series plots"),
                ]),
                ui.h4("Contact"),
                ui.p("For questions or feedback about this dashboard, please contact the development team."),
                class_="container"
            )
        ),
        ui.nav_panel("📍 Gridbox Map",
            ui.div(
                # FIXED: Layout optimized for better space utilization with scrollable container
                ui.div(
                    ui.div(
                        ui.output_ui("progressive_dropdown_ui"),
                        class_="col-12 col-xl-2 col-lg-3"
                    ),
                    ui.div(
                        ui.output_ui("map_and_data_output"),
                        class_="col-12 col-xl-10 col-lg-9",
                        style="overflow-y: auto; max-height: 90vh;"
                    ),
                    class_="row"
                ),
                class_="container-fluid px-3",
                style="max-width: 100%;"
            )
        ),
        # ui.nav_panel("🗺️ Variable Maps",
        #     ui.div(
        #         ui.h3("Climate Variable Spatial Maps"),
        #         ui.p("Explore spatial distributions of climate variables across the Santa Barbara region. Select different variables, models, and scenarios to see how climate patterns vary geographically.", class_="text-muted"),
        #         ui.div(
        #             ui.div(
        #                 ui.h4("Map Configuration", class_="text-center"),
        #                 ui.p("Configure the map visualization below.", class_="text-center text-muted"),
        #                 ui.output_ui("variable_map_controls"),
        #                 class_="col-lg-3"
        #             ),
        #             ui.div(
        #                 output_widget("variable_spatial_map"),
        #                 class_="col-lg-9"
        #             ),
        #             class_="row"
        #         ),
        #         class_="container"
        #     )
        # )
    )
)
def create_empty_map(message="No data available"):
    """Create an empty map with a message."""
    fig = go.Figure()
    fig.add_annotation(
        text=message,
        x=0.5, y=0.5,
        showarrow=False,
        font=dict(size=16, color="gray")
    )
    fig.update_layout(
        mapbox=dict(
            style="open-street-map",
            center=dict(lat=34.4, lon=-119.7),
            zoom=8
        ),
        margin=dict(l=0, r=0, t=0, b=0),
        height=400
    )
    return fig
# --- Server Logic ---
def server(input, output, session: Session):
    # Debug: Print all available inputs
    print("DEBUG: Available inputs:", [attr for attr in dir(input) if not attr.startswith('_')])
    
    # reactive_netcdf_filepaths: Stores (variable, model, ssp) -> file_path
    reactive_netcdf_filepaths = reactive.Value({})
    
    # _cached_datasets: Stores (variable, model, ssp) -> xr.Dataset
    _cached_datasets = reactive.Value({})

    reactive_grid_geojson = reactive.Value(None)
    reactive_grid_feature_ids = reactive.Value([])
    # Per-file observed points (prec, tmax, tmin)
    obs_points_prec = reactive.Value([])  # list of (lat, lon)
    obs_points_tmax = reactive.Value([])
    obs_points_tmin = reactive.Value([])
    # Store discovered file paths for fallback rendering
    obs_prec_path = reactive.Value(None)
    obs_tmax_path = reactive.Value(None)
    obs_tmin_path = reactive.Value(None)

    # Reactive value for CSV data
    csv_data = reactive.Value(None)

    # Reactive value for selected grid ID (from map click or dropdown), now holds "lat_idx,lon_idx"
    selected_grid_id = reactive.Value(None)
    
    # Explicitly clear any previous selection on startup
    selected_grid_id.set(None)
    print("DEBUG: Cleared grid selection on session start")
    
    # Note: Grid selection clearing is handled in the multi-plot mode toggle handler

    # Reactive values for NetCDF plot selection
    selected_netcdf_variable = reactive.Value(None)
    selected_netcdf_model = reactive.Value(None)
    selected_netcdf_frequency = reactive.Value(None)  # NEW: Add frequency selection
    selected_netcdf_ssp = reactive.Value(None)
    
    # Reactive trigger for plot updates
    plot_update_trigger = reactive.Value(0)
    
    # Reactive trigger for startup
    startup_trigger = reactive.Value(0)
    
    # Reactive values for progressive dropdown system
    dropdown_step_1_completed = reactive.Value(False)
    dropdown_step_2_completed = reactive.Value(False)
    dropdown_step_3_completed = reactive.Value(False)
    
    # Reactive flag to track when variable filtering is complete for a specific grid
    variable_filtering_complete = reactive.Value(False)
    filtered_grid_id = reactive.Value(None)
    dropdown_step_4_completed = reactive.Value(False)
    dropdown_step_5_completed = reactive.Value(False)
    dropdown_step_6_completed = reactive.Value(False)
    
    # Reactive values for multi-plot mode
    multi_plot_step_1_completed = reactive.Value(False)
    multi_plot_step_2_completed = reactive.Value(False)
    multi_plot_step_3_completed = reactive.Value(False)
    multi_plot_step_4_completed = reactive.Value(False)
    multi_plot_step_5_completed = reactive.Value(False)
    multi_plot_step_6_completed = reactive.Value(False)
    
    # Reactive flags to track when variable filtering is complete for multi-plot mode
    multi_plot_variable_filtering_complete = reactive.Value(False)
    multi_plot_filtered_grid_id = reactive.Value(None)
    
    # Reactive values for multi-plot data
    multi_plot_grid_id = reactive.Value(None)
    multi_plot_variable = reactive.Value(None)
    multi_plot_model = reactive.Value(None)
    multi_plot_frequency = reactive.Value(None)  # NEW: Add frequency for multi-plot
    multi_plot_ssp = reactive.Value(None)
    # Reactive value to control multi-plot mode checkbox
    multi_plot_mode_enabled = reactive.Value(False)
    multi_plot_action = reactive.Value(None)  # 'add_to_existing' or 'new_plot'
    
    # System for unlimited multi-plots
    multi_plots_list = reactive.Value([])  # List of plot configurations
    current_multi_plot_id = reactive.Value(0)  # Counter for unique plot IDs
    multi_plot_time_ranges = reactive.Value({})  # Store time ranges for each plot ID
    multi_plot_update_trigger = reactive.Value(0)  # Trigger for multi-plot updates
    multi_plot_status_message = reactive.Value("")  # Status messages for user feedback
    
    # Additional traces for main plot when user chooses to combine on main
    main_plot_additional_traces = reactive.Value([])
    main_plot_update_trigger = reactive.Value(0)
    
    # FIXED: Add persistent storage for main plot additional traces
    # This prevents traces from being lost when plots are regenerated
    main_plot_persistent_traces = reactive.Value([])
    
    # FIXED: Add flag to prevent clearing traces during plot updates
    preserve_additional_traces = reactive.Value(True)
    
    # Initialize reactive values safely to prevent undefined errors
    try:
        # Ensure all reactive values have safe initial states
        if main_plot_additional_traces.get() is None:
            main_plot_additional_traces.set([])
        if main_plot_persistent_traces.get() is None:
            main_plot_persistent_traces.set([])
        if main_plot_update_trigger.get() is None:
            main_plot_update_trigger.set(0)
    except Exception as init_error:
        print(f"DEBUG: Error initializing reactive values: {init_error}")
        # Force set safe defaults
        try:
            main_plot_additional_traces.set([])
            main_plot_persistent_traces.set([])
            main_plot_update_trigger.set(0)
        except:
            pass
    
    # Execute startup data loading immediately when server function is called
    print("STARTUP: Loading data synchronously...")
    print("DEBUG: Startup data loading effect called")
    
    # 1. Build the complete filepath map and collect all unique options
    filepaths_map = {}
    variables = set()
    models = set()
    scenarios = set()
    
    files_analyzed = 0
    print("Checking file paths:")
    for filename in NETCDF_FILE_LIST:
        exists = os.path.exists(filename)
        print(f"- {filename}: {'Exists' if exists else 'MISSING'}")
        if not exists:
            continue
            
        var, model, ssp = parse_filename(filename)
        if var and model and ssp:
            filepaths_map[(var, model, ssp)] = filename
            variables.add(var)
            models.add(model)
            scenarios.add(ssp)
            files_analyzed += 1
    
    print(f"Analyzed {files_analyzed} NetCDF files")
    print(f"Found variables: {sorted(variables)}")
    print(f"Found models: {sorted(models)}")
    print(f"Found scenarios: {sorted(scenarios)}")
    
    # Set reactive values
    reactive_netcdf_filepaths.set(filepaths_map)
    print(f"STARTUP: Set reactive_netcdf_filepaths with {len(filepaths_map)} entries")
    
    def get_trace_data_for_mad(trace_config, time_range=None):
        """
        Extract trace data for MAD calculation, respecting current time range settings.
        This function applies the same data processing as the plotting functions.
        
        Args:
            trace_config: Dictionary with trace configuration (variable, model, ssp, frequency, lat_idx, lon_idx)
            time_range: Optional tuple of (start_year, end_year) to filter data
        
        Returns:
            DataFrame with 'Date' and variable columns, or None if extraction fails
        """
        try:
            # Create cache key
            cache_key = (
                trace_config.get('variable'),
                trace_config.get('model'),
                trace_config.get('ssp'),
                trace_config.get('frequency'),
                trace_config.get('lat_idx'),
                trace_config.get('lon_idx'),
                time_range
            )
            
            # Check cache first
            if cache_key in _time_series_cache:
                print(f"DEBUG: MAD - Using cached data for {cache_key[0]}")
                return _time_series_cache[cache_key].copy()
            variable = trace_config['variable']
            model = trace_config['model']
            ssp = trace_config['ssp']
            frequency = trace_config['frequency']
            lat_idx = trace_config['lat_idx']
            lon_idx = trace_config['lon_idx']
            
            # Get file path
            filepaths_map = reactive_netcdf_filepaths.get()
            if not filepaths_map:
                return None
            
            # Handle observed data
            if variable.startswith('obs_'):
                ds = load_observed_dataset(variable)
                if ds is None:
                    return None
                
                # For observed data, use coordinate mapping like plotting functions
                # Get the target coordinates for the grid indices
                try:
                    trace_grid_id = f"{lat_idx},{lon_idx}"
                    target_lat, target_lon = _get_coordinates_from_grid_id(trace_grid_id, filepaths_map)
                    if target_lat is not None and target_lon is not None:
                        # Find the most encompassing observed grid cell
                        obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(target_lat, target_lon, ds)
                        if obs_lat_idx is not None and obs_lon_idx is not None:
                            # Update lat/lon indices to use mapped coordinates
                            lat_idx, lon_idx = obs_lat_idx, obs_lon_idx
                            print(f"DEBUG: MAD - Mapped observed grid coordinates ({lat_idx}, {lon_idx}) for variable {variable}")
                        else:
                            print(f"DEBUG: MAD - Using fallback observed grid coordinates for {variable}")
                    else:
                        print(f"DEBUG: MAD - Could not get target coordinates for grid {trace_grid_id}")
                except Exception as e:
                    print(f"DEBUG: MAD - Error mapping observed coordinates: {e}")
                    # Continue with original indices
            else:
                # Handle model data
                file_key = (variable, model, ssp)
                if file_key not in filepaths_map:
                    return None
                
                file_path = filepaths_map[file_key]
                try:
                    with xr.open_dataset(file_path, engine="netcdf4") as temp_ds:
                        ds = temp_ds.load()
                except Exception as e:
                    print(f"DEBUG: Error loading dataset for MAD: {e}")
                    return None
            
            # Extract time series
            df_raw = extract_timeseries(ds, lat_idx, lon_idx, variable)
            if df_raw is None or df_raw.empty:
                return None
            
            df = df_raw.copy()
            
            # Get the actual variable name used in the dataset for unit conversion
            actual_var_name = None
            if variable.startswith('obs_'):
                # For observed data, find the actual data variable name
                data_vars = [var for var in ds.data_vars.keys() if var != 'spatial_ref']
                if data_vars:
                    actual_var_name = data_vars[0]
                else:
                    actual_var_name = variable
            else:
                actual_var_name = variable
            
            # Apply unit conversions (same as plotting functions)
            if variable in ["tasmin", "tasmax"]:
                original_units = ds[actual_var_name].attrs.get('units', '').lower()
                if original_units in ['k', 'kelvin']:
                    df[variable] = (df[variable] - 273.15) * 9/5 + 32
                    print(f"DEBUG: MAD - Converted projected temperature {variable} from Kelvin to Fahrenheit")
            elif variable in ["obs_tmin", "obs_tmax"]:
                # Observed temperature conversion - Livneh data is always in Celsius
                print(f"DEBUG: MAD - Converting observed temperature {variable} from Celsius to Fahrenheit")
                df[variable] = (df[variable] * 9/5) + 32
            elif variable == "pr":
                original_units = ds[actual_var_name].attrs.get('units', '').lower()
                if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                    df[variable] = df[variable] * 86400 / 25.4
            elif variable == "obs_pr":
                # Observed precipitation conversion - Livneh data is in mm
                df[variable] = df[variable] / 25.4  # Convert mm to inches
            
            # Apply frequency conversion (same as plotting functions)
            if frequency == 'daily':
                # For daily data, keep it as daily for accurate MAD calculations
                df_freq = df.copy()
                df_freq["Year"] = df_freq["Date"].dt.year
            elif frequency == 'monthly':
                # For monthly data, resample to monthly
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            elif frequency == 'annual':
                # For annual data, resample to annual
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            else:
                # Default to annual if frequency not specified or invalid
                if variable in ["pr", "obs_pr"]:
                    df_freq = df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                else:
                    df_freq = df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                df_freq["Year"] = df_freq["Date"].dt.year
            
            # Apply time range filtering if provided
            if time_range and len(time_range) == 2:
                try:
                    start_year, end_year = time_range
                    if frequency == 'daily':
                        # For daily data, filter by date range
                        start_date = pd.Timestamp(f"{start_year}-01-01")
                        end_date = pd.Timestamp(f"{end_year}-12-31")
                        df_freq = df_freq[(df_freq["Date"] >= start_date) & (df_freq["Date"] <= end_date)]
                    else:
                        # For monthly and annual data, filter by year
                        df_freq = df_freq[(df_freq['Year'] >= start_year) & (df_freq['Year'] <= end_year)]
                except Exception as e:
                    print(f"DEBUG: MAD - Error applying time range filter: {e}")
                    # Continue without filtering if there's an error
            
            # Store in cache before returning
            _time_series_cache[cache_key] = df_freq.copy()
            
            return df_freq
            
        except Exception as e:
            print(f"DEBUG: Error in get_trace_data_for_mad: {e}")
            return None
    
    # Load grid cells - find common cells across all files
    print("Computing common grid cells across all NetCDF files...")
    try:
        if filepaths_map:
            filepaths_list = list(filepaths_map.values())
            print(f"Computing common valid grid cells across {len(filepaths_list)} files...")
            
            # Use representative files since all 398 files have identical grid structure
            sample_files = filepaths_list[:5]  # Use first 5 files - all files have identical grids
            print(f"Using {len(sample_files)} representative files for grid cell computation (all files have identical grid structure)...")
            
            import signal
            import time
            
            def timeout_handler(signum, frame):
                raise TimeoutError("Grid cell computation timed out")
            
            # Set timeout of 10 seconds for processing 5 representative files
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(10)
            
            try:
                common_cells = find_common_valid_gridcells(sample_files)
            except TimeoutError:
                print("WARNING: Grid cell computation timed out, using fallback")
                common_cells = None
            except Exception as e:
                print(f"WARNING: Grid cell computation failed: {e}, using fallback")
                common_cells = None
            finally:
                signal.alarm(0)  # Cancel the alarm
            
            if common_cells:
                grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
                print(f"Found {len(common_cells)} common valid grid cells")
                
                # Set reactive values
                reactive_grid_feature_ids.set(grid_feature_ids)
                print(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
                    
                # Create GeoJSON for map using the first file for coordinate reference
                sample_file = next(iter(filepaths_map.values()))
                with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                    geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                    reactive_grid_geojson.set(geojson_data)
                    print("STARTUP: Set reactive_grid_geojson")
                    
                    # Don't auto-select first grid cell - let user choose explicitly
                    # first_grid_id = grid_feature_ids[0]
                    # selected_grid_id.set(first_grid_id)
                    # print(f"STARTUP: Auto-selected first grid cell: {first_grid_id}")
                    print("STARTUP: Grid cells loaded - user must select explicitly")
            else:
                print("WARNING: No common grid cells found, using fallback")
                # Fallback: use all cells from first file within Santa Barbara bounds
                sample_file = next(iter(filepaths_map.values()))
                with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                    fallback_cells = get_grid_in_bounds(ds)
                    grid_feature_ids = [f"{lat},{lon}" for lat, lon in fallback_cells]
                    reactive_grid_feature_ids.set(grid_feature_ids)
                    print(f"STARTUP: Using fallback with {len(grid_feature_ids)} grid cells within Santa Barbara bounds")
                    
                    geojson_data, feature_ids = create_grid_geojson(fallback_cells, ds)
                    reactive_grid_geojson.set(geojson_data)
                    print("STARTUP: Set reactive_grid_geojson (fallback)")
                    
                    # Don't auto-select first grid cell - let user choose explicitly
                    # first_grid_id = grid_feature_ids[0]
                    # selected_grid_id.set(first_grid_id)
                    # print(f"STARTUP: Auto-selected first grid cell (fallback): {first_grid_id}")
                    print("STARTUP: Grid cells loaded (fallback) - user must select explicitly")
        else:
            print("No filepaths available for grid computation!")
    except Exception as e:
        print(f"Error computing common grid cells: {e}")
        print(f"Full traceback: {traceback.format_exc()}")
        # Create fallback grid cells
        fallback_cells = [(i, j) for i in range(10, 30) for j in range(10, 30)]
        grid_feature_ids = [f"{lat},{lon}" for lat, lon in fallback_cells]
        reactive_grid_feature_ids.set(grid_feature_ids)
        print(f"STARTUP: Set fallback grid_feature_ids with {len(grid_feature_ids)} entries")
    
    # Build observations (Livneh) common cells
    try:
        # Try both original temporal filenames and converted .nc names
        obs_prec = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_prec_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_prec:
            obs_prec = _discover_obs_path(["livneh_day_Prec_clip_15Oct2014.1950-2013.nc"])
        obs_tmax = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_tmax_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_tmax:
            obs_tmax = _discover_obs_path(["livneh_day_Tmax_clip_15Oct2014.1950-2013.nc"])
        obs_tmin = next((f for f in NETCDF_FILE_LIST if os.path.basename(f).lower().endswith("livneh_day_tmin_clip_15oct2014.1950-2013_temporal.nc")), None)
        if not obs_tmin:
            obs_tmin = _discover_obs_path(["livneh_day_Tmin_clip_15Oct2014.1950-2013.nc"])
        if not obs_prec:
            obs_prec = _discover_obs_path(["livneh_day_Prec_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_prec
        if not obs_tmax:
            obs_tmax = _discover_obs_path(["livneh_day_Tmax_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_tmax
        if not obs_tmin:
            obs_tmin = _discover_obs_path(["livneh_day_Tmin_clip_15Oct2014.1950-2013_temporal.nc"]) or obs_tmin
        print(f"OBS STARTUP: Discovered files -> prec={obs_prec} tmax={obs_tmax} tmin={obs_tmin}")
        obs_files = [p for p in [obs_prec, obs_tmax, obs_tmin] if p and os.path.exists(p)]
        # Save discovered paths for later use in render fallbacks
        if obs_prec and os.path.exists(obs_prec):
            obs_prec_path.set(obs_prec)
        if obs_tmax and os.path.exists(obs_tmax):
            obs_tmax_path.set(obs_tmax)
        if obs_tmin and os.path.exists(obs_tmin):
            obs_tmin_path.set(obs_tmin)
        # Remove intersection logic; we no longer compute common cells
            # Also compute per-file valid points (no intersection) for display
            for f, holder in [
                (obs_prec, obs_points_prec),
                (obs_tmax, obs_points_tmax),
                (obs_tmin, obs_points_tmin),
            ]:
                try:
                    ds = _open_dataset_fallback(f)
                    if ds is None:
                        holder.set([])
                        continue
                    var = list(ds.data_vars.keys())[0]
                    lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
                    if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
                        holder.set([])
                        ds.close(); continue
                    dims = ds[var].dims
                    if 'time' not in dims or lat_dim not in dims or lon_dim not in dims:
                        holder.set([])
                        ds.close(); continue
                    reduce_dims = tuple(d for d in dims if d not in (lat_dim, lon_dim))
                    mask = ~ds[var].isnull().all(dim=reduce_dims)
                    n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
                    pts = []
                    for i in range(n_i):
                        for j in range(n_j):
                            if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                    # Fallback: if no valid-mask points were detected, still plot all coordinate points
                    if not pts and lat2d is not None and lon2d is not None:
                        try:
                            total_i, total_j = lat2d.shape[0], lat2d.shape[1] if lat2d.ndim==2 else 0
                            if total_j == 0 and hasattr(lon2d, 'shape'):
                                total_j = lon2d.shape[1] if lon2d.ndim==2 else 0
                            if total_i and total_j:
                                for i in range(total_i):
                                    for j in range(total_j):
                                        lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                        pts.append((lat, lon))
                        except Exception:
                            pass
                    holder.set(pts)
                    ds.close()
                except Exception as _e:
                    holder.set([])
        else:
            print("OBS STARTUP: One or more Livneh files missing")

        # Always compute per-file points (regardless of intersection success)
        for f, holder in [
            (obs_prec, obs_points_prec),
            (obs_tmax, obs_points_tmax),
            (obs_tmin, obs_points_tmin),
        ]:
            try:
                if not (f and os.path.exists(f)):
                    holder.set([])
                    continue
                ds = _open_dataset_fallback(f)
                if ds is None:
                    holder.set([])
                    continue
                var = list(ds.data_vars.keys())[0]
                lat2d, lon2d, lat_dim, lon_dim = _extract_spatial_coords(ds)
                if lat2d is None or lon2d is None or lat_dim is None or lon_dim is None:
                    holder.set([])
                    ds.close(); continue
                pts = []
                dims = ds[var].dims
                if 'time' in dims and lat_dim in dims and lon_dim in dims:
                    reduce_dims = tuple(d for d in dims if d not in (lat_dim, lon_dim))
                    mask = ~ds[var].isnull().all(dim=reduce_dims)
                    n_i, n_j = mask.sizes[lat_dim], mask.sizes[lon_dim]
                    for i in range(n_i):
                        for j in range(n_j):
                            if bool(mask.isel({lat_dim: i, lon_dim: j}).item()):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                if not pts and hasattr(lat2d, 'shape') and hasattr(lon2d, 'shape'):
                    try:
                        total_i = lat2d.shape[0]
                        total_j = lat2d.shape[1] if getattr(lat2d, 'ndim', 1) == 2 else (lon2d.shape[1] if getattr(lon2d, 'ndim', 1) == 2 else 0)
                        for i in range(total_i):
                            for j in range(total_j):
                                lat = float(lat2d[i, j]); lon = float(lon2d[i, j])
                                pts.append((lat, lon))
                    except Exception:
                        pass
                holder.set(pts)
                print(f"OBS STARTUP: {os.path.basename(f)} -> points stored: {len(pts)}")
                ds.close()
            except Exception as _e:
                holder.set([])
    except Exception as e:
        print(f"OBS STARTUP: Error computing observed cells: {e}")
    
    print("STARTUP: Data loading complete")
    
    @reactive.Effect
    def _trigger_startup():
        """Trigger startup effect"""
        print("STARTUP: _trigger_startup called")
        startup_trigger.set(1)
    @reactive.Effect
    @reactive.event(startup_trigger)
    def _load_data_on_startup():
        """Load data when the server starts"""
        print("STARTUP: _load_data_on_startup called")
        
        # 1. Build the complete filepath map and collect all unique options
        filepaths_map = {}
        variables = set()
        models = set()
        scenarios = set()
        
        files_analyzed = 0
        print("Checking file paths (optimized for faster startup):")
        # Only check first 50 files for faster startup - all files follow same pattern
        files_to_check = NETCDF_FILE_LIST[:50] if len(NETCDF_FILE_LIST) > 50 else NETCDF_FILE_LIST
        for filename in files_to_check:
            exists = os.path.exists(filename)
            if files_analyzed < 10:  # Only print first 10 for faster startup
                print(f"- {filename}: {'Exists' if exists else 'MISSING'}")
            if not exists:
                continue
                
            var, model, ssp = parse_filename(filename)
            if var and model and ssp:
                filepaths_map[(var, model, ssp)] = filename
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
                files_analyzed += 1
        
        # Fill in remaining files without checking existence (for faster startup)
        if len(NETCDF_FILE_LIST) > 50:
            print(f"Fast-scanning remaining {len(NETCDF_FILE_LIST) - 50} files...")
            for filename in NETCDF_FILE_LIST[50:]:
                var, model, ssp = parse_filename(filename)
                if var and model and ssp:
                    filepaths_map[(var, model, ssp)] = filename
                    variables.add(var)
                    models.add(model)
                    scenarios.add(ssp)
                    files_analyzed += 1
        
        print(f"Analyzed {files_analyzed} NetCDF files")
        print(f"Found variables: {sorted(variables)}")
        print(f"Found models: {sorted(models)}")
        print(f"Found scenarios: {sorted(scenarios)}")
        
        # Set reactive values
        reactive_netcdf_filepaths.set(filepaths_map)
        print(f"STARTUP: Set reactive_netcdf_filepaths with {len(filepaths_map)} entries")
        
        # Use pre-computed map data for INSTANT loading
        map_data = _precompute_map_data()
        if map_data:
            # Set reactive values from pre-computed data - INSTANT LOADING
            reactive_grid_feature_ids.set(map_data['grid_feature_ids'])
            reactive_grid_geojson.set(map_data['geojson_data'])
            print(f"STARTUP: ✅ INSTANT MAP LOADING - Set reactive values from pre-computed data - {len(map_data['grid_feature_ids'])} grid cells")
        else:
            print("WARNING: Could not pre-compute map data, falling back to standard loading")
            # Fallback to original method if pre-computation fails
        common_cells, cached_hash = load_grid_cells_cache()
        current_hash = compute_file_list_hash()
        
        if common_cells is None or current_hash != cached_hash:
            print("Computing common grid cells...")
            filepaths_list = list(filepaths_map.values())
            print(f"Computing common valid grid cells across {len(filepaths_list)} files...")
            
            try:
                # Since all files have the same grid structure, use just a few representative files
                sample_files = filepaths_list[:3]  # Use first 3 files - all files have identical grids
                print(f"Using {len(sample_files)} representative files for grid cell computation (all files have identical grid structure)...")
                common_cells = find_common_valid_gridcells(sample_files)
                
                if common_cells:
                    grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
                    print(f"Found {len(common_cells)} common valid grid cells")
                    
                    # Save to cache
                    save_grid_cells_cache(common_cells, current_hash)
                    
                    # Set reactive values
                    reactive_grid_feature_ids.set(grid_feature_ids)
                    print(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
                    
                    # Create GeoJSON for map
                    if filepaths_map:
                        sample_file = next(iter(filepaths_map.values()))
                        try:
                            with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                                geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                                reactive_grid_geojson.set(geojson_data)
                                print("STARTUP: Set reactive_grid_geojson")
                        except Exception as e:
                            print(f"Error creating GeoJSON: {e}")
                else:
                    print("No common valid grid cells found")
            except Exception as e:
                print(f"Error computing common grid cells: {e}")
        else:
            print("Using cached grid cells")
            grid_feature_ids = [f"{lat},{lon}" for lat, lon in common_cells]
            reactive_grid_feature_ids.set(grid_feature_ids)
            print(f"STARTUP: Set reactive_grid_feature_ids with {len(grid_feature_ids)} entries")
            
            # Create GeoJSON for map
            if filepaths_map:
                sample_file = next(iter(filepaths_map.values()))
                try:
                    with xr.open_dataset(sample_file, engine="netcdf4", decode_times=False) as ds:
                        geojson_data, feature_ids = create_grid_geojson(common_cells, ds)
                        reactive_grid_geojson.set(geojson_data)
                        print("STARTUP: Set reactive_grid_geojson")
                except Exception as e:
                    print(f"Error creating GeoJSON from cached cells: {e}")
        
        print("STARTUP: Data loading complete")
    


    # Gridbox selection is now handled by map clicks, not dropdowns

    @reactive.Effect
    @reactive.event(selected_grid_id)
    def _handle_variable_filtering():
        """Handle variable filtering asynchronously when grid changes - OPTIMIZED"""
        try:
            current_grid_id = selected_grid_id.get()
            if not current_grid_id:
                return
                
            print(f"DEBUG: _handle_variable_filtering - Processing grid {current_grid_id}")
            
            # OPTIMIZED: Use isolation to read cached values without triggering reactivity
            with reactive.isolate():
                is_filtering_complete = variable_filtering_complete.get()
                cached_grid_id = filtered_grid_id.get()
            
            # Skip if already cached for this grid
            if is_filtering_complete and cached_grid_id == current_grid_id:
                print(f"DEBUG: _handle_variable_filtering - Already cached for grid {current_grid_id}")
                return
            
            # OPTIMIZED: Read filepaths once with isolation to avoid triggering during filtering
            with reactive.isolate():
                filepaths_map = reactive_netcdf_filepaths.get()
            
            if not filepaths_map:
                return
                
            # Get coordinates for the grid ID
            lat, lon = _get_coordinates_from_grid_id(current_grid_id, filepaths_map)
            if lat is None or lon is None:
                print(f"DEBUG: _handle_variable_filtering - Could not convert grid {current_grid_id} to coordinates")
                return
                
            print(f"DEBUG: _handle_variable_filtering - Converted grid {current_grid_id} to coordinates ({lat:.4f}, {lon:.4f})")
            
            # Check observational data availability
            observed_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
            valid_obs_vars = []
            
            for obs_var in observed_variables:
                if _check_observational_data_availability(current_grid_id, obs_var, lat, lon):
                    valid_obs_vars.append(obs_var)
                    print(f"DEBUG: _handle_variable_filtering - Added {obs_var} to available variables for grid {current_grid_id}")
                else:
                    print(f"DEBUG: _handle_variable_filtering - Excluded {obs_var} from available variables for grid {current_grid_id} - no valid data")
            
            # OPTIMIZED: Batch reactive updates together
            variable_filtering_complete.set(True)
            filtered_grid_id.set(current_grid_id)
            print(f"DEBUG: _handle_variable_filtering - ✅ FILTERING COMPLETE for grid {current_grid_id}")
            
        except Exception as e:
            print(f"DEBUG: Error in _handle_variable_filtering: {e}")

    @reactive.Effect
    @reactive.event(multi_plot_grid_id)
    def _handle_multi_plot_variable_filtering():
        """Handle variable filtering asynchronously when multi-plot grid changes - OPTIMIZED"""
        try:
            current_grid_id = multi_plot_grid_id.get()
            if not current_grid_id:
                return
                
            print(f"DEBUG: _handle_multi_plot_variable_filtering - Processing grid {current_grid_id}")
            
            # OPTIMIZED: Use isolation to read cached values without triggering reactivity
            with reactive.isolate():
                is_filtering_complete = multi_plot_variable_filtering_complete.get()
                cached_grid_id = multi_plot_filtered_grid_id.get()
            
            # Skip if already cached for this grid
            if is_filtering_complete and cached_grid_id == current_grid_id:
                print(f"DEBUG: _handle_multi_plot_variable_filtering - Already cached for grid {current_grid_id}")
                return
            
            # OPTIMIZED: Read filepaths once with isolation to avoid triggering during filtering
            with reactive.isolate():
                filepaths_map = reactive_netcdf_filepaths.get()
            
            if not filepaths_map:
                return
                
            # Get coordinates for the grid ID
            lat, lon = _get_coordinates_from_grid_id(current_grid_id, filepaths_map)
            if lat is None or lon is None:
                print(f"DEBUG: _handle_multi_plot_variable_filtering - Could not convert grid {current_grid_id} to coordinates")
                return
                
            print(f"DEBUG: _handle_multi_plot_variable_filtering - Converted grid {current_grid_id} to coordinates ({lat:.4f}, {lon:.4f})")
            
            # Check observational data availability
            observed_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
            valid_obs_vars = []
            
            for obs_var in observed_variables:
                if _check_observational_data_availability(current_grid_id, obs_var, lat, lon):
                    valid_obs_vars.append(obs_var)
                    print(f"DEBUG: _handle_multi_plot_variable_filtering - Added {obs_var} to available variables for grid {current_grid_id}")
                else:
                    print(f"DEBUG: _handle_multi_plot_variable_filtering - Excluded {obs_var} from available variables for grid {current_grid_id} - no valid data")
            
            # OPTIMIZED: Batch all reactive updates together
            multi_plot_variable_filtering_complete.set(True)
            multi_plot_filtered_grid_id.set(current_grid_id)
            print(f"DEBUG: _handle_multi_plot_variable_filtering - ✅ FILTERING COMPLETE for grid {current_grid_id}")
            
            # OPTIMIZED: Trigger UI update efficiently
            try:
                with reactive.isolate():
                    current_trigger = multi_plot_update_trigger.get()
                multi_plot_update_trigger.set(current_trigger + 1)
                print(f"DEBUG: _handle_multi_plot_variable_filtering - Triggered UI update")
            except Exception as trigger_error:
                print(f"DEBUG: Error triggering UI update: {trigger_error}")
            
        except Exception as e:
            print(f"DEBUG: Error in _handle_multi_plot_variable_filtering: {e}")

    @reactive.Effect
    @reactive.event(input.netcdf_model)
    def _update_selected_model():
        model_selection = input.netcdf_model()
        if model_selection and model_selection != "Select a model...":
            print(f"DEBUG: Model selected: {model_selection}")
            selected_netcdf_model.set(model_selection)
            # FIXED ORDER: Model is now Step 2 in the flow
            dropdown_step_2_completed.set(True)
            print(f"DEBUG: Step 2 completed")
            # Reset subsequent steps when model changes
            dropdown_step_3_completed.set(False)
            dropdown_step_4_completed.set(False)
            dropdown_step_5_completed.set(False)
            # Trigger plot update when model changes
            plot_update_trigger.set(plot_update_trigger.get() + 1)
        else:
            selected_netcdf_model.set("")
            # Reset Step 2 when model cleared
            dropdown_step_2_completed.set(False)
            print(f"DEBUG: Step 2 reset")

    @reactive.Effect
    @reactive.event(input.netcdf_variable)
    def _update_selected_variable():
        try:
            # The dropdown returns the VALUE from choices dict, which is the technical variable name
            selected_value = input.netcdf_variable()
            print(f"DEBUG: _update_selected_variable - Selected value from dropdown: {selected_value}")
            
            if selected_value and selected_value != "" and selected_value != "Select a variable...":
                # The value from the dropdown is already the technical variable name
                # No conversion needed since our choices dict has display names as keys and technical names as values
                technical_var = selected_value
                print(f"DEBUG: _update_selected_variable - Using technical name: {technical_var}")
                
                # Store the technical variable name
                selected_netcdf_variable.set(technical_var)
                # FIXED ORDER: Variable is now Step 3 in the flow
                dropdown_step_3_completed.set(True)
                print(f"DEBUG: _update_selected_variable - Step 3 completed, variable: {technical_var}")
                # Reset subsequent steps when variable changes
                dropdown_step_4_completed.set(False)
                dropdown_step_5_completed.set(False)
                # Trigger plot update when variable changes
                plot_update_trigger.set(plot_update_trigger.get() + 1)
            else:
                selected_netcdf_variable.set("")
                dropdown_step_3_completed.set(False)
                print(f"DEBUG: _update_selected_variable - Step 3 reset")
        except Exception as e:
            print(f"DEBUG: _update_selected_variable - Error: {e}")
            dropdown_step_3_completed.set(False)

    @reactive.Effect
    @reactive.event(input.netcdf_frequency)
    def _update_selected_frequency():
        frequency_selection = input.netcdf_frequency()
        if frequency_selection and frequency_selection != "Select frequency...":
            print(f"DEBUG: Frequency selected: {frequency_selection}")
            selected_netcdf_frequency.set(frequency_selection)
            dropdown_step_4_completed.set(True)
            print(f"DEBUG: Step 4 completed")
            # If observational variable, skip scenario and complete steps 5 and 6; also set slider years
            try:
                sel_var = selected_netcdf_variable.get()
            except Exception:
                sel_var = None
            if sel_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                dropdown_step_5_completed.set(True)
                dropdown_step_6_completed.set(True)
                # Observational range 1950–2013
                ui.update_slider("time_series_range_main", min=1950, max=2013, value=(1950, 2013))
            else:
                # Reset subsequent steps when frequency changes
                dropdown_step_5_completed.set(False)
            # Trigger plot update when frequency changes
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            # Ensure main slider reflects observed range if an observed variable is selected
            try:
                sel_var = selected_netcdf_variable.get()
                if sel_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                    ui.update_slider("time_series_range_main", min=1950, max=2013, value=(1950, 2013))
            except Exception:
                pass

    @reactive.Effect
    @reactive.event(input.netcdf_ssp)
    def _update_selected_scenario():
        ssp_selection = input.netcdf_ssp()
        if ssp_selection and ssp_selection != "Select scenario...":
            print(f"DEBUG: SSP selected: {ssp_selection}")
            selected_netcdf_ssp.set(ssp_selection)
            dropdown_step_5_completed.set(True)
            # Automatically complete step 6 when step 5 is done
            dropdown_step_6_completed.set(True)
            print(f"DEBUG: Steps 4 and 5 completed")
            
            # Don't auto-select grid cell - let user choose explicitly
            current_grid_id = selected_grid_id.get()
            print(f"DEBUG: Current grid ID: {current_grid_id}")
            if current_grid_id is None:
                print(f"DEBUG: No grid selected - user must select explicitly")
            else:
                print(f"DEBUG: Grid ID already selected: {current_grid_id}")
            
            # Update time series range slider based on SSP
            ssp_value = input.netcdf_ssp()
            if ssp_value == "historical":
                ui.update_slider("time_series_range_main", min=1950, max=2014, value=(1950, 2014))
            else:
                ui.update_slider("time_series_range_main", min=2015, max=2100, value=(2015, 2100))
            print(f"DEBUG: Updated time series range slider for SSP: {ssp_value}")
            
            # Trigger plot update
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            print(f"DEBUG: Plot update trigger incremented")

    # Debounced time series range update
    _last_time_series_update = reactive.Value(time.time())
    
    @reactive.Effect
    @reactive.event(input.time_series_range)
    def _update_time_series_range():
        if input.time_series_range():
            # Update the last update time
            _last_time_series_update.set(time.time())
    
    @reactive.Effect
    @reactive.event(_last_time_series_update)
    def _debounced_time_series_update():
        # OPTIMIZED: Reduced wait time from 0.05 to 0.01 seconds for faster responsiveness
        time.sleep(0.01)
        # Check if this is still the most recent update
        if time.time() - _last_time_series_update.get() >= 0.01:
            plot_update_trigger.set(plot_update_trigger.get() + 1)
    
    # Cache for processed time series data to avoid reprocessing
    _time_series_cache = {}
    
    # Note: Slider and checkbox reactivity is handled automatically by Shiny
    # When the plot function reads slider/checkbox values, it establishes reactive dependencies
    # No separate monitoring needed - this prevents cascading updates and errors





    # Trendline checkboxes are handled directly in the plot rendering logic
    # The plot function reads checkbox values when it renders, establishing natural reactivity
    # No separate monitoring needed - Shiny's reactive system handles this automatically

    @reactive.Effect
    @reactive.event(input.reset_selection)
    def _reset_progressive_selection():
        # Reset all step completion flags
        dropdown_step_1_completed.set(False)
        dropdown_step_2_completed.set(False)
        dropdown_step_3_completed.set(False)
        dropdown_step_4_completed.set(False)
        dropdown_step_5_completed.set(False)
        dropdown_step_6_completed.set(False)
        # Clear selected values
        print(f"DEBUG: Clearing selected_grid_id (was: {selected_grid_id.get()})")
        selected_grid_id.set(None)
        selected_netcdf_variable.set(None)
        selected_netcdf_model.set(None)
        selected_netcdf_frequency.set(None)
        selected_netcdf_ssp.set(None)
        
        # Also reset multi-plot selections when starting new selection
        multi_plot_step_1_completed.set(False)
        multi_plot_step_2_completed.set(False)
        multi_plot_step_3_completed.set(False)
        multi_plot_step_4_completed.set(False)
        multi_plot_step_5_completed.set(False)
        multi_plot_step_6_completed.set(False)
        # Clear multi-plot selected values
        multi_plot_grid_id.set(None)
        multi_plot_variable.set(None)
        multi_plot_model.set(None)
        multi_plot_frequency.set(None)
        multi_plot_ssp.set(None)
        multi_plot_action.set(None)
        
        # Disable multi-plot mode to ensure we're in Start New Selection mode
        multi_plot_mode_enabled.set(False)
        
        # Force UI update by triggering updates
        try:
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
            print("DEBUG: Triggered UI updates to reset dropdown selections to blank")
        except Exception as e:
            print(f"DEBUG: Warning - Could not trigger UI updates: {e}")
        
        # Clear additional traces when resetting selection
        main_plot_additional_traces.set([])
        main_plot_persistent_traces.set([])
        
        # Clear multi-plot list
        multi_plots_list.set([])
        
        # Reset multi-plot update trigger
        multi_plot_update_trigger.set(0)
        
        # Reset plot update trigger to force re-render
        plot_update_trigger.set(plot_update_trigger.get() + 1)
        
        print("DEBUG: All selections reset (main and multi-plot), additional traces cleared")
    


    @reactive.Effect
    @reactive.event(plot_update_trigger)
    def _trigger_plot_update():
        """Trigger plot re-render when data is loaded"""
        print(f"DEBUG: _trigger_plot_update called - trigger value: {plot_update_trigger.get()}")
        pass

    @reactive.Effect
    @reactive.event(dropdown_step_5_completed)
    def _debug_step_5_completion():
        """Debug effect to test if step 5 completion triggers"""
        print(f"DEBUG: _debug_step_5_completion called - step 5 completed: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(plot_update_trigger)
    def _debug_plot_trigger():
        """Debug effect to test if plot_update_trigger is working"""
        print(f"DEBUG: _debug_plot_trigger called - trigger value: {plot_update_trigger.get()}")
        print(f"DEBUG: Current step completion status:")
        print(f"DEBUG:   Step 1: {dropdown_step_1_completed.get()}")
        print(f"DEBUG:   Step 2: {dropdown_step_2_completed.get()}")
        print(f"DEBUG:   Step 3: {dropdown_step_3_completed.get()}")
        print(f"DEBUG:   Step 4: {dropdown_step_4_completed.get()}")
        print(f"DEBUG:   Step 5: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(dropdown_step_1_completed)
    def _trigger_ui_update_on_step1_completion():
        """Trigger UI update when step 1 is completed"""
        print(f"DEBUG: Step 1 completion changed to: {dropdown_step_1_completed.get()}")
        if dropdown_step_1_completed.get():
            print(f"DEBUG: Step 1 completed - triggering UI update")
            try:
                plot_update_trigger.set(plot_update_trigger.get() + 1)
            except Exception as e:
                print(f"DEBUG: Error triggering UI update: {e}")
        
    @reactive.Effect
    @reactive.event(dropdown_step_5_completed)
    def _debug_step5_trigger():
        """Debug effect to test if dropdown_step_5_completed triggers"""
        print(f"DEBUG: _debug_step5_trigger called - step5 value: {dropdown_step_5_completed.get()}")

    @reactive.Effect
    @reactive.event(input.netcdf_model, input.netcdf_variable, input.netcdf_ssp)
    def _debug_input_changes():
        """Debug effect to test if input changes are being detected"""
        print(f"DEBUG: _debug_input_changes called")
        print(f"DEBUG: netcdf_model: {input.netcdf_model()}")
        print(f"DEBUG: netcdf_variable: {input.netcdf_variable()}")
        print(f"DEBUG: netcdf_ssp: {input.netcdf_ssp()}")



    # Multi-plot gridbox selection is now handled by map clicks, not dropdowns

    @reactive.Effect
    @reactive.event(input.multi_plot_model)
    def _update_multi_plot_selected_model():
        try:
            model_value = input.multi_plot_model()
            print(f"DEBUG: Multi-plot model changed to: {model_value}")
            if model_value and model_value != "Select a model..." and model_value != "":
                multi_plot_model.set(model_value)
                # FIXED ORDER: Model is now Step 2
                multi_plot_step_2_completed.set(True)
                # Reset subsequent steps when model changes
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                # FIXED: Don't trigger plot update immediately when model changes
                # This prevents losing existing traces during selection changes
                print(f"DEBUG: Multi-plot step 2 completed, but plot update deferred until selection completion")
            else:
                multi_plot_model.set("")
                # Reset Step 2 when model cleared
                multi_plot_step_2_completed.set(False)
                print(f"DEBUG: Multi-plot step 2 reset")
        except Exception as e:
            print(f"DEBUG: Error in _update_multi_plot_selected_model: {e}")
            # Gracefully handle error by resetting
            multi_plot_model.set("")
            multi_plot_step_2_completed.set(False)
    @reactive.Effect
    @reactive.event(input.multi_plot_variable)
    def _update_multi_plot_selected_variable():
        try:
            # The dropdown returns the VALUE from choices dict, which is the technical variable name
            selected_value = input.multi_plot_variable()
            print(f"DEBUG: _update_multi_plot_selected_variable - Selected value from dropdown: {selected_value}")
            
            if selected_value and selected_value != "" and selected_value != "Select a variable...":
                # The value from the dropdown is already the technical variable name
                # No conversion needed since our choices dict has display names as keys and technical names as values
                technical_var = selected_value
                print(f"DEBUG: _update_multi_plot_selected_variable - Using technical name: {technical_var}")
                
                # Store the technical variable name
                multi_plot_variable.set(technical_var)
                # FIXED ORDER: Variable is now Step 3
                multi_plot_step_3_completed.set(True)
                print(f"DEBUG: _update_multi_plot_selected_variable - Step 3 completed, variable: {technical_var}")
                # Reset subsequent steps when variable changes
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                # FIXED: Don't trigger plot update immediately when variable changes
                # This prevents losing existing traces during selection changes
            else:
                multi_plot_variable.set("")
                # Reset Step 3 when variable cleared
                multi_plot_step_3_completed.set(False)
                print(f"DEBUG: _update_multi_plot_selected_variable - Step 3 reset")
        except Exception as e:
            print(f"DEBUG: _update_multi_plot_selected_variable - Error: {e}")
            multi_plot_step_3_completed.set(False)

    @reactive.Effect
    @reactive.event(input.multi_plot_frequency)
    def _update_multi_plot_selected_frequency():
        frequency_value = input.multi_plot_frequency()
        print(f"DEBUG: Multi-plot frequency changed to: {frequency_value}")
        if frequency_value and frequency_value != "Select frequency...":
            multi_plot_frequency.set(frequency_value)
            multi_plot_step_4_completed.set(True)
            print(f"DEBUG: Multi-plot step 4 completed")
            
            # For observed variables, automatically complete scenario step (Step 5)
            var = multi_plot_variable.get()
            if var and var.startswith('obs_'):
                multi_plot_ssp.set('historical')
                multi_plot_step_5_completed.set(True)
                multi_plot_step_6_completed.set(True)
                print(f"DEBUG: Observed variable selected, auto-completing scenario step (Step 5)")
            else:
                # Reset subsequent steps when frequency changes for non-observed variables
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
            
            # FIXED: Don't trigger plot update immediately when frequency changes
            # This prevents losing existing traces during selection changes
            print(f"DEBUG: Multi-plot step 4 completed, but plot update deferred until selection completion")
    @reactive.Effect
    @reactive.event(multi_plot_variable, multi_plot_frequency)
    def _auto_complete_observed_variables():
        """Auto-complete steps 5 and 6 for observed variables when frequency is selected"""
        var = multi_plot_variable.get()
        freq = multi_plot_frequency.get()
        
        if var and var.startswith('obs_') and freq and not multi_plot_step_5_completed.get():
            print(f"DEBUG: Auto-completing observed variable steps: {var}")
            multi_plot_ssp.set('historical')
            multi_plot_step_5_completed.set(True)
            multi_plot_step_6_completed.set(True)
            print(f"DEBUG: Observed variable steps 5 and 6 auto-completed")

    @reactive.Effect
    @reactive.event(input.multi_plot_ssp)
    def _update_multi_plot_selected_scenario():
        ssp_value = input.multi_plot_ssp()
        print(f"DEBUG: Multi-plot SSP changed to: {ssp_value}")
        if ssp_value and ssp_value != "Select scenario...":
            multi_plot_ssp.set(ssp_value)
            multi_plot_step_5_completed.set(True)
            # Automatically complete step 6 when step 5 is done
            multi_plot_step_6_completed.set(True)
            
            # FIXED: Don't trigger plot update immediately when SSP changes
            # This prevents losing existing traces during selection changes
            # The plot will update when the user completes the selection
            print(f"DEBUG: Multi-plot steps 5 and 6 completed, but plot update deferred until selection completion")

    @reactive.Effect
    @reactive.event(input.multi_plot_action)
    def _update_multi_plot_action():
        action_value = input.multi_plot_action()
        print(f"DEBUG: Multi-plot action changed to: {action_value}")
        if action_value:
            multi_plot_action.set(action_value)
            # Trigger plot update when action is selected
            plot_update_trigger.set(plot_update_trigger.get() + 1)
            print(f"DEBUG: Multi-plot action set, triggered plot update")



    @reactive.Effect
    @reactive.event(input.multi_plot_mode)
    def _handle_multi_plot_mode_toggle():
        """Handle multi-plot mode toggle"""
        try:
            multi_mode = input.multi_plot_mode()
            print(f"DEBUG: Multi-plot mode toggled: {multi_mode}")
            
            # Update the reactive value to match the input
            multi_plot_mode_enabled.set(multi_mode)
            print(f"DEBUG: Multi-plot mode reactive value updated to: {multi_mode}")
            
            # Handle the toggle logic
            if multi_mode:
                # When enabling multi-plot mode, just enable it without resetting anything
                print("DEBUG: Enabling Additional Traces mode - keeping all selections")
                
                # Check if there are already additional traces
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent = list(main_plot_persistent_traces.get() or [])
                total_traces = len(current_traces) + len(persistent)
                
                if total_traces > 0:
                    # If there are already traces, just enable multi-plot mode
                    print(f"DEBUG: Additional traces mode enabled - {total_traces} existing traces preserved")
                    multi_plot_status_message.set(f"✅ Additional traces mode enabled - {total_traces} existing traces preserved")
                else:
                    # No existing traces - ready for new selections
                    multi_plot_status_message.set("🔄 Additional traces mode enabled - ready for new selections")
            else:
                # When disabling multi-plot mode, clear ALL additional trace data
                print("DEBUG: Disabling Additional Traces mode - clearing all additional traces")
                
                # FIXED: Clear additional traces when disabling mode
                main_plot_additional_traces.set([])
                main_plot_persistent_traces.set([])
                
                # Clear multi-plot specific data
                multi_plots_list.set([])
                multi_plot_time_ranges.set({})
                current_multi_plot_id.set(0)
                
                # Reset multi-plot step completion flags
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear selected values
                multi_plot_grid_id.set(None)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                multi_plot_status_message.set("🔄 Start New Selection mode - additional traces cleared")
                
                # FIXED: Trigger a plot update to remove additional traces from display
                plot_update_trigger.set(plot_update_trigger.get() + 1)
        
            # FIXED: Only trigger UI update for multi-plot dropdown visibility, not plot updates when enabling
            if multi_mode:
                # When enabling, just update UI without triggering plot refresh
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
            
            print(f"DEBUG: Multi-plot mode toggle complete")
            
        except Exception as e:
            print(f"DEBUG: Error in _handle_multi_plot_mode_toggle: {e}")

    @reactive.Calc
    def get_plot_selections():
        """Reactive calculation that returns the current plot selections"""
        variable = selected_netcdf_variable.get()
        model = selected_netcdf_model.get()
        frequency = selected_netcdf_frequency.get()
        ssp = selected_netcdf_ssp.get()
        grid_indices = get_selected_grid_indices()
        
        print(f"DEBUG: get_plot_selections - Variable: {variable}, Model: {model}, Frequency: {frequency}, SSP: {ssp}, Grid: {grid_indices}")
        
        return {
            'variable': variable,
            'model': model,
            'frequency': frequency,
            'ssp': ssp,
            'grid_indices': grid_indices
        }
    @reactive.Effect
    @reactive.event(dropdown_step_1_completed, dropdown_step_2_completed, dropdown_step_3_completed, dropdown_step_4_completed, dropdown_step_5_completed, dropdown_step_6_completed)
    def _trigger_main_plot_generation():
        """Trigger main plot generation when all steps are completed"""
        try:
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            all_steps_completed = step1_completed and step2_completed and step3_completed and step4_completed and step5_completed and step6_completed
            
            print(f"DEBUG: _trigger_main_plot_generation - all_steps_completed: {all_steps_completed}")
            
            if all_steps_completed:
                # Trigger plot update
                plot_update_trigger.set(plot_update_trigger.get() + 1)
                print(f"DEBUG: Triggered plot update - new trigger value: {plot_update_trigger.get()}")
        except Exception as e:
            print(f"DEBUG: Error in _trigger_main_plot_generation: {e}")

    @reactive.Effect
    @reactive.event(dropdown_step_1_completed, dropdown_step_2_completed, dropdown_step_3_completed, dropdown_step_4_completed, dropdown_step_5_completed, dropdown_step_6_completed)
    async def _update_progressive_dropdown_classes():
        """Update CSS classes for progressive dropdown steps with error handling"""
        try:
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            # Determine which step is currently active
            active_step = 1
            if step1_completed and not step2_completed:
                active_step = 2
            elif step2_completed and not step3_completed:
                active_step = 3
            elif step3_completed and not step4_completed:
                active_step = 4
            elif step4_completed and not step5_completed:
                active_step = 5
            elif step5_completed and not step6_completed:
                active_step = 6
            elif step6_completed:
                active_step = 7
            
            print(f"DEBUG: Progressive dropdown classes - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, active: {active_step}")
            
            # Send custom message to update CSS classes via JavaScript with error handling
            try:
                await session.send_custom_message("update_progressive_classes", {
                    "step1_completed": step1_completed,
                    "step2_completed": step2_completed,
                    "step3_completed": step3_completed,
                    "step4_completed": step4_completed,
                    "step5_completed": step5_completed,
                    "step6_completed": step6_completed,
                    "active_step": active_step
                })
            except Exception as msg_error:
                print(f"DEBUG: Non-critical error sending custom message: {msg_error}")
        except Exception as e:
            print(f"DEBUG: Error in _update_progressive_dropdown_classes: {e}")

    @reactive.Effect
    @reactive.event(multi_plot_grid_id)
    def _reset_multi_plot_steps_on_grid_change():
        """Reset multi-plot steps 2-6 when grid selection changes"""
        try:
            grid_id = multi_plot_grid_id.get()
            if grid_id:
                print(f"DEBUG: Multi-plot grid changed to {grid_id} - resetting steps 2-6")
                # Minimal delay for faster response
                time.sleep(0.05)
                # Keep step 1 completed, but reset subsequent steps
                # Don't reset step 1 since the grid was just selected
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear subsequent selections
                multi_plot_model.set(None)
                multi_plot_variable.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                # UI update will be triggered automatically by step completion changes (debounced)
                print(f"DEBUG: Multi-plot steps reset, ready for Step 2 (UI will update via debounced trigger)")
        except Exception as e:
            print(f"DEBUG: Error in _reset_multi_plot_steps_on_grid_change: {e}")
    
    # Debounce mechanism for multi-plot UI updates
    _last_multi_plot_ui_update = reactive.Value(time.time())
    
    @reactive.Effect
    @reactive.event(multi_plot_step_1_completed, multi_plot_step_2_completed, multi_plot_step_3_completed, multi_plot_step_4_completed, multi_plot_step_5_completed, multi_plot_step_6_completed)
    def _trigger_multi_plot_ui_update():
        """Trigger multi-plot UI update when steps change (debounced)"""
        print(f"DEBUG: Multi-plot step changed - scheduling UI update")
        print(f"DEBUG: Multi-plot steps: {multi_plot_step_1_completed.get()}, {multi_plot_step_2_completed.get()}, {multi_plot_step_3_completed.get()}, {multi_plot_step_4_completed.get()}, {multi_plot_step_5_completed.get()}, {multi_plot_step_6_completed.get()}")
        # Update the last update time
        _last_multi_plot_ui_update.set(time.time())
    
    @reactive.Effect
    @reactive.event(_last_multi_plot_ui_update)
    def _debounced_multi_plot_ui_update():
        """Debounced UI update to prevent race conditions - OPTIMIZED"""
        # OPTIMIZED: Reduced wait time from 0.05 to 0.01 seconds for faster UI updates
        time.sleep(0.01)
        # Check if this is still the most recent update
        if time.time() - _last_multi_plot_ui_update.get() >= 0.01:
            # Trigger multi-plot UI update
            multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)





    @output
    @render.ui
    def progressive_dropdown_ui():
        """Render progressive dropdown steps - only one visible at a time"""
        print("DEBUG: progressive_dropdown_ui - FUNCTION CALLED")
        try:
            print("DEBUG: progressive_dropdown_ui - starting function")
            # Only access the essential reactive values to minimize re-renders
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            # Access other reactive values only when needed
            filepaths_map = reactive_netcdf_filepaths.get()
            grid_feature_ids = reactive_grid_feature_ids.get()
            current_grid_id = selected_grid_id.get()
            
            print(f"DEBUG: Step completion status - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, step6: {step6_completed}")
            
            # Reduced logging for performance
            
            # Check if data is still loading
            if not filepaths_map or not grid_feature_ids:
                print("DEBUG: Data not loaded yet, showing loading state")
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div(
                        ui.div("Loading...", class_="step-title"),
                        ui.div("Please wait while the climate data is being loaded.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                )
            
            print("DEBUG: Data loaded, checking step completion")
            
            # Extract available options from filepaths_map
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            
            # If model is selected, filter variables and scenarios
            selected_model = selected_netcdf_model.get()
            if selected_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == selected_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model])

            
            # If variable is selected, filter scenarios further
            selected_variable = selected_netcdf_variable.get()
            if selected_variable and selected_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model and var == selected_variable])
            
            # Prepare choices for dropdowns
            gridbox_choices = {"Select a grid cell...": ""}
            if grid_feature_ids and len(grid_feature_ids) > 0:
                # Use actual grid IDs as both keys and display text, with blank initial option
                for fid in grid_feature_ids:
                    gridbox_choices[fid] = fid
            else:
                # Comprehensive fallback choices with actual grid IDs - include more grid cells
                # Generate a comprehensive list of grid cells
                for lat in range(7, 43):  # Based on the logs showing grid cells up to 42
                    for lon in range(3, 42):  # Based on the logs showing grid cells up to 41
                        grid_id = f"{lat},{lon}"
                        gridbox_choices[grid_id] = grid_id
            
            # Get all available options
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            
            # If model is selected, filter variables and scenarios
            selected_model = selected_netcdf_model.get()
            if selected_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == selected_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model])

            
            # If variable is selected, filter scenarios further
            selected_variable = selected_netcdf_variable.get()
            if selected_variable and selected_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == selected_model and var == selected_variable])

            
            # Determine which step to show based on completion status
            if not step1_completed:
                # Step 1: Gridbox selection dropdown
                grid_options = {"Select a gridbox...": ""}
                for grid_id in grid_feature_ids:
                    grid_options[f"Grid {grid_id}"] = grid_id
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 1: Select Grid Location", class_="step-title"),
                    ui.div("Choose a gridbox from the dropdown below:", class_="step-explanation"),
                    ui.input_select(
                        "gridbox_selector",
                        "Select Gridbox:",
                        choices=grid_options,
                        selected=selected_grid_id.get() if selected_grid_id.get() else ""
                    ),
                    class_="dropdown-step active"
                )
            elif step1_completed and not step2_completed:
                # FIXED ORDER: Step 2 is now Model/Data Type selection
                # Model choices in Shiny format: {value: label}
                model_choices = {"": "Select a model..."}
                for model in available_models:
                    model_choices[model] = model
                
                # Also add observed data option with capitalized value
                model_choices["Observed"] = "Observational Data"
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 2: Select Data Type", class_="step-title"),
                    ui.input_select("netcdf_model", "Select data type:", choices=model_choices, selected=""),
                    ui.div("Choose whether to use observational data or climate model projections for your analysis.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step2_completed and not step3_completed:
                # FIXED ORDER: Step 3 is now Variable selection (after model selection)
                # Get the selected model to filter variables
                current_model = selected_netcdf_model.get()
                print(f"DEBUG: progressive_dropdown_ui - current_model: {current_model}")
                
                # Filter variables based on model selection
                if current_model == "Observed":
                    # For observational data, only show observed variables
                    filtered_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
                    print(f"DEBUG: Main step 3 - Using observed variables: {filtered_variables}")
                else:
                    # For model data, filter by selected model
                    if current_model:
                            # For specific models, filter by model name
                            model_filtered_variables = sorted(set(var for (var, model, ssp) in filepaths_map.keys() if model == current_model))
                            filtered_variables = model_filtered_variables
                            print(f"DEBUG: Main step 3 - Using model-filtered variables for {current_model}: {model_filtered_variables}")
                    else:
                        # If no model selected yet, use all variables
                        filtered_variables = list(available_variables)
                        print(f"DEBUG: Main step 3 - No model selected, using all variables: {available_variables}")
                
                variable_choices = create_variable_choices(sorted(filtered_variables))
                
                # Get the current selected variable (technical name)
                current_variable = selected_netcdf_variable.get()
                # For Shiny, the selected value should match a choice key (technical name)
                selected_value = current_variable if current_variable and current_variable in variable_choices else ""
                
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 3: Choose Climate Variable", class_="step-title"),
                    ui.input_select("netcdf_variable", "Select climate variable:", choices=variable_choices, selected=selected_value),
                    ui.div("Choose which climate variable you want to analyze. Only variables with valid data for the selected gridbox are shown.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step3_completed and not step4_completed:
                # Frequency choices in Shiny format: {value: label}
                frequency_choices = {"": "Select frequency...", "daily": "Daily", "monthly": "Monthly", "annual": "Annual"}
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 4: Choose Frequency", class_="step-title"),
                    ui.input_select("netcdf_frequency", "Select data frequency:", choices=frequency_choices, selected=""),
                    ui.div("Choose the frequency of data to display: daily (raw data), monthly (averaged), or annual (averaged).", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step4_completed and not step5_completed:
                # Step 5: Scenario (skipped for observed)
                sel_var = selected_netcdf_variable.get()
                is_observed = sel_var in ("obs_tmin","obs_tmax","obs_pr")
                if is_observed:
                    return ui.div(
                        ui.h4("Step-by-Step Data Selection", class_="text-center"),
                        ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                        ui.div("Step 5: Scenario (skipped for observations)", class_="step-title"),
                        ui.div("Observational datasets do not require an SSP scenario.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                else:
                    scenario_choices = {"Select scenario...": ""}
                    for scenario in available_scenarios:
                        scenario_choices[scenario] = scenario
                    return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 5: Choose Scenario", class_="step-title"),
                    ui.input_select("netcdf_ssp", "Select SSP scenario:", choices=scenario_choices, selected=""),
                    ui.div("Select the Shared Socioeconomic Pathway (SSP) scenario that represents different future emission trajectories.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step5_completed and not step6_completed:
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Step 6: Generate Plot", class_="step-title"),
                    ui.div("All selections complete! The climate data plot will appear below. You can then adjust the time series range using the slider that will appear.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            else:
                # All steps completed - keep UI open to allow more additions; no completion banner
                return ui.div(
                    ui.h4("Step-by-Step Data Selection", class_="text-center"),
                    ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                    ui.div("Ready to add more or adjust selections.", class_="step-explanation"),
                    ui.div(
                        ui.input_action_button("reset_selection", "Start New Selection", class_="btn btn-primary"),
                        class_="mt-3 text-center"
                    ),
                    class_="dropdown-step active"
                )
        except Exception as e:
            print(f"DEBUG: Error in progressive_dropdown_ui: {e}")
            print(f"DEBUG: Full traceback: {traceback.format_exc()}")
            print(f"DEBUG: Exception type: {type(e)}")
            # Create comprehensive fallback choices
            fallback_choices = {'': 'Select a grid cell...'}
            for lat in range(7, 43):
                for lon in range(3, 42):
                    grid_id = f"{lat},{lon}"
                    fallback_choices[grid_id] = grid_id
            print(f"DEBUG: Created {len(fallback_choices)} fallback choices for exception handler")
            return ui.div(
                ui.h4("Step-by-Step Data Selection", class_="text-center"),
                ui.p("Follow the steps below to configure your climate data visualization. Each step will be unlocked as you complete the previous one.", class_="text-center text-muted mb-3"),
                ui.div("Step 1: Select Grid Location (Fallback)", class_="step-title"),
                ui.input_select("gridbox_selector", "Choose a grid cell:", choices=fallback_choices, selected=selected_grid_id.get() if selected_grid_id.get() else ""),
                ui.div("Select a specific grid cell on the map to analyze climate data for that location.", class_="step-explanation"),
                class_="dropdown-step active"
            )

    @output
    @render.ui
    def multi_plot_checkbox_ui():
        """Render the Additional Traces Mode checkbox with reactive state"""
        try:
            current_state = multi_plot_mode_enabled.get()
            return ui.input_checkbox("multi_plot_mode", "Enable Additional Traces Mode", value=current_state)
        except Exception as e:
            print(f"DEBUG: Error rendering multi_plot_checkbox_ui: {e}")
            return ui.input_checkbox("multi_plot_mode", "Enable Additional Traces Mode", value=False)

    @output
    @render.ui
    def multi_plot_progressive_dropdown_ui():
        """Render progressive dropdown steps for multi-plot mode"""
        print(f"DEBUG: multi_plot_progressive_dropdown_ui - FUNCTION CALLED")
        
        # Make reactive to update trigger
        try:
            _ = multi_plot_update_trigger.get()
        except:
            pass
        
        # Check if multi-plot mode is enabled
        try:
            multi_mode = multi_plot_mode_enabled.get()
            print(f"DEBUG: multi_plot_progressive_dropdown_ui - multi_mode: {multi_mode}")
            print(f"DEBUG: multi_plot_progressive_dropdown_ui - multi_plot_mode_enabled.get(): {multi_plot_mode_enabled.get()}")
            if not multi_mode:
                print(f"DEBUG: multi_plot_progressive_dropdown_ui - returning None because multi_mode is False")
                return None
        except Exception as e:
            print(f"DEBUG: Error checking multi_plot_mode: {e}")
            return None
        
        try:
            # Get multi-plot step completion status
            step1_completed = multi_plot_step_1_completed.get()
            step2_completed = multi_plot_step_2_completed.get()
            step3_completed = multi_plot_step_3_completed.get()
            step4_completed = multi_plot_step_4_completed.get()
            step5_completed = multi_plot_step_5_completed.get()
            step6_completed = multi_plot_step_6_completed.get()
            
            print(f"DEBUG: Multi-plot step completion - step1: {step1_completed}, step2: {step2_completed}, step3: {step3_completed}, step4: {step4_completed}, step5: {step5_completed}, step6: {step6_completed}")
            
            # Get available options
            filepaths_map = reactive_netcdf_filepaths.get()
            grid_feature_ids = reactive_grid_feature_ids.get()
            
            # Check if data is still loading
            if not filepaths_map or not grid_feature_ids:
                return ui.div(
                    ui.div(
                        ui.div("Loading...", class_="step-title"),
                        ui.div("Please wait while the climate data is being loaded.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                )
            
            # Prepare choices for dropdowns
            gridbox_choices = {"Select a grid cell...": ""}
            if grid_feature_ids and len(grid_feature_ids) > 0:
                for fid in grid_feature_ids:
                    gridbox_choices[fid] = fid
            else:
                # Comprehensive fallback choices with actual grid IDs - include more grid cells
                for lat in range(7, 43):  # Based on the logs showing grid cells up to 42
                    for lon in range(3, 42):  # Based on the logs showing grid cells up to 41
                        grid_id = f"{lat},{lon}"
                        gridbox_choices[grid_id] = grid_id
            
            # Get all available options
            variables = set()
            models = set()
            scenarios = set()
            
            for (var, model, ssp) in filepaths_map.keys():
                variables.add(var)
                models.add(model)
                scenarios.add(ssp)
            
            # Filter options based on previous selections
            available_models = sorted(models)
            available_variables = sorted(variables)
            available_scenarios = sorted(scenarios)
            
            # If model is selected, filter variables and scenarios
            current_model = multi_plot_model.get()
            if current_model:
                available_variables = sorted([var for (var, model, ssp) in filepaths_map.keys() if model == current_model])
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == current_model])
            
            # If variable is selected, filter scenarios further
            current_variable = multi_plot_variable.get()
            if current_variable and current_model:
                available_scenarios = sorted([ssp for (var, model, ssp) in filepaths_map.keys() if model == current_model and var == current_variable])
            
            # Determine which step to show based on completion status
            if not step1_completed:
                # Step 1: Gridbox selection dropdown for multi-plot mode
                grid_options = {"Select a gridbox...": ""}
                for grid_id in grid_feature_ids:
                    grid_options[f"Grid {grid_id}"] = grid_id
                
                return ui.div(
                    ui.div("Multi-Plot Step 1: Select Grid Location", class_="step-title"),
                    ui.div("Choose a gridbox from the dropdown below for your additional dataset:", class_="step-explanation"),
                    ui.input_select(
                        "multi_plot_gridbox_selector",
                        "Select Gridbox:",
                        choices=grid_options,
                        selected=multi_plot_grid_id.get() if multi_plot_grid_id.get() else ""
                    ),
                    class_="dropdown-step active"
                )
            elif step1_completed and not step2_completed:
                # FIXED ORDER: Step 2 is now Model (before variable selection)
                model_choices = {"Select a model...": ""}
                for model in available_models:
                    model_choices[model] = model
                
                # Also add observed data option with capitalized value
                model_choices["Observed"] = "Observational Data"
                
                return ui.div(
                    ui.div("Multi-Plot Step 2: Select Data Type", class_="step-title"),
                    ui.input_select("multi_plot_model", "Select data type:", choices=model_choices, selected=""),
                    ui.div("Choose whether to use observational data or climate model projections for your additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step2_completed and not step3_completed:
                # FIXED ORDER: Step 3 is now Variable (after model selection)
                # Get the selected model to filter variables
                current_model = multi_plot_model.get()
                
                # Filter variables based on model selection
                if current_model == "Observed":
                    # For observational data, only show observed variables
                    filtered_variables = ["obs_tmin", "obs_tmax", "obs_pr"]
                else:
                    # For model data, filter by selected model
                    if current_model:
                            # For specific models, filter by model name
                            model_filtered_variables = sorted(set(var for (var, model, ssp) in filepaths_map.keys() if model == current_model))
                            filtered_variables = model_filtered_variables
                    else:
                        # If no model selected yet, use all variables
                        filtered_variables = list(available_variables)
                
                variable_choices = create_variable_choices(sorted(filtered_variables))
                
                # Get the current selected variable (technical name)
                current_multi_var = multi_plot_variable.get()
                # For Shiny, the selected value should match a choice key (technical name)
                selected_value = current_multi_var if current_multi_var and current_multi_var in variable_choices else ""
                
                return ui.div(
                    ui.div("Multi-Plot Step 3: Choose Climate Variable", class_="step-title"),
                    ui.input_select("multi_plot_variable", "Select climate variable:", choices=variable_choices, selected=selected_value),
                    ui.div("Choose which climate variable for the additional dataset. Only variables with valid data for the selected gridbox are shown.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step3_completed and not step4_completed:
                # Frequency choices in Shiny format: {value: label}
                frequency_choices = {"": "Select frequency...", "daily": "Daily", "monthly": "Monthly", "annual": "Annual"}
                return ui.div(
                    ui.div("Multi-Plot Step 4: Choose Frequency", class_="step-title"),
                    ui.input_select("multi_plot_frequency", "Select data frequency:", choices=frequency_choices, selected=""),
                    ui.div("Choose the frequency of data to display: daily (raw data), monthly (averaged), or annual (averaged).", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step4_completed and not step5_completed:
                # Step 5: Scenario (skipped for observations)
                sel_var = multi_plot_variable.get()
                is_observed = sel_var in ("obs_tmin","obs_tmax","obs_pr")
                if is_observed:
                    return ui.div(
                        ui.div("Multi-Plot Step 5: Scenario (skipped for observations)", class_="step-title"),
                        ui.div("Observational datasets do not require an SSP scenario.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
                scenario_choices = {"Select scenario...": ""}
                for scenario in available_scenarios:
                    scenario_choices[scenario] = scenario
                return ui.div(
                    ui.div("Multi-Plot Step 5: Choose Scenario", class_="step-title"),
                    ui.input_select("multi_plot_ssp", "Select SSP scenario:", choices=scenario_choices, selected=""),
                    ui.div("Select the scenario for the additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
            elif step5_completed and not step6_completed:
                # Show action options immediately without premature unit compatibility check
                current_variable = multi_plot_variable.get()
                plots_list = multi_plots_list.get()
                
                if current_variable:
                    # Only show option to add to main plot - no separate plots
                    return ui.div(
                        ui.div("Multi-Plot Step 6: Add to Main Plot", class_="step-title"),
                        ui.div(f"Ready to add {get_variable_display_name(current_variable)} to your main plot.", class_="step-explanation"),
                        ui.div(
                            ui.div("✅ This trace will be added to your main plot (if units are compatible)", class_="alert alert-success"),
                            ui.div("Note: Only traces with compatible units will be added to the same plot. You can add the same variable with different frequencies.", class_="text-muted"),
                            class_="mt-3"
                        ),
                        class_="dropdown-step active"
                    )
                else:
                    return ui.div(
                        ui.div("Multi-Plot Step 5: Waiting for Selection", class_="step-title"),
                        ui.div("Please complete the previous steps to select a variable for comparison.", class_="step-explanation"),
                        class_="dropdown-step active"
                    )
            else:
                # When steps were previously completed, immediately present Step 1 UI again
                # so the user can start a new selection without seeing a banner
                # Recreate the gridbox choices (already computed above)
                return ui.div(
                    ui.div("Multi-Plot Step 1: Select Grid Location", class_="step-title"),
                    ui.input_select("multi_plot_gridbox_selector", "Choose a grid cell:", choices=gridbox_choices, selected=""),
                    ui.div("Select a specific grid cell for your additional dataset.", class_="step-explanation"),
                    class_="dropdown-step active"
                )
        except Exception as e:
            print(f"DEBUG: Error in multi_plot_progressive_dropdown_ui: {e}")
            return ui.div(
                ui.div("Multi-Plot Mode Error", class_="step-title"),
                ui.div("An error occurred while loading multi-plot options.", class_="step-explanation"),
                class_="dropdown-step active"
            )
    @output
    @render.ui
    def map_and_data_output():
        print("DEBUG: map_and_data_output FUNCTION CALLED")
        
        # FIXED: Batch all reactive reads at the start to ensure atomic rendering
        # This prevents sequential loading by reading all reactive values first
        with reactive.isolate():
            grid_geojson = reactive_grid_geojson.get()
            netcdf_filepaths = reactive_netcdf_filepaths.get()
            grid_feature_ids = reactive_grid_feature_ids.get()
        
        # Re-read without isolation for reactivity
        grid_geojson = reactive_grid_geojson.get()
        netcdf_filepaths = reactive_netcdf_filepaths.get()
        
        # Check if ALL necessary reactive values are ready for map display
        if grid_geojson is None or not netcdf_filepaths:
            print("DEBUG: map_and_data_output - Data not loaded yet")
            print(f"DEBUG: reactive_grid_geojson: {grid_geojson}")
            print(f"DEBUG: reactive_netcdf_filepaths: {len(netcdf_filepaths) if netcdf_filepaths else 0}")
            
            # Show a simple UI even if data is not loaded to test functionality
            # Note: Do NOT include progressive_dropdown_ui here to avoid duplicate output IDs
            return ui.div(
                ui.div(
                    ui.h5("Data Loading Status", class_="text-center"),
                    ui.p(f"Grid GeoJSON: {'Loaded' if grid_geojson else 'Not loaded'}", class_="text-center"),
                    ui.p(f"File paths: {'Loaded' if netcdf_filepaths else 'Not loaded'}", class_="text-center"),
                    ui.p(f"Grid feature IDs: {'Loaded' if grid_feature_ids else 'Not loaded'}", class_="text-center"),
                    class_="alert alert-info"
                ),
                ui.div(
                    ui.div(
                        ui.h5("Test Plot Area", class_="text-center text-muted"),
                        ui.p("Waiting for data to load...", class_="text-center text-muted"),
                        class_="mt-3"
                    ),
                    class_="mt-3"
                )
            )
        else:
            # Check if all progressive steps are completed
            step1_completed = dropdown_step_1_completed.get()
            step2_completed = dropdown_step_2_completed.get()
            step3_completed = dropdown_step_3_completed.get()
            step4_completed = dropdown_step_4_completed.get()
            step5_completed = dropdown_step_5_completed.get()
            step6_completed = dropdown_step_6_completed.get()
            
            all_steps_completed = step1_completed and step2_completed and step3_completed and step4_completed and step5_completed and step6_completed
            
            # Debug output
            print(f"DEBUG: map_and_data_output - Step completion status:")
            print(f"DEBUG:   Step 1: {step1_completed}")
            print(f"DEBUG:   Step 2: {step2_completed}")
            print(f"DEBUG:   Step 3: {step3_completed}")
            print(f"DEBUG:   Step 4: {step4_completed}")
            print(f"DEBUG:   Step 5: {step5_completed}")
            print(f"DEBUG:   Step 6: {step6_completed}")
            print(f"DEBUG:   All steps completed: {all_steps_completed}")
            print(f"DEBUG:   Selected grid ID: {selected_grid_id.get()}")
            print(f"DEBUG:   Selected variable: {selected_netcdf_variable.get()}")
            print(f"DEBUG:   Selected model: {selected_netcdf_model.get()}")
            print(f"DEBUG:   Selected frequency: {selected_netcdf_frequency.get()}")
            print(f"DEBUG:   Selected SSP: {selected_netcdf_ssp.get()}")
            
            # FIXED: Header and description now load with map and data to prevent sequential loading
            return ui.div(
                ui.h3("Explore Climate Data by Grid Location"),
                ui.p("Click a grid cell on the map to view its detailed climate projections. Only grid cells with data available across ALL loaded climate variables, models, and scenarios are shown.", class_="text-muted mb-3"),
                ui.div(
                output_widget("gridbox_map"),
                    class_="map-container",
                    style="width: 100%; max-width: none; overflow: visible; margin-bottom: 20px;"
                ),
                ui.output_text("selected_gridbox_info"),
                ui.div(
                    # Plot container with absolutely locked dimensions
                    ui.div(
                        output_widget("selected_gridbox_plot"),
                        class_="plot-container",
                        style="width: 100%; max-width: none; overflow: visible; height: 600px !important; min-height: 600px !important; max-height: 600px !important; flex-shrink: 0; flex-grow: 0; flex-basis: 600px; margin-bottom: 0px; padding-bottom: 20px; position: relative;"
                    ),
                    # Mean Absolute Difference Table
                    ui.div(
                        ui.output_ui("mad_table"),
                        style="margin-top: 40px; margin-bottom: 20px; flex-shrink: 0; flex-grow: 0;"
                    ),
                    # Unified trace controls (includes main plot + additional traces in scrollable table)
                    ui.div(
                        ui.output_ui("additional_traces_controls"),
                        style="margin-top: 30px; flex-shrink: 0; flex-grow: 0;"
                    ),
                    class_="mt-3",
                    style="display: flex; flex-direction: column; gap: 20px; width: 100%;"
                ),
                # Multi-plot output - show additional plots when needed (isolated container)
                ui.div(
                    ui.output_ui("multi_plot_output"),
                    style="flex-shrink: 0; flex-grow: 0; margin-top: 30px;"
                ),
                # Multi-plot UI section - show when main plot is complete (isolated container)
                ui.div(
                    ui.div(
                        ui.h4("Additional Traces Mode", class_="mt-4"),
                        ui.div(
                            ui.output_ui("multi_plot_checkbox_ui"),
                            ui.div("Enable this to add additional time series traces to your main plot using the progressive dropdown system. All traces with compatible units will be displayed on the same graph. No separate plots will be created.", class_="step-explanation"),
                            class_="mt-3"
                        ),
                        ui.output_ui("multi_plot_progressive_dropdown_ui"),
                        class_="mt-3"
                    ),
                    style="flex-shrink: 0; flex-grow: 0; margin-top: 30px;"
                ) if all_steps_completed else None,
                ui.div(
                    ui.div(
                        ui.h5("Select Options to View Data", class_="text-center text-muted"),
                        ui.p("Please complete the progressive dropdown steps above to view climate data plots. You can also click directly on the map to select a grid cell.", class_="text-center text-muted"),
                        class_="alert alert-info"
                    ),
                    class_="mt-3"
                ) if not all_steps_completed else None
            )
    
    @reactive.Effect
    @reactive.event(input.time_series_range_main)
    def _handle_main_time_range_change():
        """Handle changes to the main time series range slider"""
        try:
            main_range = input.time_series_range_main()
            if main_range and len(main_range) == 2:
                print(f"DEBUG: Main time series range changed to: {main_range}")
                # Trigger plot update to apply new range to all traces
                plot_update_trigger.set(plot_update_trigger.get() + 1)
        except Exception as e:
            print(f"DEBUG: Error handling main time range change: {e}")
    
    @reactive.Effect
    @reactive.event(input.trendline_main, ignore_none=True)
    def _handle_main_trendline_change():
        """Handle changes to the main trendline checkbox"""
        try:
            show_trend = input.trendline_main()
            print(f"DEBUG: Main trendline checkbox changed to: {show_trend}")
            # Trigger plot update to show/hide trendline
            plot_update_trigger.set(plot_update_trigger.get() + 1)
        except Exception as e:
            print(f"DEBUG: Error handling main trendline change: {e}")
    
    # Note: Additional trace sliders and checkboxes will automatically trigger plot updates
    # because the plot function reads them directly when rendering each trace

    # IMPROVED: Debounced update mechanism with better batching for slider changes
    _last_trace_change_time = reactive.Value(0)
    _update_scheduled = reactive.Value(False)
    
    @reactive.Effect
    @reactive.event(main_plot_additional_traces, main_plot_persistent_traces)
    def _debounced_trace_change_handler():
        """Debounced handler for trace changes to prevent UI errors"""
        try:
            import time
            current_time = time.time()
            _last_trace_change_time.set(current_time)
            _update_scheduled.set(True)
            print("DEBUG: Trace change detected, scheduling debounced update")
        except Exception as e:
            print(f"DEBUG: Error in debounced trace change handler: {e}")
    
    @reactive.Effect
    @reactive.event(_last_trace_change_time)
    def _trigger_plot_update_on_trace_changes():
        """Trigger plot update when additional traces change (debounced with better batching)"""
        try:
            import time
            # IMPROVED: Longer delay (0.5s) to properly batch rapid changes
            time.sleep(0.5)
            
            # Check if enough time has passed since the last change
            time_since_change = time.time() - _last_trace_change_time.get()
            
            # IMPROVED: Only update if at least 0.5s has passed AND an update is scheduled
            if time_since_change >= 0.45 and _update_scheduled.get():
                # This will trigger the plot to re-render when traces change
                current_trigger = plot_update_trigger.get()
                plot_update_trigger.set(current_trigger + 1)
                _update_scheduled.set(False)
                print("DEBUG: Triggered plot update due to trace changes (debounced, batched)")
            else:
                print(f"DEBUG: Skipping update - more changes incoming (time since: {time_since_change:.2f}s)")
        except Exception as e:
            print(f"DEBUG: Error triggering plot update: {e}")
            # Don't let errors crash the app - just log them
            import traceback
            traceback.print_exc()
    # Test handler for any plotly events - DISABLED for performance
    # @reactive.Effect
    # def _test_any_input():
    #     try:
    #         # Try to access any plotly-related inputs
    #         for attr in dir(input):
    #             if 'gridbox' in attr.lower() or 'map' in attr.lower():
    #                 try:
    #                     value = getattr(input, attr)()
    #                     if value is not None:
    #                         print(f"DEBUG: Found input {attr}: {value}")
    #                 except:
    #                     pass
    #     except Exception as e:
    #         print(f"DEBUG: Error in test handler: {e}")

    # Add a more comprehensive click detection approach - DISABLED for performance
    # @reactive.Effect
    # def _comprehensive_click_detection():
    #     """Try to detect clicks using multiple approaches"""
    #     try:
    #         # Check all possible plotly input patterns
    #         possible_inputs = [
    #             'gridbox_map_click',
    #             'gridbox_map_selected', 
    #             'gridbox_map_hover',
    #             'gridbox_map_brushed',
    #             'gridbox_map_brushedPoints',
    #             'gridbox_map_selectedPoints',
    #             'gridbox_map_restyle',
    #             'gridbox_map_relayout'
    #         ]
    #         
    #         for input_name in possible_inputs:
    #             if hasattr(input, input_name):
    #                 try:
    #                     value = getattr(input, input_name)()
    #                     if value is not None:
    #                         print(f"DEBUG: ===== CLICK DETECTED via {input_name} =====")
    #                         print(f"DEBUG: Click data: {value}")
    #                         _process_click_data(value)
    #                 except Exception as e:
    #                     pass
    #     except Exception as e:
    #         print(f"DEBUG: Error in comprehensive click detection: {e}")

    def _process_click_data(click_data):
        """Process click data and update selections"""
        print(f"DEBUG: Processing click data: {click_data}")
        print(f"DEBUG: Click data type: {type(click_data)}")
        
        # Extract the clicked grid ID from the click data
        clicked_id = None
        if click_data and 'points' in click_data and len(click_data['points']) > 0:
            point = click_data['points'][0]
            print(f"DEBUG: First point data: {point}")
            print(f"DEBUG: Point keys: {point.keys() if isinstance(point, dict) else 'Not a dict'}")
            
            if 'customdata' in point:
                clicked_id = point['customdata']
                print(f"DEBUG: Found customdata: {clicked_id}")
            elif 'text' in point:
                clicked_id = point['text']
                print(f"DEBUG: Found text: {clicked_id}")
            else:
                print(f"DEBUG: No customdata or text found in point")
        else:
            print(f"DEBUG: No points in click data or click_data is None")
        
        print(f"DEBUG: Extracted clicked ID: {clicked_id}")
        
        if clicked_id and clicked_id in reactive_grid_feature_ids.get():
            # Determine if we are in multi-plot mode
            multi_mode = multi_plot_mode_enabled.get()
            print(f"DEBUG: Processing click for grid ID: {clicked_id}, multi_mode: {multi_mode}")
            
            if multi_mode:
                # In multi-plot mode, update the multi-plot grid selection
                multi_plot_grid_id.set(clicked_id)
                print(f"DEBUG: Set multi_plot_grid_id to: {clicked_id}")
            else:
                # In single-plot mode, update the main selection
                selected_grid_id.set(clicked_id)
                print(f"DEBUG: Set selected_grid_id to: {clicked_id}")
            
            # Update the dropdown to reflect the selection
            ui.update_select("gridbox_selector", selected=clicked_id)
            
            # Mark step 1 as completed
            dropdown_step_1_completed.set(True)
            print(f"DEBUG: Step 1 completed!")
        else:
            print(f"DEBUG: Invalid grid ID or not in valid IDs: {clicked_id}")
            print(f"DEBUG: Valid IDs: {list(reactive_grid_feature_ids.get())[:5]}...")

    # OPTIMIZED: Batch reactive updates to prevent redundant triggers
    # Handle gridbox selector dropdown changes
    @reactive.Effect
    @reactive.event(input.gridbox_selector)
    def _handle_gridbox_selector():
        """Handle gridbox selector dropdown changes - OPTIMIZED to reduce cascading updates"""
        print("DEBUG: ===== GRIDBOX SELECTOR CHANGED =====")
        selected_display = input.gridbox_selector()
        print(f"DEBUG: Selected gridbox: {selected_display}")
        
        if selected_display and selected_display != "" and selected_display != "Select a gridbox...":
            # Extract the actual grid ID from the display text (e.g., "Grid 8,19" -> "8,19")
            if selected_display.startswith("Grid "):
                actual_grid_id = selected_display[5:]  # Remove "Grid " prefix
            else:
                actual_grid_id = selected_display
            
            # OPTIMIZED: Check validity without triggering excessive reactive reads
            with reactive.isolate():
                valid_ids = reactive_grid_feature_ids.get()
            
            if actual_grid_id in valid_ids:
                print(f"DEBUG: Valid grid ID: {actual_grid_id}, processing selection...")
                # OPTIMIZED: Batch updates to prevent cascading reactive triggers
                # Only update if the value actually changed
                current_grid_id = selected_grid_id.get()
                if current_grid_id != actual_grid_id:
                    selected_grid_id.set(actual_grid_id)
                    print(f"DEBUG: Set selected_grid_id to: {actual_grid_id}")
                    # Mark step 1 as completed immediately after grid selection
                    dropdown_step_1_completed.set(True)
                    print(f"DEBUG: Step 1 completed!")
                else:
                    print(f"DEBUG: Grid ID unchanged, skipping redundant update")
            else:
                print(f"DEBUG: Invalid grid ID: {actual_grid_id}")
        else:
            print(f"DEBUG: No gridbox selected")
    
    # OPTIMIZED: Handle multi-plot gridbox selector dropdown changes
    @reactive.Effect
    @reactive.event(input.multi_plot_gridbox_selector)
    def _handle_multi_plot_gridbox_selector():
        """Handle multi-plot gridbox selector dropdown changes - OPTIMIZED"""
        print("DEBUG: ===== MULTI-PLOT GRIDBOX SELECTOR CHANGED =====")
        selected_display = input.multi_plot_gridbox_selector()
        print(f"DEBUG: Selected multi-plot gridbox: {selected_display}")
        
        if selected_display and selected_display != "" and selected_display != "Select a gridbox...":
            # Extract the actual grid ID from the display text (e.g., "Grid 8,19" -> "8,19")
            if selected_display.startswith("Grid "):
                actual_grid_id = selected_display[5:]  # Remove "Grid " prefix
            else:
                actual_grid_id = selected_display
            
            # OPTIMIZED: Check validity without triggering excessive reactive reads
            with reactive.isolate():
                valid_ids = reactive_grid_feature_ids.get()
            
            if actual_grid_id in valid_ids:
                print(f"DEBUG: Valid grid ID: {actual_grid_id}, processing multi-plot selection...")
                # FIXED: Always reset steps 2-6 when grid is selected (even if same grid)
                # This allows adding multiple traces from the same grid
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                # Clear subsequent selections
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                
                # OPTIMIZED: Only update grid_id if value changed to avoid cascading updates
                current_grid_id = multi_plot_grid_id.get()
                if current_grid_id != actual_grid_id:
                    multi_plot_grid_id.set(actual_grid_id)
                    print(f"DEBUG: Set multi_plot_grid_id to: {actual_grid_id}")
                else:
                    print(f"DEBUG: Multi-plot grid ID unchanged, but resetting workflow for new trace")
                # FIXED: Always mark step 1 as completed when valid grid is selected (even if unchanged)
                # This allows adding multiple traces from the same grid
                multi_plot_step_1_completed.set(True)
                print(f"DEBUG: Multi-plot Step 1 completed! Ready for new trace from grid {actual_grid_id}")
            else:
                print(f"DEBUG: Invalid multi-plot grid ID: {actual_grid_id}")
        else:
            print(f"DEBUG: No multi-plot gridbox selected")


    # Try different input name patterns for plotly clicks - DISABLED for performance
    # @reactive.Effect
    # def _try_all_plotly_inputs():
    #     try:
    #         # Try various possible input names
    #         possible_inputs = [
    #             'gridbox_map_click',
    #             'gridbox_map_selected', 
    #             'gridbox_map_hover',
    #             'gridbox_map_brushed',
    #             'gridbox_map_brushedPoints',
    #             'gridbox_map_selectedPoints',
    #             'gridbox_map_restyle',
    #             'gridbox_map_relayout'
    #         ]
    #         
    #         for input_name in possible_inputs:
    #             try:
    #                 if hasattr(input, input_name):
    #                     value = getattr(input, input_name)()
    #                     if value is not None:
    #                         print(f"DEBUG: Found plotly input {input_name}: {value}")
    #             except:
    #                 pass
    #     except Exception as e:
    #         print(f"DEBUG: Error checking plotly inputs: {e}")

    @reactive.Effect
    @reactive.event(input.gridbox_map_click)
    def _handle_map_click():
        print("DEBUG: ===== CLICK HANDLER TRIGGERED =====")
        try:
            click_data = input.gridbox_map_click()
            print(f"DEBUG: Map click data: {click_data}")
        except Exception as e:
            print(f"DEBUG: Error getting map click input: {e}")
            return
        print(f"DEBUG: Map click detected! Click data: {click_data}")
        print(f"DEBUG: Click data type: {type(click_data)}")
        
        # Extract the clicked grid ID from the click data
        clicked_id = None
        if click_data and 'points' in click_data and len(click_data['points']) > 0:
            point = click_data['points'][0]
            print(f"DEBUG: First point data: {point}")
            print(f"DEBUG: Point keys: {point.keys() if isinstance(point, dict) else 'Not a dict'}")
            
            if 'customdata' in point:
                clicked_id = point['customdata']
                print(f"DEBUG: Found customdata: {clicked_id}")
            elif 'text' in point:
                clicked_id = point['text']
                print(f"DEBUG: Found text: {clicked_id}")
            else:
                print(f"DEBUG: No customdata or text found in point")
        else:
            print(f"DEBUG: No points in click data or click_data is None")
        
        print(f"DEBUG: Extracted clicked ID: {clicked_id}")
        
        if clicked_id and clicked_id in reactive_grid_feature_ids.get():
            # Determine if we are in multi-plot mode
            multi_mode = multi_plot_mode_enabled.get()

            if multi_mode:
                # In multi-plot mode, map clicks should select the grid for the multi-plot flow
                if multi_plot_grid_id.get() != clicked_id:
                    multi_plot_grid_id.set(clicked_id)
                    print(f"DEBUG:   --> multi_plot_grid_id reactive value set to: {multi_plot_grid_id.get()}")
                # No need to update dropdown since we removed it
                multi_plot_step_1_completed.set(True)
                print(f"DEBUG: Multi-plot Step 1 completed - advancing to Step 2")
                # Trigger multi-plot UI update
                try:
                    multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                except Exception:
                    pass
            else:
                # In regular mode, map clicks select the main grid
                if selected_grid_id.get() != clicked_id:
                    selected_grid_id.set(clicked_id)
                    print(f"DEBUG:   --> selected_grid_id reactive value set to: {selected_grid_id.get()}")
                # No need to update dropdown since we removed it
                dropdown_step_1_completed.set(True)
                print(f"DEBUG: Step 1 completed - advancing to Step 2")
                # Trigger UI update
                try:
                    plot_update_trigger.set(plot_update_trigger.get() + 1)
                except Exception:
                    pass

    @reactive.Calc
    def get_selected_grid_indices():
        grid_id = selected_grid_id.get()
        print(f"DEBUG: get_selected_grid_indices triggered. current selected_grid_id: {grid_id}")
        if grid_id:
            try:
                lat_idx_str, lon_idx_str = grid_id.split(",")
                new_indices = (int(lat_idx_str), int(lon_idx_str))
                print(f"DEBUG:   --> calculated new indices: {new_indices}")
                print(f"DEBUG:   --> lat_idx type: {type(new_indices[0])}, lon_idx type: {type(new_indices[1])}")
                return new_indices
            except (ValueError, TypeError) as e:
                print(f"ERROR: Error parsing grid ID: {grid_id}. Expected format 'lat_idx,lon_idx'.")
                print(f"ERROR: Exception: {e}")
                return None
        return None

    @output
    @render.text
    def selected_gridbox_info():
        current_id = selected_grid_id.get()
        all_filepaths = reactive_netcdf_filepaths.get()
    
        if not all_filepaths:
            return "Map data is loading..."

        # Try to get coordinates from a cached dataset first
        ds_for_coords = None
        for key, ds_obj in _cached_datasets.get().items():
            if 'lat' in ds_obj.coords and 'lon' in ds_obj.coords:
                ds_for_coords = ds_obj
                break
    
        # If not in cache, open the first available file to get coordinates
        if ds_for_coords is None and all_filepaths:
            first_filepath = next(iter(all_filepaths.values()))
            try:
                with xr.open_dataset(first_filepath, engine="netcdf4", decode_times=False) as ds_temp:
                    if 'lat' in ds_temp.coords and 'lon' in ds_temp.coords:
                        ds_for_coords = ds_temp
            except Exception as e:
                print(f"Error opening first file for coordinate reference: {e}")

        if ds_for_coords is None or 'lat' not in ds_for_coords.coords or 'lon' not in ds_for_coords.coords:
            return "Map data is loading (coordinates not found)..."

        grid_indices = get_selected_grid_indices()

        print(f"DEBUG: selected_gridbox_info rendering. Current ID: {current_id}, Indices: {grid_indices}")
    
        if current_id and grid_indices:
            lat_idx, lon_idx = grid_indices
            if 0 <= lat_idx < ds_for_coords['lat'].size and 0 <= lon_idx < ds_for_coords['lon'].size:
                actual_lat = ds_for_coords['lat'].values[lat_idx]
                actual_lon = ds_for_coords['lon'].values[lon_idx]
                display_lon = actual_lon
                if np.any(ds_for_coords['lon'].values > 180) and actual_lon > 180:
                    display_lon = actual_lon - 360
                return f"Selected Gridbox: (Lat Index: {lat_idx}, Lon Index: {lon_idx}) | Approx. Lat: {actual_lat:.3f}, Approx. Lon: {display_lon:.3f}"
            else:
                print(f"ERROR: Indices ({lat_idx}, {lon_idx}) out of bounds for ds_for_coords in selected_gridbox_info.")
                return f"Selected Gridbox ID: {current_id} (Indices out of bounds or dataset not ready)"
        return "No gridbox selected. Click a cell on the map."

    @output
    @render_widget
    def gridbox_map():
        """Render gridbox map with error handling for widget cleanup"""
        try:
            print("DEBUG: gridbox_map FUNCTION CALLED - Creating simple map display")
            
            # Prefer highlighting the multi-plot grid selection if present; otherwise use the main selection
            multi_current_id = multi_plot_grid_id.get()
            current_id = multi_current_id or selected_grid_id.get()
            grid_geojson_data = reactive_grid_geojson.get()
            grid_feature_ids_data = reactive_grid_feature_ids.get()

            print(f"DEBUG: gridbox_map rendering. Current highlighted grid_id: {current_id}")
            print(f"DEBUG: gridbox_map - grid_geojson_data: {grid_geojson_data is not None}, features: {len(grid_geojson_data.features) if grid_geojson_data and grid_geojson_data.features else 0}")
            print(f"DEBUG: gridbox_map - grid_feature_ids_data: {len(grid_feature_ids_data) if grid_feature_ids_data else 0}")
        except Exception as e:
            print(f"DEBUG: Error in gridbox_map initialization: {e}")
            # FIXED: Return blank map instead of text - data loads in background
            fig = go.Figure()
            fig.update_layout(
                height=500, width=None,
                margin=dict(l=30, r=30, t=50, b=100),
                autosize=True,
                xaxis=dict(visible=False),
                yaxis=dict(visible=False)
            )
            return fig

        try:
            if not grid_geojson_data or not grid_geojson_data.features or not grid_feature_ids_data:
                # FIXED: Return blank map instead of text - data loads in background  
                fig = go.Figure()
                fig.update_layout(
                    height=500, width=None,
                    margin=dict(l=30, r=30, t=50, b=100),
                    autosize=True,
                    xaxis=dict(visible=False),
                    yaxis=dict(visible=False)
                )
                return fig

            # Create choropleth map with blue filled areas
            fig = go.Figure()

            # Add choropleth layer for filled blue areas
            fig.add_trace(go.Choroplethmapbox(
                geojson=grid_geojson_data,
                locations=[feature['id'] for feature in grid_geojson_data.features],
                z=[1 if feature['id'] == current_id else 0 for feature in grid_geojson_data.features],
                colorscale=[[0, 'blue'], [1, 'red']],
                showscale=False,
                marker_opacity=0.8,
                marker_line_width=0,
                name="Grid Cells",
                hovertemplate="<b>Grid ID</b>: %{location}<br><extra></extra>"
            ))

            fig.update_layout(
                mapbox_style="open-street-map",
                mapbox_center={"lat": (SB_LAT_MIN + SB_LAT_MAX) / 2, "lon": (SB_LON_MIN + SB_LON_MAX) / 2},
                mapbox_zoom=9,
                margin={"r":0,"t":0,"l":0,"b":0},
                height=500,
                mapbox_bounds={"west": SB_LON_MIN, "east": SB_LON_MAX, "south": SB_LAT_MIN, "north": SB_LAT_MAX}
            )

            print("DEBUG: Gridbox map created - display only")
            return fig
        except Exception as e:
            print(f"DEBUG: Error creating gridbox_map: {e}")
            # Return fallback figure
            fig = go.Figure()
            fig.add_annotation(text="Map temporarily unavailable", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
            fig.update_layout(height=500, width=None, margin=dict(l=30, r=30, t=50, b=100), autosize=True)
            return fig







    def _render_points_map(points, title):
        fig = go.Figure()
        if not points:
            fig.add_annotation(text=f"No points detected for {title}", x=0.5, y=0.5, showarrow=False)
        else:
            lats = [p[0] for p in points]
            lons = [p[1] for p in points]
            # adjust 0..360 to -180..180 if needed
            lons = [lon-360 if lon>180 else lon for lon in lons]
            fig.add_trace(go.Scattermapbox(
                lat=lats,
                lon=lons,
                mode='markers',
                marker=dict(size=6, color='#1f77b4', opacity=0.8),
                name=title,
                hovertemplate="Lat: %{lat}<br>Lon: %{lon}<extra></extra>"
            ))
        # Auto-center/zoom to data (no Santa Barbara bounds)
        if points:
            lat_min, lat_max = min(lats), max(lats)
            lon_min, lon_max = min(lons), max(lons)
            center_lat = (lat_min + lat_max) / 2.0
            center_lon = (lon_min + lon_max) / 2.0
            # Approximate zoom based on extent
            lat_span = max(1e-6, lat_max - lat_min)
            lon_span = max(1e-6, lon_max - lon_min)
            extent = max(lat_span, lon_span)
            if extent > 40:
                zoom = 2
            elif extent > 20:
                zoom = 3
            elif extent > 10:
                zoom = 4
            elif extent > 5:
                zoom = 5
            elif extent > 2:
                zoom = 6
            elif extent > 1:
                zoom = 7
            elif extent > 0.5:
                zoom = 8
            else:
                zoom = 9
        else:
            center_lat = (SB_LAT_MIN + SB_LAT_MAX) / 2
            center_lon = (SB_LON_MIN + SB_LON_MAX) / 2
            zoom = 9

        fig.update_layout(
            mapbox_style="open-street-map",
            mapbox_center={"lat": center_lat, "lon": center_lon},
            mapbox_zoom=zoom,
            margin={"r":0,"t":0,"l":0,"b":0},
            height=600
        )
        return fig
    @output
    @render_widget
    def selected_gridbox_plot():
        """Main plot function - handles single plots and multi-plot combinations"""
        # React to additions to the main plot from multi-plot UI
        try:
            _ = main_plot_update_trigger.get()
        except Exception:
            pass
        
        print("DEBUG: selected_gridbox_plot FUNCTION CALLED")
        print(f"DEBUG: plot_update_trigger value: {plot_update_trigger.get()}")
        print(f"DEBUG: dropdown_step_5_completed value: {dropdown_step_5_completed.get()}")
        
        # Add timeout protection
        def timeout_handler(signum, frame):
            raise TimeoutError("Plot generation timed out")
        # Set a 30-second timeout
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(30)
        try:
            # Get the current selections reactively
            selections = get_plot_selections()
            variable = selections['variable']
            model = selections['model']
            frequency = selections['frequency']
            ssp = selections['ssp']
            grid_indices = selections['grid_indices']
            lat_idx, lon_idx = grid_indices if grid_indices else (None, None)
            
            print(f"DEBUG: Plot function received - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
            print(f"DEBUG: Grid indices: {grid_indices}, lat_idx: {lat_idx}, lon_idx: {lon_idx}")
            print(f"DEBUG: Step completion status:")
            print(f"DEBUG:   Step 1: {dropdown_step_1_completed.get()}")
            print(f"DEBUG:   Step 2: {dropdown_step_2_completed.get()}")
            print(f"DEBUG:   Step 3: {dropdown_step_3_completed.get()}")
            print(f"DEBUG:   Step 4: {dropdown_step_4_completed.get()}")
            print(f"DEBUG:   Step 5: {dropdown_step_5_completed.get()}")
            print(f"DEBUG: Selected grid ID: {selected_grid_id.get()}")
            
            multi_mode = multi_plot_mode_enabled.get()
            print(f"DEBUG: Multi-plot mode detected: {multi_mode}")

            # Check if all required values are available (observations skip model/ssp)
            is_observed_main = variable in ("obs_tmin", "obs_tmax", "obs_pr")
            if not variable or not frequency or (not is_observed_main and (not model or not ssp)):
                print(f"DEBUG: Missing required values - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
                fig = go.Figure()
                fig.add_annotation(text="Please complete all selections to view the plot.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                signal.alarm(0)  # Cancel timeout
                return fig

            # Check if grid indices are available
            if lat_idx is None or lon_idx is None:
                print(f"DEBUG: No grid indices available - lat_idx: {lat_idx}, lon_idx: {lon_idx}")
                # Don't auto-select grid cell - user must select explicitly
                if selected_grid_id.get() is None:
                    print(f"DEBUG: No grid selected - user must select explicitly")
                    # Don't try to auto-select - just show error message
                # If still no grid indices, show error
                if lat_idx is None or lon_idx is None:
                    print(f"DEBUG: No grid indices available - user must select a grid cell")
                    fig = go.Figure()
                    fig.add_annotation(text="Please select a grid cell on the map first.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                    fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                    signal.alarm(0)  # Cancel timeout
                    return fig

            filepaths_map = reactive_netcdf_filepaths.get()
            print(f"DEBUG: Filepaths map has {len(filepaths_map) if filepaths_map else 0} entries")
            
            # Resolve file path for projections; observations handled separately
            if not is_observed_main:
                file_key = (variable, model, ssp)
                print(f"DEBUG: Looking for file key: {file_key}")
                if file_key not in filepaths_map:
                    print(f"DEBUG: File key not found in filepaths_map")
                    available_keys = list(filepaths_map.keys())[:5] if filepaths_map else []
                    print(f"DEBUG: Available keys (first 5): {available_keys}")
                    fig = go.Figure()
                    fig.add_annotation(text=f"No data file found for {variable}, {model}, {ssp}<br>Available combinations: {len(filepaths_map)}", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="gray"))
                    fig.update_layout(height=600, width=None, margin=dict(l=50, r=50, t=50, b=50), autosize=True)  # Match container height
                    signal.alarm(0)  # Cancel timeout
                    return fig
                file_path = filepaths_map[file_key]
                print(f"DEBUG: Found file path: {file_path}")
            else:
                # For observed data, we don't need a file path from filepaths_map
                file_path = None
                print(f"DEBUG: Processing observed data - no file path needed")
            
            fig = go.Figure()

            # Determine what combinations to plot
            plot_combinations = []
            # FIXED: Include grid coordinates in combo key to prevent overwriting
            # Format: (var, model, freq, ssp, lat_idx, lon_idx)
            
            # Always start with the main selection
            main_combo = (variable, (model if not is_observed_main else 'Observed'), frequency, (ssp if not is_observed_main else 'historical'), lat_idx, lon_idx)
            plot_combinations.append(main_combo)
            print(f"DEBUG: Main combo: {main_combo[:4]} at grid ({lat_idx},{lon_idx})")
            
            # FIXED: Always include additional traces from main_plot_additional_traces
            try:
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent_traces = list(main_plot_persistent_traces.get() or [])
                
                # Combine current and persistent traces, avoiding duplicates
                all_traces = current_traces + persistent_traces
                seen_traces = set()
                unique_extra_traces = []
                
                for trace_cfg in all_traces:
                    trace_key = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'), trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                    if trace_key not in seen_traces:
                        seen_traces.add(trace_key)
                        unique_extra_traces.append(trace_cfg)
                
                if unique_extra_traces:
                    print(f"DEBUG: Found {len(unique_extra_traces)} additional traces to include in plot combinations")
                    
                    # Add additional traces to plot combinations
                    for trace_cfg in unique_extra_traces:
                        trace_var = trace_cfg.get('variable')
                        trace_model = trace_cfg.get('model')
                        trace_ssp = trace_cfg.get('ssp')
                        trace_freq = trace_cfg.get('frequency', 'annual')
                        trace_lat_idx = trace_cfg.get('lat_idx')
                        trace_lon_idx = trace_cfg.get('lon_idx')
                        
                        # FIXED: Include grid coordinates in combo tuple
                        trace_combo = (trace_var, trace_model, trace_freq, trace_ssp, trace_lat_idx, trace_lon_idx)
                        
                        # Check unit compatibility before adding
                        if trace_var and check_unit_compatibility(variable, trace_var, filepaths_map, frequency, trace_freq):
                            # Check if this exact combo (including grid) already exists
                            if trace_combo not in plot_combinations:
                                plot_combinations.append(trace_combo)
                                print(f"DEBUG: Added additional trace to plot combinations: {trace_var}, {trace_model}, {trace_freq}, {trace_ssp} at grid ({trace_lat_idx},{trace_lon_idx})")
                        else:
                            print(f"DEBUG: Skipping incompatible additional trace: {trace_var} ({trace_freq}) - incompatible with {variable} ({frequency})")
                else:
                    print(f"DEBUG: No additional traces found")
                    
            except Exception as e:
                print(f"DEBUG: Error processing additional traces for plot combinations: {e}")
            
            if multi_mode:
                print(f"DEBUG: Multi-plot mode enabled")
                
                # Preserve existing plots when multi-plot mode is enabled
                plots_list = multi_plots_list.get()
                if plots_list and multi_mode:
                    print(f"DEBUG: Preserving {len(plots_list)} stored multi-plots")
                    # Add compatible plots from multi_plots_list to plot_combinations
                    for plot_config in plots_list:
                        multi_var = plot_config.get('variable')
                        multi_model = plot_config.get('model')
                        multi_frequency = plot_config.get('frequency')
                        multi_ssp = plot_config.get('ssp')
                        
                        # Check unit compatibility before adding (with frequency check for precipitation)
                        if multi_var and check_unit_compatibility(variable, multi_var, filepaths_map, frequency, multi_frequency):
                            # FIXED: Check if the combo (without grid) already exists - multi_plots don't have grid stored
                            multi_combo_key = (multi_var, multi_model, multi_frequency, multi_ssp)
                            already_exists = any(combo[:4] == multi_combo_key for combo in plot_combinations)
                            if not already_exists:
                                # Add multi-plot at main grid (since multi_plots_list doesn't store grid coords)
                                multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
                                plot_combinations.append(multi_combo)
                                print(f"DEBUG: Adding compatible multi-plot: {multi_var}, {multi_model}, {multi_frequency}, {multi_ssp}")
                        else:
                            print(f"DEBUG: Skipping incompatible multi-plot: {multi_var} ({multi_frequency}) - incompatible with {variable} ({frequency})")
                
                # Check if multi-plot progressive system is completed
                multi_plot_completed = (multi_plot_step_1_completed.get() and 
                                      multi_plot_step_2_completed.get() and 
                                      multi_plot_step_3_completed.get() and 
                                      multi_plot_step_4_completed.get() and 
                                      multi_plot_step_5_completed.get() and
                                      multi_plot_step_6_completed.get())
                
                # Check if we have existing completed multi-plots that should be displayed
                plots_list = multi_plots_list.get()
                has_existing_plots = plots_list and len(plots_list) > 0
                
                # Also check if we have at least some multi-plot selections for new additions
                has_multi_selections = (multi_plot_variable.get() and 
                                      multi_plot_model.get() and 
                                      multi_plot_frequency.get() and
                                      multi_plot_ssp.get())
                
                print(f"DEBUG: Multi-plot completion status: {multi_plot_completed}")
                print(f"DEBUG: Multi-plot step completion: {multi_plot_step_1_completed.get()}, {multi_plot_step_2_completed.get()}, {multi_plot_step_3_completed.get()}, {multi_plot_step_4_completed.get()}, {multi_plot_step_5_completed.get()}, {multi_plot_step_6_completed.get()}")
                print(f"DEBUG: Has existing plots: {has_existing_plots}, Has multi-selections: {has_multi_selections}")
                
                # If we have existing plots, include them in the main plot
                if has_existing_plots:
                    print(f"DEBUG: Found {len(plots_list)} stored multi-plots - including them in main plot")
                    for plot_config in plots_list:
                        multi_var = plot_config.get('variable')
                        multi_model = plot_config.get('model')
                        multi_frequency = plot_config.get('frequency')
                        multi_ssp = plot_config.get('ssp')
                        
                        # Check unit compatibility before adding (with frequency check for precipitation)
                        if multi_var and check_unit_compatibility(variable, multi_var, filepaths_map, frequency, multi_frequency):
                            # FIXED: Check if the combo (without grid) already exists
                            multi_combo_key = (multi_var, multi_model, multi_frequency, multi_ssp)
                            already_exists = any(combo[:4] == multi_combo_key for combo in plot_combinations)
                            if not already_exists:
                                # Add existing multi-plot at main grid
                                multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
                                plot_combinations.append(multi_combo)
                                print(f"DEBUG: Adding existing multi-plot to main plot: {multi_var}, {multi_model}, {multi_frequency}, {multi_ssp}")
                        else:
                            print(f"DEBUG: Skipping incompatible existing plot: {multi_var} ({multi_frequency}) - incompatible with {variable} ({frequency})")
                
                if has_multi_selections:
                    # Get multi-plot selections
                    multi_var = multi_plot_variable.get()
                    multi_model = multi_plot_model.get()
                    multi_frequency = multi_plot_frequency.get()
                    multi_ssp = multi_plot_ssp.get()
                    multi_action = multi_plot_action.get()
                    
                    print(f"DEBUG: Multi-plot selections - var: {multi_var}, model: {multi_model}, frequency: {multi_frequency}, ssp: {multi_ssp}, action: {multi_action}")
                    
                    if multi_var and multi_model and multi_frequency and multi_ssp:
                        multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp)
                        
                        # Check unit compatibility (with frequency check for precipitation)
                        is_compatible = check_unit_compatibility(multi_var, variable, filepaths_map, multi_frequency, frequency)
                        print(f"DEBUG: Unit compatibility check (with freq) - {multi_var} ({multi_frequency}) vs {variable} ({frequency}): {is_compatible}")
                        
                        # Store the multi-plot info for later use in plotting
                        multi_plot_info = {
                            'combo': multi_combo,
                            'is_compatible': is_compatible,
                            'action': multi_action
                        }
                        
                        # Check unit compatibility and user action
                        if not is_compatible:
                            # Incompatible units - always separate plots
                            print(f"DEBUG: Incompatible units - main plot shows only main selection")
                        elif multi_action == "new_plot":
                            # User wants separate plot - main plot should only show main selection
                            # Additional plot will be handled by additional_plot function
                            print(f"DEBUG: User wants separate plot - main plot shows only main selection")
                        else:
                            # Compatible units and user wants same plot - handled by multi-plot UI, not the main plot
                            print(f"DEBUG: Compatible units, but main plot will not combine; use multi-plot UI instead")
                    else:
                        print(f"DEBUG: Multi-plot selections incomplete - var: {multi_var}, model: {multi_model}, ssp: {multi_ssp}")
                        multi_plot_info = None
                else:
                    print(f"DEBUG: No multi-plot selections found")
                    multi_plot_info = None
                
                # Check if we should include current multi-plot selection in main plot
                multi_plot_included = False
                if (multi_plot_info and multi_plot_info['is_compatible'] and 
                    multi_action != "new_plot" and multi_var and multi_model and multi_frequency and multi_ssp):
                    # User wants to combine on same plot - check if it's already in additional traces
                    current_multi_combo = (multi_var, multi_model, multi_frequency, multi_ssp)
                    
                    # Check if this selection already exists in additional traces (include grid in comparison)
                    already_in_additional = False
                    main_extra_traces = list(main_plot_additional_traces.get() or [])
                    # Resolve grids for comparison
                    multi_grid_id = multi_plot_grid_id.get()
                    main_grid_id = get_selected_grid_indices()
                    try:
                        multi_lat, multi_lon = map(int, multi_grid_id.split(",")) if multi_grid_id else (None, None)
                    except Exception:
                        multi_lat, multi_lon = (None, None)
                    for trace_cfg in main_extra_traces:
                        trace_combo = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'))
                        trace_grid = (trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                        if trace_combo == current_multi_combo and trace_grid == (multi_lat, multi_lon):
                            already_in_additional = True
                            print(f"DEBUG: Current multi-plot selection already exists in additional traces at same grid: {current_multi_combo} @ {trace_grid}")
                            break
                    
                    # FIXED: Check if combo+grid exists in plot_combinations
                    current_multi_combo_with_grid = (multi_var, multi_model, multi_frequency, multi_ssp, multi_lat, multi_lon)
                    same_combo_in_main = current_multi_combo_with_grid in plot_combinations

                    if same_combo_in_main or already_in_additional:
                        print("DEBUG: Skipping current multi-plot selection - identical to existing plotted series (combo + grid)")
                        multi_plot_included = False
                    else:
                        # If different grid than main, include; otherwise, let additional traces manage
                        # Do not include current multi-plot selection directly in plot_combinations
                        # Different-grid selections should be handled via additional traces to use their grid indices
                        multi_plot_included = False
                        print("DEBUG: Not including current multi-plot selection in plot_combinations; will be handled as additional trace")
                
                # FIXED: Only process additional traces if the current multi-plot selection wasn't already included
                # to prevent duplication
                if multi_plot_included:
                    print(f"DEBUG: Skipping additional traces processing - current multi-plot selection already included in plot_combinations")
                else:
                    print(f"DEBUG: Additional traces processing will handle any remaining multi-plot selections")
                
                # Check if we have additional traces with different frequencies for the same variable
                # FIXED: Only process additional traces if the current multi-plot selection wasn't already included
                additional_traces = []
                try:
                    main_extra_traces = list(main_plot_additional_traces.get() or [])
                    for trace_cfg in main_extra_traces:
                        trace_var = trace_cfg.get('variable')
                        trace_model = trace_cfg.get('model')
                        trace_ssp = trace_cfg.get('ssp')
                        trace_freq = trace_cfg.get('frequency', 'annual')  # Default to annual if not specified
                        
                        # FIXED: Skip only if this trace matches an existing plot combo including grid
                        trace_combo = (trace_var, trace_model, trace_freq, trace_ssp, trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                        if trace_combo in plot_combinations:
                            print(f"DEBUG: Skipping additional trace - already in plot combinations for same grid: {trace_var} ({trace_model}, {trace_freq}, {trace_ssp}) @ {trace_cfg.get('lat_idx')},{trace_cfg.get('lon_idx')}")
                            continue
                        
                        # FIXED: Include if units are compatible (allow different variables)
                        # Pass frequency information to properly check precipitation unit compatibility
                        if check_unit_compatibility(variable, trace_var, filepaths_map, frequency, trace_freq):
                            additional_traces.append(trace_combo)
                            print(f"DEBUG: Adding additional trace: {trace_var} ({trace_model}, {trace_freq}, {trace_ssp})")
                        else:
                            print(f"DEBUG: Skipping incompatible trace: {trace_var} (units not compatible with {variable} - freq: {frequency} vs {trace_freq})")
                except Exception as e:
                    print(f"DEBUG: Error processing additional traces: {e}")
                
                # FIXED: Don't add additional traces to plot combinations - they will be processed separately
                # Additional traces have their own grid coordinates and should be processed independently
                print(f"DEBUG: Found {len(additional_traces)} additional traces to process separately")
                

            else:
                # Single plot mode: include both observed and projected versions if available
                plot_combinations = []
                
                # Add the main selection with grid coordinates
                main_combo = (variable, (model if not is_observed_main else 'Observed'), frequency, (ssp if not is_observed_main else 'historical'), lat_idx, lon_idx)
                plot_combinations.append(main_combo)
                
                # Note: Removed automatic addition of observed/projected counterparts
                # Users should explicitly select what they want to plot
                
                print(f"DEBUG: Single plot mode - created {len(plot_combinations)} combinations for variable {variable}")

            print(f"DEBUG: Plot combinations to process: {plot_combinations}")

            # Load and plot data for each combination
            cached_datasets = _cached_datasets.get()
            plotted_data = []
            # No limit on combinations - allow unlimited traces as long as units are compatible
            print(f"DEBUG: Processing {len(plot_combinations)} plot combinations (no limit)")
            trace_index = 0  # Track which trace we're processing (0 = main, 1+ = additional)
            for var, mod, freq, scenario, combo_lat_idx, combo_lon_idx in plot_combinations:
                file_key = (var, mod, scenario)
                combo_key = (var, mod, freq, scenario, combo_lat_idx, combo_lon_idx)
                # Grid coordinates are now directly in the combination tuple
                print(f"DEBUG: Processing combination: {file_key} with frequency: {freq} at grid ({combo_lat_idx},{combo_lon_idx})")
                
                # Load dataset if not cached
                ds = cached_datasets.get(file_key)
                if ds is None:
                    if var in ("obs_tmin","obs_tmax","obs_pr"):
                        # Load observed dataset via discovered paths
                        try:
                            if var == "obs_pr":
                                obs_fp = obs_prec_path.get()
                            elif var == "obs_tmax":
                                obs_fp = obs_tmax_path.get()
                            else:
                                obs_fp = obs_tmin_path.get()
                        except Exception:
                            obs_fp = None
                        if not obs_fp or not os.path.exists(obs_fp):
                            print(f"DEBUG: Observed dataset path missing for {var}")
                            continue
                        try:
                            with xr.open_dataset(obs_fp, engine="netcdf4") as temp_ds:
                                ds = temp_ds.load()
                            current_cache = _cached_datasets.get()
                            current_cache[file_key] = ds
                            _cached_datasets.set(current_cache)
                            print(f"DEBUG: Loaded observed dataset for {var}")
                        except Exception as e:
                            print(f"DEBUG: Error loading observed dataset {var}: {e}")
                            continue
                    else:
                        # FIXED: Load projection data - keep all logic inside else block
                        if file_key not in filepaths_map:
                            print(f"DEBUG: File key not in filepaths_map: {file_key}")
                            continue
                        file_path = filepaths_map[file_key]
                        print(f"DEBUG: Loading dataset from: {file_path}")
                        try:
                            with xr.open_dataset(file_path, engine="netcdf4") as temp_ds:
                                ds = temp_ds.load()
                            current_cache = _cached_datasets.get()
                            if len(current_cache) > 10:
                                oldest_keys = list(current_cache.keys())[:-10]
                                for old_key in oldest_keys:
                                    del current_cache[old_key]
                                print(f"DEBUG: Removed {len(oldest_keys)} old datasets from cache")
                            current_cache[file_key] = ds
                            _cached_datasets.set(current_cache)
                            print(f"DEBUG: Successfully loaded and cached dataset")
                        except Exception as e:
                            print(f"DEBUG: Error loading dataset: {e}")
                            continue

                if ds is None:
                    print(f"DEBUG: Dataset is None for {file_key}")
                    continue

                if var in ("obs_tmin","obs_tmax","obs_pr"):
                    # Map selected projection grid center to most encompassing observed grid index
                    # FIXED: Use combo-specific grid coordinates to find the right projection gridbox
                    try:
                        # FIXED: Construct grid_id from combo-specific coordinates (this trace's grid)
                        grid_id_str = f"{combo_lat_idx},{combo_lon_idx}"
                        target_lat = target_lon = None
                        grid_geo = reactive_grid_geojson.get()
                        if grid_geo and grid_id_str:
                            features = grid_geo['features'] if isinstance(grid_geo, dict) else grid_geo.features
                            for feat in features:
                                fid = feat.get('id') or (feat.get('properties', {}) or {}).get('id')
                                if fid == grid_id_str:
                                    props = feat.get('properties', {}) if isinstance(feat, dict) else feat.properties
                                    target_lat = float(props.get('lat')) if props and 'lat' in props else None
                                    target_lon = float(props.get('lon')) if props and 'lon' in props else None
                                    break
                        
                        # Find the most encompassing observed grid cell
                        if target_lat is not None and target_lon is not None:
                            obs_lat_idx, obs_lon_idx = _find_most_encompassing_obs_gridbox(target_lat, target_lon, ds)
                            if obs_lat_idx is not None and obs_lon_idx is not None:
                                use_lat_idx, use_lon_idx = obs_lat_idx, obs_lon_idx
                                print(f"DEBUG: Using most encompassing observed grid coordinates ({use_lat_idx}, {use_lon_idx}) for variable {var} from projection grid ({combo_lat_idx}, {combo_lon_idx})")
                            else:
                                # Fallback to center of observed grid
                                lat2d, lon2d, _, _ = _extract_spatial_coords(ds)
                                if lat2d is not None and lon2d is not None:
                                    use_lat_idx = lat2d.shape[0] // 2
                                    use_lon_idx = lat2d.shape[1] // 2
                                    print(f"DEBUG: Using center of observed grid at ({use_lat_idx}, {use_lon_idx})")
                                else:
                                    use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx
                        else:
                            # If we can't find target coordinates, this should not happen since
                            # the dropdown filtering should prevent this situation
                            print(f"ERROR: No valid observational gridbox found for grid {grid_id_str} - this should have been filtered out in dropdown")
                            continue
                    except Exception as e:
                        print(f"DEBUG: Error mapping observed grid coordinates: {e}")
                        # FIXED: Use combo-specific coordinates as fallback
                        use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx
                else:
                    # FIXED: Use combo-specific grid coordinates
                    if not (0 <= combo_lat_idx < ds['lat'].size and 0 <= combo_lon_idx < ds['lon'].size):
                        print(f"DEBUG: Indices out of bounds - combo_lat_idx: {combo_lat_idx}, combo_lon_idx: {combo_lon_idx}, ds lat size: {ds['lat'].size}, ds lon size: {ds['lon'].size}")
                        continue
                    use_lat_idx, use_lon_idx = combo_lat_idx, combo_lon_idx

                # Extract time series with error handling
                try:
                    print(f"DEBUG: Extracting time series for variable: {var}")
                    if var in ("obs_tmin","obs_tmax","obs_pr"):
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        extract_var = data_vars[0] if data_vars else None
                        if not extract_var:
                            print("DEBUG: No data variable in observed dataset")
                            continue
                        df_raw = extract_timeseries(ds, use_lat_idx, use_lon_idx, variable_name=extract_var)
                        # Normalize column name to observed alias for downstream processing
                        if extract_var in df_raw.columns:
                            df_raw.rename(columns={extract_var: var}, inplace=True)
                    else:
                        df_raw = extract_timeseries(ds, use_lat_idx, use_lon_idx, variable_name=var)
                    print(f"DEBUG: Extracted time series shape: {df_raw.shape}")
                except Exception as e:
                    print(f"DEBUG: Error extracting time series: {e}")
                    continue

                if df_raw.empty or var not in df_raw.columns:
                    print(f"DEBUG: No data in extracted time series - empty: {df_raw.empty}, columns: {list(df_raw.columns)}")
                    continue

                # Process data with memory management
                try:
                    df_processed = df_raw.copy()
                    display_name = get_variable_display_name(var)
                    display_units = get_variable_metadata(var)["units"]
                    # Generate distinct colors for multiple plots
                    base_color = get_variable_metadata(var)["color"]
                    # Create a distinct color by modifying the base color based on the combination
                    import colorsys
                    try:
                        if base_color.startswith('#'):
                            rgb = tuple(int(base_color[i:i+2], 16) / 255.0 for i in (1, 3, 5))
                            h, s, v = colorsys.rgb_to_hsv(*rgb)
                            # Shift hue based on the combination to ensure distinctness
                            hue_shift = (hash(f"{var}_{mod}_{freq}_{scenario}") % 360) / 360.0
                            h = (h + hue_shift * 0.3) % 1.0  # Shift hue by up to 30%
                            s = min(1.0, s * 1.1)  # Slightly increase saturation
                            v = min(1.0, v * 0.9)  # Slightly darker
                            rgb_new = colorsys.hsv_to_rgb(h, s, v)
                            line_color = f"rgb({int(rgb_new[0]*255)}, {int(rgb_new[1]*255)}, {int(rgb_new[2]*255)})"
                        else:
                            line_color = base_color
                    except:
                        line_color = base_color

                    # Unit conversions (data conversion only - display_units already set from metadata)
                    if var in ["tasmin", "tasmax"]:
                        original_units = ds[var].attrs.get('units', '').lower()
                        if original_units in ['k', 'kelvin']:
                            df_processed[var] = (df_processed[var] - 273.15) * 9/5 + 32
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "pr":
                        original_units = ds[var].attrs.get('units', '').lower()
                        if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var in ("obs_tmin","obs_tmax"):
                        # Observed temperature to °F if needed
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        print(f"DEBUG: Observed temperature units: {src_units}")
                        
                        # Livneh observed data is in Celsius - always convert to Fahrenheit
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample value: {avg_value:.2f}")
                        
                        # Convert from Celsius to Fahrenheit for observed data
                        if src_units in ['c', 'degc', 'celsius', '°c', 'deg_c', 'degrees_celsius']:
                            print(f"DEBUG: Units explicitly indicate Celsius - converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] * 9/5) + 32
                            print(f"DEBUG: Sample values after Celsius conversion: {df_processed[var].head().tolist()}")
                        elif src_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                            print(f"DEBUG: Units explicitly indicate Kelvin - converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] - 273.15) * 9/5 + 32
                            print(f"DEBUG: Sample values after Kelvin conversion: {df_processed[var].head().tolist()}")
                        elif src_units in ['f', 'degf', 'fahrenheit', '°f', 'deg_f', 'degrees_fahrenheit']:
                            print(f"DEBUG: Units explicitly indicate Fahrenheit - no conversion needed")
                            # Data is already in Fahrenheit, no conversion needed
                        else:
                            # For Livneh observed data, if no units specified, assume Celsius (as confirmed by user)
                            print(f"DEBUG: No units specified - Livneh data is in Celsius, converting to Fahrenheit")
                            df_processed[var] = (df_processed[var] * 9/5) + 32
                            print(f"DEBUG: Sample values after Celsius conversion: {df_processed[var].head().tolist()}")
                        # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "obs_pr":
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        if src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                            df_processed[var] = df_processed[var] / 25.4
                            print(f"DEBUG: Converted from mm/day to inches")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            print(f"DEBUG: Converted from kg m-2 s-1 to inches")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # For Livneh observed data, if no units specified, assume mm/day (as confirmed by user)
                            print(f"DEBUG: No units specified - Livneh precipitation data is in mm/day, converting to inches")
                            df_processed[var] = df_processed[var] / 25.4
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "pr":
                        # Projected precipitation conversion - typically in kg m-2 s-1
                        try:
                            # Get the actual data variable name from the dataset
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample projected precipitation values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample projected precipitation value: {avg_value:.2f}")
                        print(f"DEBUG: Projected precipitation units: {src_units}")
                        
                        if src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            print(f"DEBUG: Converted projected precipitation from kg m-2 s-1 to inches")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                            df_processed[var] = df_processed[var] / 25.4
                            print(f"DEBUG: Converted projected precipitation from mm/day to inches")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume kg m-2 s-1 for projected precipitation
                            print(f"DEBUG: No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                            df_processed[var] = df_processed[var] * 86400 / 25.4
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var in ["hursmax", "hursmin"]:
                        # Relative humidity conversion - typically in % but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        print(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['%', 'percent', 'pct']:
                            # Already in percentage, no conversion needed
                            print(f"DEBUG: {var} already in percentage - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                            # Convert from fraction to percentage
                            df_processed[var] = df_processed[var] * 100
                            print(f"DEBUG: Converted {var} from fraction to percentage")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume percentage
                            print(f"DEBUG: No units specified - assuming {var} is in percentage")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "huss":
                        # Specific humidity conversion - typically in kg/kg but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        print(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                            # Already in kg/kg, no conversion needed
                            print(f"DEBUG: {var} already in kg/kg - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['g/kg', 'g kg-1', 'g per kg']:
                            # Convert from g/kg to kg/kg
                            df_processed[var] = df_processed[var] / 1000
                            print(f"DEBUG: Converted {var} from g/kg to kg/kg")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume kg/kg
                            print(f"DEBUG: No units specified - assuming {var} is in kg/kg")
                            # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "rsds":
                        # Radiation - typically already in W/m² (no conversion needed)
                        # NOTE: Climate model radiation data is standardized to W/m² (watts per square meter)
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        print(f"DEBUG: {var} units: {src_units}")
                        
                        # Radiation is typically already in W/m² - no conversion needed
                        # If data were in J/m²/day (joules per square meter per day), we'd divide by 86400
                        # But standard climate models use W/m² directly
                        print(f"DEBUG: {var} assumed to be in W/m² (standard for climate models) - no conversion needed")
                        # display_units already set from get_variable_metadata() - keep consistent
                    elif var == "wspeed":
                        # Wind speed conversion - typically in m/s but may need conversion
                        try:
                            data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                            if data_vars:
                                src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                            else:
                                src_units = ''
                        except Exception:
                            src_units = ''
                        
                        sample_values = df_processed[var].dropna().head(10).tolist()
                        avg_value = np.mean(sample_values) if sample_values else 0
                        print(f"DEBUG: Sample {var} values before conversion: {sample_values}")
                        print(f"DEBUG: Average sample {var} value: {avg_value:.2f}")
                        print(f"DEBUG: {var} units: {src_units}")
                        
                        if src_units in ['m/s', 'm s-1', 'meter/s', 'meter s-1', 'ms-1']:
                            # Already in m/s, no conversion needed
                            print(f"DEBUG: {var} already in m/s - no conversion needed")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['km/h', 'kmh', 'km per hour']:
                            # Convert from km/h to m/s
                            df_processed[var] = df_processed[var] * 1000 / 3600
                            print(f"DEBUG: Converted {var} from km/h to m/s")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        elif src_units in ['knots', 'kt', 'kts']:
                            # Convert from knots to m/s
                            df_processed[var] = df_processed[var] * 0.514444
                            print(f"DEBUG: Converted {var} from knots to m/s")
                            print(f"DEBUG: Sample values after conversion: {df_processed[var].head().tolist()}")
                            # display_units already set from get_variable_metadata() - keep consistent
                        else:
                            # Default: assume m/s
                            print(f"DEBUG: No units specified - assuming {var} is in m/s")
                            # display_units already set from get_variable_metadata() - keep consistent

                    if var in df_processed.columns and not df_processed[var].isnull().all():
                        try:
                            # Process data based on selected frequency
                            if freq == 'daily':
                                # Use daily data as-is, but create a proper year column for filtering
                                df_freq = df_processed.copy()
                                df_freq["Year"] = df_freq["Date"].dt.year
                                # For daily data, we'll use the actual date for x-axis but filter by year
                                print(f"DEBUG: Using daily data with {len(df_freq)} rows")
                            elif freq == 'monthly':
                                # For precipitation, sum to get monthly totals; for other variables, average
                                if var in ["pr", "obs_pr"]:
                                    df_freq = df_processed.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                                    print(f"DEBUG: Created monthly total precipitation data with {len(df_freq)} rows")
                                else:
                                    df_freq = df_processed.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                                    print(f"DEBUG: Created monthly average data with {len(df_freq)} rows")
                                # Update units based on frequency
                                display_units = get_variable_units_for_frequency(var, freq)
                                df_freq["Year"] = df_freq["Date"].dt.year
                            elif freq == 'annual':
                                # For precipitation, sum to get annual totals; for other variables, average
                                if var in ["pr", "obs_pr"]:
                                    df_freq = df_processed.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                                    print(f"DEBUG: Created annual total precipitation data with {len(df_freq)} rows")
                                else:
                                    df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                    print(f"DEBUG: Created annual average data with {len(df_freq)} rows")
                                # Update units based on frequency
                                display_units = get_variable_units_for_frequency(var, freq)
                                df_freq["Year"] = df_freq["Date"].dt.year
                            else:
                                # Default to annual if frequency not specified
                                df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                df_freq["Year"] = df_freq["Date"].dt.year
                                print(f"DEBUG: Defaulted to annual averages with {len(df_freq)} rows")
                        except Exception as freq_error:
                            print(f"DEBUG: Error creating frequency data: {freq_error}")
                            continue
                        
                        # Filter by time series range
                        try:
                            # Use the appropriate slider based on trace index:
                            # trace_index = 0 -> main slider (time_series_range_main)
                            # trace_index = 1+ -> additional trace slider (time_series_range_extra_{trace_index-1})
                            time_range = None
                            if trace_index == 0:
                                # Main trace - use main slider
                                try:
                                    time_range = input.time_series_range_main()
                                except:
                                    time_range = None
                            else:
                                # Additional trace - use its specific slider
                                try:
                                    extra_index = trace_index - 1
                                    slider_id = f"time_series_range_extra_{extra_index}"
                                    time_range = getattr(input, slider_id)()
                                except:
                                    # Fallback to main slider if individual slider doesn't exist
                                    try:
                                        time_range = input.time_series_range_main()
                                    except:
                                        time_range = None
                            
                            if time_range and len(time_range) == 2:
                                plot_start_year, plot_end_year = time_range
                                # For observed data, ensure the range is within the available data range (1950-2013)
                                if var in ("obs_tmin", "obs_tmax", "obs_pr"):
                                    plot_start_year = max(1950, plot_start_year)
                                    plot_end_year = min(2013, plot_end_year)
                                # Validate that the range makes sense for the scenario
                                elif scenario == "historical" and plot_start_year >= 2015:
                                    # If historical scenario but slider shows future range, use historical defaults
                                    plot_start_year, plot_end_year = 1950, 2014
                                elif scenario != "historical" and plot_start_year < 2015:
                                    # If future scenario but slider shows historical range, use future defaults
                                    plot_start_year, plot_end_year = 2015, 2100
                            else:
                                # Use scenario-based defaults when slider is not available
                                if var in ("obs_tmin", "obs_tmax", "obs_pr"):
                                    plot_start_year, plot_end_year = 1950, 2013
                                elif scenario == "historical":
                                    plot_start_year, plot_end_year = 1950, 2014
                                else:
                                    plot_start_year, plot_end_year = 2015, 2100
                            
                            # Filter based on frequency type
                            if freq == 'daily':
                                # For daily data, filter by date range
                                start_date = pd.Timestamp(f"{plot_start_year}-01-01")
                                end_date = pd.Timestamp(f"{plot_end_year}-12-31")
                                df_freq = df_freq[(df_freq["Date"] >= start_date) & (df_freq["Date"] <= end_date)]
                                print(f"DEBUG: Filtered daily data to date range {start_date.date()}-{end_date.date()}, {len(df_freq)} rows remaining")
                            else:
                                # For monthly and annual data, filter by year range
                                df_freq = df_freq[(df_freq["Year"] >= plot_start_year) & (df_freq["Year"] <= plot_end_year)]
                                print(f"DEBUG: Filtered {freq} data to time range {plot_start_year}-{plot_end_year}, {len(df_freq)} rows remaining")
                        except Exception as filter_error:
                            print(f"DEBUG: Error filtering time series: {filter_error}")
                            pass
                        
                        # Create legend name - FIXED: Show projection grid ID for ALL data types
                        # For observational data, show the projection grid ID (what user selected)
                        # For projection data, show the projection grid ID (same as extraction coordinates)
                        try:
                            # FIXED: Always use combo_lat_idx, combo_lon_idx for legend (projection grid coordinates)
                            legend_grid_lat_idx = combo_lat_idx
                            legend_grid_lon_idx = combo_lon_idx
                        except Exception:
                            legend_grid_lat_idx = None
                            legend_grid_lon_idx = None
                        
                        # Always include grid coordinates in legend for clarity - FULL DETAILS, NO TRUNCATION
                        if legend_grid_lat_idx is not None and legend_grid_lon_idx is not None:
                            if var in ("obs_tmin","obs_tmax","obs_pr"):
                                legend_name = f"{display_name} (Observed, {freq}, historical) - Grid {legend_grid_lat_idx},{legend_grid_lon_idx}"
                            else:
                                legend_name = f"{display_name} ({mod}, {freq}, {scenario}) - Grid {legend_grid_lat_idx},{legend_grid_lon_idx}"
                        else:
                            legend_name = f"{display_name} (Observed, {freq}, historical)" if var in ("obs_tmin","obs_tmax","obs_pr") else f"{display_name} ({mod}, {freq}, {scenario})"
                        
                        if not df_freq.empty:
                            print(f"DEBUG: Adding trace for {legend_name}")
                            # Use appropriate x-axis based on frequency for proper visualization
                            if freq == 'daily':
                                # For daily data, use years for x-axis (convert dates to years)
                                x_values = pd.to_datetime(df_freq["Date"]).dt.year + (pd.to_datetime(df_freq["Date"]).dt.dayofyear - 1) / 365.25
                            elif freq == 'monthly':
                                # For monthly data, use years for x-axis (convert dates to years with fractional months)
                                x_values = df_freq["Year"] + (pd.to_datetime(df_freq["Date"]).dt.month - 1) / 12.0
                            else:
                                # For annual data, use integer years
                                x_values = df_freq["Year"]
                            
                            # Generate distinct color for this trace using trace_index for guaranteed uniqueness
                            import colorsys
                            try:
                                # Use trace_index to ensure each trace gets a unique color
                                # Define a palette of highly distinguishable colors
                                color_palette = [
                                    "rgb(31, 119, 180)",    # Blue
                                    "rgb(255, 127, 14)",    # Orange
                                    "rgb(44, 160, 44)",     # Green
                                    "rgb(214, 39, 40)",     # Red
                                    "rgb(148, 103, 189)",   # Purple
                                    "rgb(140, 86, 75)",     # Brown
                                    "rgb(227, 119, 194)",   # Pink
                                    "rgb(127, 127, 127)",   # Gray
                                    "rgb(188, 189, 34)",    # Olive
                                    "rgb(23, 190, 207)",    # Cyan
                                    "rgb(255, 187, 120)",   # Light Orange
                                    "rgb(152, 223, 138)",   # Light Green
                                    "rgb(255, 152, 150)",   # Light Red
                                    "rgb(197, 176, 213)",   # Light Purple
                                    "rgb(196, 156, 148)",   # Light Brown
                                    "rgb(247, 182, 210)",   # Light Pink
                                    "rgb(199, 199, 199)",   # Light Gray
                                    "rgb(219, 219, 141)",   # Light Olive
                                    "rgb(158, 218, 229)",   # Light Cyan
                                    "rgb(65, 68, 81)",      # Dark Blue-Gray
                                ]
                                
                                # Use trace_index to cycle through the palette
                                if trace_index < len(color_palette):
                                    distinct_color = color_palette[trace_index]
                                else:
                                    # For traces beyond the palette, generate colors using HSV
                                    # Distribute hues evenly across the color wheel for maximum distinction
                                    hue = (trace_index * 0.618033988749895) % 1.0  # Golden ratio for better distribution
                                    saturation = 0.75 + (trace_index % 3) * 0.08  # Vary saturation slightly
                                    value = 0.85 - (trace_index % 4) * 0.05  # Vary brightness slightly
                                    rgb = colorsys.hsv_to_rgb(hue, saturation, value)
                                    distinct_color = f"rgb({int(rgb[0]*255)}, {int(rgb[1]*255)}, {int(rgb[2]*255)})"
                            except Exception as color_error:
                                print(f"DEBUG: Error generating color for trace {trace_index}: {color_error}")
                                # Fallback to a simple color cycle
                                fallback_colors = ["blue", "red", "green", "orange", "purple", "brown", "pink", "gray", "olive", "cyan"]
                                distinct_color = fallback_colors[trace_index % len(fallback_colors)]
                            
                            fig.add_trace(go.Scatter(
                                x=x_values, 
                                y=df_freq[var], 
                                mode="lines", 
                                name=legend_name, 
                                line=dict(color=distinct_color, width=2)
                            ))
                            plotted_data.append(legend_name)
                            # Optional trendline, controlled by individual checkbox for each trace
                            try:
                                # Main trace (trace_index == 0) uses trendline_main checkbox
                                # Additional traces (trace_index >= 1) use show_trendline_extra_{trace_index - 1} checkbox
                                if trace_index == 0:
                                    show_trend = input.trendline_main()
                                else:
                                    # For additional traces, use their individual checkbox
                                    checkbox_id = f"show_trendline_extra_{trace_index - 1}"
                                    show_trend = getattr(input, checkbox_id, lambda: False)()
                            except Exception as e:
                                print(f"DEBUG: Error reading trendline checkbox for trace {trace_index}: {e}")
                                show_trend = False
                            
                            if show_trend:
                                tl = compute_trendline(x_values, df_freq[var].values)
                                if tl is not None:
                                    slope, intercept, x_line, y_line = tl
                                    trend_name = f"{legend_name} (trend)"
                                    fig.add_trace(go.Scatter(
                                        x=x_line,
                                        y=y_line,
                                        mode="lines",
                                        name=trend_name,
                                        line=dict(color=distinct_color, width=2, dash="dash"),
                                        showlegend=False  # Don't show trendline in legend to avoid duplicates
                                    ))
                                    plotted_data.append(trend_name)
                        else:
                            print(f"DEBUG: Frequency data is empty for {legend_name}")
                except Exception as e:
                    print(f"DEBUG: Error processing data for {var}: {e}")
                    continue
                
                # Increment trace index for next iteration
                trace_index += 1

            print(f"DEBUG: Total plotted data series: {len(plotted_data)}")
            print(f"DEBUG: Plot data series: {plotted_data}")
            
            # Keep grid coordinates in legend names for clarity
            # No post-processing needed

            # Additional traces are now processed earlier in plot_combinations
            # No need to process them again here

            # Get grid coordinates for title (moved outside conditional)
            grid_id = selected_grid_id.get()
            grid_coords = ""
            if grid_id:
                try:
                    lat_idx, lon_idx = grid_id.split(",")
                    grid_coords = f" (Grid: {lat_idx}, {lon_idx})"
                except:
                    grid_coords = ""

            if not plotted_data:
                print(f"DEBUG: No data plotted, showing error message")
                fig.add_annotation(text="No data available for the selected combinations.<br>Try selecting different options.", showarrow=False, x=0.5, y=0.5, font=dict(size=14, color="gray"))
                fig.update_layout(height=600, margin=dict(l=50, r=50, t=50, b=50))  # Match container height
            
            # Set up plot layout based on multi-plot mode and unit compatibility
            if multi_mode and len(plot_combinations) > 1:
                # Check if we have incompatible units
                main_var_meta = get_variable_metadata(variable)
                main_units = main_var_meta["units"]
                # Adjust units for aggregated precipitation
                if variable in ["pr", "obs_pr"] and frequency in ["monthly", "annual"]:
                    main_units = "inches"
                
                # Group by units
                plots_by_units = {}
                for combo in plot_combinations:
                    combo_var = combo[0]
                    combo_meta = get_variable_metadata(combo_var)
                    combo_units = combo_meta["units"]
                    
                    if combo_units not in plots_by_units:
                        plots_by_units[combo_units] = []
                    plots_by_units[combo_units].append(combo)
                
                if len(plots_by_units) > 1:
                    # Incompatible units - main plot should only show main selection
                    # Additional plot will be handled by additional_plot function
                    title = f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}){grid_coords}"
                    
                    fig.update_layout(
                        title=title,
                        xaxis_title="Year",
                        yaxis_title=main_units, 
                        hovermode="x unified",
                        showlegend=True,  # FIXED: Explicitly enable legend
                        legend=dict(
                            orientation="h",  # Horizontal below plot for unlimited traces
                            yanchor="top",
                            y=-0.12,  # Below plot area
                            xanchor="center",
                            x=0.5,  # Centered below plot
                            bgcolor="rgba(255,255,255,0.95)",
                            bordercolor="rgba(0,0,0,0.3)",
                            borderwidth=1,
                            font=dict(size=9),  # Compact font for many traces
                            itemsizing="constant",
                            itemclick="toggle",  # Click to toggle traces
                            itemdoubleclick="toggleothers",  # Double-click to isolate
                            tracegroupgap=5,  # Small gap between items
                            traceorder="normal"  # Keep trace order
                        ),
                        margin=dict(l=60, t=80, r=60, b=300),  # Large bottom margin for unlimited traces legend wrapping
                        # FIXED: Constant plot height matching container
                        height=600,  # Match the 350px container height
                        xaxis=dict(
                            showgrid=True,
                            gridwidth=1,
                            gridcolor='lightgray',
                            tickmode='auto',
                            nticks=10,
                            tickangle=0,
                            range=[1950, 2100],  # Force full range from 1950 to 2100
                            autorange=False,  # Disable auto-ranging to force our range
                            constrain='domain',
                            fixedrange=False,  # Allow zooming
                            side='bottom',
                            showline=True,
                            linewidth=1,
                            linecolor='black',
                            mirror=True,
                            rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                        ),
                        autosize=True,  # Let Plotly auto-size
                        width=None,  # Let Plotly auto-size
                        uirevision=True  # Maintain zoom/pan state
                    )
                else:
                    # All data has same units - check if user wants separate plots
                    if multi_plot_info and multi_plot_info['action'] == 'new_plot':
                        # User wants separate plot - main plot should only show main selection
                        title = f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}){grid_coords}"
                    else:
                        # Combine on same plot
                        title = f"{get_variable_display_name(variable)}{grid_coords}"
                    
                    fig.update_layout(
                        title=title,
                        xaxis_title="Year", 
                        yaxis_title=main_units, 
                        xaxis=dict(
                            range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                            automargin=True,
                            dtick=10,  # Show ticks every 10 years
                            tick0=1950,  # Start ticks at 1950
                            tickmode='linear',  # Linear tick spacing
                            rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                        ),
                        yaxis=dict(
                            autorange=True,  # Automatically adjust y-axis to show full data range
                            automargin=True  # Automatically adjust margins
                        ),
                        height=600,  # Match the 350px container height
                        width=None,
                        hovermode="x unified",
                        showlegend=True,  # FIXED: Explicitly enable legend
                        legend=dict(
                            orientation="h",  # Horizontal below plot for unlimited traces
                            yanchor="top",
                            y=-0.12,  # Below plot area
                            xanchor="center",
                            x=0.5,  # Centered below plot
                            bgcolor="rgba(255,255,255,0.95)",
                            bordercolor="rgba(0,0,0,0.3)",
                            borderwidth=1,
                            font=dict(size=9),  # Compact font for many traces
                            itemsizing="constant",
                            itemclick="toggle",  # Click to toggle traces
                            itemdoubleclick="toggleothers",  # Double-click to isolate
                            tracegroupgap=5,  # Small gap between items
                            traceorder="normal"  # Keep trace order
                        ),
                        margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                    )
            else:
                # Single plot mode or compatible units
                # Set initial layout without title (we'll set it after all traces are added)
                x_axis_title = "Year"  # Always label x-axis in years per preference
                
                # Determine correct y-axis units based on variable and frequency
                base_units = get_variable_metadata(variable)["units"]
                if variable in ["pr", "obs_pr"] and frequency in ["monthly", "annual"]:
                    # For aggregated precipitation, use "inches" instead of "inches/day"
                    y_axis_units = "inches"
                else:
                    y_axis_units = base_units
                
                fig.update_layout(
                    xaxis_title=x_axis_title, 
                    yaxis_title=y_axis_units, 
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    width=None,
                    hovermode="x unified",
                    showlegend=True,  # FIXED: Explicitly enable legend
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    ),
                    margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                )

            # Add a simple test trace to ensure the plot is working
            if len(fig.data) == 0:
                print(f"DEBUG: No traces in figure, adding test trace")
                fig.add_trace(go.Scatter(
                    x=[2020, 2021, 2022, 2023, 2024],
                    y=[1, 2, 3, 4, 5],
                    mode="lines",
                    name="Test Plot",
                    line=dict(color="red")
                ))
                fig.update_layout(
                    title="Test Plot - No Data Available",
                    xaxis_title="Year",
                    yaxis_title="Test Values",
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        rangeslider=dict(visible=False)  # Disable built-in range slider for cleaner layout
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    width=None,
                    margin=dict(l=60, t=80, r=60, b=300),  # Large bottom margin for unlimited traces legend wrapping
                    showlegend=True,  # FIXED: Explicitly enable legend
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    )
                )
            
            # FIXED: Set the title after all traces are added
            if len(fig.data) > 1:
                # Multiple plots - show just the variable name (no grid coordinates)
                title = f"{get_variable_display_name(variable)}"
                # Always keep x-axis title as Years
                x_axis_title = "Year"
            else:
                # Single plot - show descriptive title with frequency information
                if is_observed_main:
                    # For observed data, show frequency in title
                    if frequency == "daily":
                        frequency_text = "Daily"
                    elif frequency == "monthly":
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Monthly Total"
                        else:
                            frequency_text = "Monthly Average"
                    else:  # annual
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Annual Total"
                        else:
                            frequency_text = "Annual Average"
                    title = f"{frequency_text} {get_variable_display_name(variable)}{grid_coords}"
                else:
                    # For model data, show frequency in title
                    if frequency == "daily":
                        frequency_text = "Daily"
                    elif frequency == "monthly":
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Monthly Total"
                        else:
                            frequency_text = "Monthly Average"
                    else:  # annual
                        # For precipitation, use "Total" instead of "Average"
                        if variable in ["pr", "obs_pr"]:
                            frequency_text = "Annual Total"
                        else:
                            frequency_text = "Annual Average"
                    title = f"{frequency_text} {get_variable_display_name(variable)} ({model}, {ssp}){grid_coords}"
                x_axis_title = "Year"
            
            # FIXED: Only update title, don't override legend or height settings
            fig.update_layout(
                title=title,
                xaxis_title=x_axis_title
            )
            
            print(f"DEBUG: Returning figure with {len(fig.data)} traces")
            signal.alarm(0)  # Cancel timeout
            return fig
        except TimeoutError:
            print("DEBUG: Plot generation timed out")
            signal.alarm(0)  # Cancel timeout
            fig = go.Figure()
            fig.add_annotation(text="Plot generation timed out. Please try again with fewer combinations.", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="orange"))
            fig.update_layout(height=600, autosize=True, margin=dict(l=60, t=80, r=60, b=100))  # Match container height
            return fig
        except Exception as e:
            print(f"ERROR in selected_gridbox_plot: {type(e).__name__}: {e}")
            print(f"DEBUG: Full traceback: {traceback.format_exc()}")
            signal.alarm(0)  # Cancel timeout
            # Return a simple error plot
            fig = go.Figure()
            fig.add_annotation(text=f"Error generating plot: {str(e)}", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color="red"))
            fig.update_layout(height=600, autosize=True, margin=dict(l=60, t=80, r=60, b=100))  # Match container height
            return fig
    
    @output
    @render.ui
    def additional_traces_controls():
        """Render time series sliders and trendline checkboxes for additional traces"""
        print("DEBUG: additional_traces_controls function called")
        try:
            # Get additional traces from both current and persistent storage with error handling
            current_traces = []
            persistent_traces = []
            
            try:
                current_traces = list(main_plot_additional_traces.get() or [])
            except Exception as e:
                print(f"DEBUG: Error getting current traces: {e}")
                current_traces = []
                
            try:
                persistent_traces = list(main_plot_persistent_traces.get() or [])
            except Exception as e:
                print(f"DEBUG: Error getting persistent traces: {e}")
                persistent_traces = []
            
            # Combine and deduplicate traces safely
            all_traces = current_traces + persistent_traces
            seen_traces = set()
            unique_traces = []
            
            for tr in all_traces:
                try:
                    if not tr or not isinstance(tr, dict):
                        continue
                    trace_key = (tr.get('variable'), tr.get('model'), tr.get('ssp'), tr.get('lat_idx'), tr.get('lon_idx'))
                    if trace_key not in seen_traces and all(k is not None for k in trace_key):
                        seen_traces.add(trace_key)
                        unique_traces.append(tr)
                except Exception as e:
                    print(f"DEBUG: Error processing trace: {e}")
                    continue
            
            # Get main plot information for inclusion in the table
            main_plot_info = None
            try:
                selections = get_plot_selections()
                if selections and selections.get('variable'):
                    main_plot_info = {
                        'variable': selections['variable'],
                        'model': selections['model'],
                        'frequency': selections['frequency'],
                        'ssp': selections['ssp'],
                        'grid_indices': selections['grid_indices']
                    }
            except Exception as e:
                print(f"DEBUG: Error getting main plot info: {e}")
            
            # If no additional traces and no main plot, return empty message
            if not unique_traces and not main_plot_info:
                return ui.div("No traces to display", class_="text-muted mt-3")
            
            print(f"DEBUG: Rendering {len(unique_traces)} additional trace controls + main plot")
            print(f"DEBUG: Additional trace controls - unique_traces: {unique_traces}")
            print(f"DEBUG: Main plot info: {main_plot_info}")
            
            # Create a more stable container structure with performance optimization
            container_elements = []
            
            # Add main plot controls as the first row if available
            if main_plot_info:
                try:
                    main_var = main_plot_info['variable']
                    main_model = main_plot_info['model']
                    main_freq = main_plot_info['frequency']
                    main_ssp = main_plot_info['ssp']
                    main_grid = main_plot_info['grid_indices']
                    
                    # Determine time range for main plot
                    if main_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                        main_min_year, main_max_year = 1950, 2013
                        main_default_range = (1950, 2013)
                    elif main_ssp == "historical":
                        main_min_year, main_max_year = 1950, 2014
                        main_default_range = (1950, 2014)
                    else:
                        main_min_year, main_max_year = 2015, 2100
                        main_default_range = (2015, 2100)
                    
                    # Create main plot slider (compact for table layout)
                    main_slider = ui.input_slider(
                        "time_series_range_main",
                        "Year Range",
                        min=main_min_year,
                        max=main_max_year,
                        value=main_default_range,
                        step=1,
                        sep=""
                    )
                    
                    # Create main plot trendline checkbox (compact for table layout)
                    main_checkbox = ui.input_checkbox(
                        "trendline_main",
                        "Show Trendline",
                        value=False
                    )
                    
                    # Create main plot row
                    main_plot_row = ui.div(
                        ui.div(
                            ui.div(
                                ui.div("Main Plot", class_="font-weight-bold"),
                                ui.div(f"{get_variable_display_name(main_var)} ({main_model}, {main_freq}, {main_ssp}) - Grid {main_grid[0]},{main_grid[1]}", class_="text-muted small"),
                                class_="col-8"
                            ),
                            ui.div(
                                main_slider,
                                class_="col-3"
                            ),
                            ui.div(
                                main_checkbox,
                                class_="col-1 text-center"
                            ),
                            class_="row align-items-center mb-2"
                        ),
                        class_="border-bottom pb-2"
                    )
                    
                    container_elements.append(main_plot_row)
                    print(f"DEBUG: Added main plot controls to table")
                    
                except Exception as e:
                    print(f"DEBUG: Error creating main plot controls: {e}")
            
            # If no additional traces, still show the table with just main plot
            if not unique_traces:
                traces_to_render = []
            else:
                traces_to_render = unique_traces
            
            print(f"DEBUG: Rendering {len(traces_to_render)} additional trace controls (unlimited)")
            
            # Render all traces
            for idx, trace in enumerate(traces_to_render):
                try:
                    trace_var = trace.get('variable')
                    trace_model = trace.get('model')
                    trace_freq = trace.get('frequency', 'annual')
                    trace_ssp = trace.get('ssp')
                    trace_lat = trace.get('lat_idx')
                    trace_lon = trace.get('lon_idx')
                    
                    # Validate trace data
                    if not all([trace_var, trace_model, trace_ssp, trace_lat is not None, trace_lon is not None]):
                        print(f"DEBUG: Skipping invalid trace at index {idx}: {trace}")
                        continue
                    
                    # Determine time range based on scenario
                    if trace_var in ("obs_tmin", "obs_tmax", "obs_pr"):
                        min_year, max_year = 1950, 2013
                        default_range = (1950, 2013)
                    elif trace_ssp == "historical":
                        min_year, max_year = 1950, 2014
                        default_range = (1950, 2014)
                    else:
                        min_year, max_year = 2015, 2100
                        default_range = (2015, 2100)
                    
                    trace_label = f"Additional trace {idx+1}: {get_variable_display_name(trace_var)} ({trace_model}, {trace_freq}, {trace_ssp}) - Grid {trace_lat},{trace_lon}"
                    
                    # Create a more stable structure with proper error handling
                    try:
                        # Use consistent IDs that match what the rest of the code expects
                        slider_id = f"time_series_range_extra_{idx}"
                        checkbox_id = f"show_trendline_extra_{idx}"
                        
                        # Create slider with error handling (compact for table layout)
                        try:
                            slider_element = ui.input_slider(
                                slider_id,
                                "Year Range",
                            min=min_year,
                            max=max_year,
                            value=default_range,
                            step=1,
                            sep=""
                            )
                        except Exception as slider_error:
                            print(f"DEBUG: Error creating slider {slider_id}: {slider_error}")
                            # Create a fallback element
                            slider_element = ui.div(f"Slider {idx+1}: {trace_label}", class_="text-muted")
                        
                        # Create checkbox with error handling (compact for table layout)
                        try:
                            checkbox_element = ui.input_checkbox(
                                checkbox_id, 
                                "Show Trendline", 
                                value=False
                            )
                        except Exception as checkbox_error:
                            print(f"DEBUG: Error creating checkbox {checkbox_id}: {checkbox_error}")
                            # Create a fallback element
                            checkbox_element = ui.div(f"Trendline option for trace {idx+1}", class_="text-muted")
                        
                        # Create container for this trace in table row format
                        trace_info = f"{trace_model} ({trace_freq}, {trace_ssp}) - Grid {trace_lat},{trace_lon}"
                        trace_container = ui.div(
                            ui.div(
                                ui.div(
                                    ui.div(f"Additional Trace {idx+1}: {get_variable_display_name(trace_var)}", class_="font-weight-bold"),
                                    ui.div(trace_info, class_="text-muted small"),
                                    class_="col-8"
                                ),
                                ui.div(
                            slider_element,
                                    class_="col-3"
                                ),
                                ui.div(
                            checkbox_element,
                                    class_="col-1 text-center"
                                ),
                                class_="row align-items-center mb-2"
                            ),
                            class_="border-bottom pb-2"
                        )
                        
                        container_elements.append(trace_container)
                        
                    except Exception as element_error:
                        print(f"DEBUG: Error creating UI elements for trace {idx}: {element_error}")
                        # Create a fallback element
                        fallback_element = ui.div(
                            f"Error loading controls for trace {idx+1}",
                            class_="mt-2 p-2 border rounded bg-warning",
                            id=f"trace_control_error_{idx}"
                        )
                        container_elements.append(fallback_element)
                        continue
                except Exception as trace_error:
                    print(f"DEBUG: Error processing trace {idx}: {trace_error}")
                    continue
            
            if not container_elements:
                return ui.div("Error loading trace controls", class_="text-danger mt-3")
            
            # Create the main container with a stable structure
            try:
                total_traces = len(container_elements)  # Includes main plot + additional traces
                additional_count = len(unique_traces)
                if main_plot_info and additional_count > 0:
                    trace_info = f"(Main plot + {additional_count} additional trace{'s' if additional_count != 1 else ''})"
                elif main_plot_info and additional_count == 0:
                    trace_info = "(Main plot only)"
                elif not main_plot_info and additional_count > 0:
                    trace_info = f"({additional_count} additional trace{'s' if additional_count != 1 else ''})"
                else:
                    trace_info = "(No traces)"
                
                # Create scrollable table layout with fixed height to maintain plot size
                scrollable_controls = ui.div(
                    # Table header
                    ui.div(
                        ui.div("Trace", class_="col-6 font-weight-bold"),
                        ui.div("Time Range", class_="col-4 font-weight-bold"),
                        ui.div("Trendline", class_="col-2 font-weight-bold text-center"),
                        class_="row border-bottom pb-2 mb-2",
                        id="trace-controls-header"
                    ),
                    # Scrollable content area - unlimited traces supported
                    ui.div(
                        *container_elements,
                        style="height: 400px; min-height: 400px; max-height: 600px; overflow-y: auto; overflow-x: auto; padding-right: 10px; width: 100%;",
                        id="trace-controls-content"
                    ),
                    # Scroll indicator for many traces
                    ui.div(
                        ui.span("↓ Scroll down for more traces", class_="text-info small"),
                        class_="text-center mt-2"
                    ) if total_traces > 4 else None,
                    class_="border rounded p-3 bg-light"
                )
                
                # Create a simple, stable container without dynamic IDs
                main_container = ui.div(
                    ui.div(
                        ui.h6(f"Trace Controls {trace_info}", style="display: inline-block; margin-top: 0px; margin-bottom: 8px;"),
                        ui.tags.button(
                            "📥 Download as CSV",
                            onclick="downloadTraceControlsAsCSV()",
                            style="float: right; padding: 6px 12px; background-color: #2196F3; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 13px; font-weight: bold; margin-top: 0px;",
                            onmouseover="this.style.backgroundColor='#0b7dda'",
                            onmouseout="this.style.backgroundColor='#2196F3'"
                        ),
                        style="overflow: auto;"
                    ),
                    scrollable_controls,
                    ui.div("Use these sliders to adjust the time period for each line. Scroll down to see more traces." + 
                           (f" ({total_traces} traces total)" if total_traces > 3 else ""), 
                           class_="text-muted mt-2"),
                    ui.div(f"All traces have compatible units and can be plotted together.", class_="text-success small mt-1"),
                    ui.tags.script("""
                        function downloadTraceControlsAsCSV() {
                            console.log('downloadTraceControlsAsCSV called');
                            let csv = [];
                            csv.push('"Trace","Time Range","Trendline"');
                            
                            const content = document.getElementById('trace-controls-content');
                            if (!content) {
                                console.error('trace-controls-content not found');
                                return;
                            }
                            
                            const traces = content.querySelectorAll('.border-bottom');
                            console.log('Found', traces.length, 'traces');
                            
                            for (let trace of traces) {
                                const traceNameDiv = trace.querySelector('[class*="col-"]');
                                const checkbox = trace.querySelector('input[type="checkbox"]');
                                
                                if (traceNameDiv) {
                                    const traceName = traceNameDiv.innerText.replace(/"/g, '""').replace(/\\n/g, ' ').trim();
                                    const timeRange = 'See dashboard';
                                    const trendline = checkbox ? (checkbox.checked ? 'Yes' : 'No') : 'N/A';
                                    csv.push(`"${traceName}","${timeRange}","${trendline}"`);
                                }
                            }
                            
                            console.log('CSV rows:', csv.length);
                            const csvContent = csv.join('\\n');
                            const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
                            const url = window.URL.createObjectURL(blob);
                            const a = document.createElement('a');
                            a.href = url;
                            a.download = 'Trace_Controls_' + new Date().toISOString().slice(0,10) + '.csv';
                            document.body.appendChild(a);
                            a.click();
                            document.body.removeChild(a);
                            window.URL.revokeObjectURL(url);
                            console.log('Download triggered');
                        }
                    """),
                    class_="mb-4",
                    style="margin-top: 0px; clear: both;"
                )
                print(f"DEBUG: Returning trace controls with download button - total_traces: {total_traces}, additional: {additional_count}")
                return main_container
            except Exception as container_error:
                print(f"DEBUG: Error creating main container: {container_error}")
                # Return a simple fallback without complex UI elements
                return ui.div(
                    ui.h6("Additional Trace Controls", class_="mt-3 mb-2"),
                    ui.div(f"Found {len(unique_traces)} compatible traces", class_="text-info"),
                    ui.div("Trace controls temporarily unavailable due to UI error.", class_="text-warning"),
                    class_="mt-3"
                )
            
        except Exception as e:
            print(f"DEBUG: Critical error in additional_traces_controls: {e}")
            return ui.div("Critical error loading trace controls", class_="text-danger mt-3")
    
    @output
    @render.ui
    def mad_table():
        """Render Mean Absolute Difference table for trace combinations"""
        # Make reactive to time series slider changes
        try:
            _ = input.time_series_range_main()
        except:
            pass
        
        # Make reactive to additional traces changes
        try:
            _ = main_plot_additional_traces.get()
            _ = main_plot_persistent_traces.get()
        except:
            pass
        
        # Also react to additional trace sliders - but don't fail if they don't exist yet
        try:
            additional_traces = main_plot_additional_traces.get() or []
            for i in range(len(additional_traces)):
                try:
                    # Only try to access slider if it exists
                    slider_id = f"time_series_range_extra_{i}"
                    if hasattr(input, slider_id):
                        _ = getattr(input, slider_id)()
                except:
                    pass  # Slider not ready yet, that's okay
        except:
            pass
        
        try:
            # Get current plot data from the main plot
            selections = get_plot_selections()
            variable = selections['variable']
            model = selections['model']
            frequency = selections['frequency']
            ssp = selections['ssp']
            grid_indices = selections['grid_indices']
            lat_idx, lon_idx = grid_indices if grid_indices else (None, None)
            
            print(f"DEBUG: mad_table - variable: {variable}, model: {model}, frequency: {frequency}, ssp: {ssp}")
            print(f"DEBUG: mad_table - grid_indices: {grid_indices}")
            
            # Check if we have valid data
            if not variable or not frequency or lat_idx is None or lon_idx is None:
                print(f"DEBUG: mad_table - Missing required data, returning None")
                return None
            
            # Get additional traces
            current_traces = list(main_plot_additional_traces.get() or [])
            persistent_traces = list(main_plot_persistent_traces.get() or [])
            all_traces = current_traces + persistent_traces
            
            print(f"DEBUG: mad_table - current_traces: {len(current_traces)}, persistent_traces: {len(persistent_traces)}")
            
            # Remove duplicates
            seen_traces = set()
            unique_traces = []
            for trace_cfg in all_traces:
                trace_key = (trace_cfg.get('variable'), trace_cfg.get('model'), trace_cfg.get('frequency'), trace_cfg.get('ssp'), trace_cfg.get('lat_idx'), trace_cfg.get('lon_idx'))
                if trace_key not in seen_traces:
                    seen_traces.add(trace_key)
                    unique_traces.append(trace_cfg)
            
            print(f"DEBUG: mad_table - unique_traces: {len(unique_traces)}")
            
            # Always show MAD table if we have a main trace, even without additional traces
            print(f"DEBUG: mad_table - Processing {len(unique_traces)} additional traces with main trace")
            
            # Create trace data for main selection
            main_trace_data = {
                'variable': variable,
                'model': model,
                'frequency': frequency,
                'ssp': ssp,
                'lat_idx': lat_idx,
                'lon_idx': lon_idx,
                'name': f"{get_variable_display_name(variable)} ({model}, {frequency}, {ssp}) - Grid {lat_idx},{lon_idx}"
            }
            
            # Add name field to additional traces if missing
            for trace in unique_traces:
                if 'name' not in trace:
                    trace['name'] = f"{get_variable_display_name(trace['variable'])} ({trace['model']}, {trace['frequency']}, {trace['ssp']}) - Grid {trace['lat_idx']},{trace['lon_idx']}"
            
            # Combine main trace with additional traces
            all_trace_data = [main_trace_data] + unique_traces
            
            print(f"DEBUG: MAD - Total traces for comparison: {len(all_trace_data)}")
            print(f"DEBUG: MAD - Trace names: {[trace['name'] for trace in all_trace_data]}")
            
            # Calculate MAD for all pairs
            mad_results = []
            for i in range(len(all_trace_data)):
                for j in range(i + 1, len(all_trace_data)):
                    trace1 = all_trace_data[i]
                    trace2 = all_trace_data[j]
                    
                    # Skip if frequencies don't match - can't compare different frequencies
                    if trace1.get('frequency') != trace2.get('frequency'):
                        print(f"DEBUG: MAD - Skipping comparison due to frequency mismatch: {trace1.get('frequency')} vs {trace2.get('frequency')}")
                        continue
                    
                    # Get time ranges for each trace
                    trace1_time_range = None
                    trace2_time_range = None
                    
                    # For main trace, use main slider
                    if i == 0:  # Main trace is always first
                        try:
                            trace1_time_range = input.time_series_range_main()
                        except:
                            pass
                    
                    # For additional traces, try to get their individual sliders
                    if i > 0:  # Additional trace
                        try:
                                trace1_time_range = getattr(input, f"time_series_range_extra_{i-1}")()
                        except:
                            # Fallback to main slider if individual slider doesn't exist
                            try:
                                trace1_time_range = input.time_series_range_main()
                            except:
                                pass
                    
                    if j == 0:  # Main trace is always first
                        try:
                            trace2_time_range = input.time_series_range_main()
                        except:
                            pass
                    
                    if j > 0:  # Additional trace
                        try:
                                trace2_time_range = getattr(input, f"time_series_range_extra_{j-1}")()
                        except:
                            # Fallback to main slider if individual slider doesn't exist
                            try:
                                trace2_time_range = input.time_series_range_main()
                            except:
                                pass
                    
                    # Get trace data with correct time ranges
                    print(f"DEBUG: MAD calculation - Trace1: {trace1['variable']} (range: {trace1_time_range})")
                    print(f"DEBUG: MAD calculation - Trace2: {trace2['variable']} (range: {trace2_time_range})")
                    
                    trace1_data = get_trace_data_for_mad(trace1, trace1_time_range)
                    trace2_data = get_trace_data_for_mad(trace2, trace2_time_range)
                    
                    print(f"DEBUG: MAD - Trace1 data: {'Success' if trace1_data is not None else 'None'}")
                    print(f"DEBUG: MAD - Trace2 data: {'Success' if trace2_data is not None else 'None'}")
                    
                    if trace1_data is None or trace2_data is None:
                        print(f"DEBUG: MAD - Skipping comparison due to missing data")
                        continue
                    
                    # Calculate MAD
                    mad_result = calculate_mad(
                        trace1_data, trace2_data,
                        trace1['frequency'], trace2['frequency'],
                        trace1['name'], trace2['name']
                    )
                    
                    # FIXED: Always add results, even if MAD is None, to show why comparison failed
                    if mad_result['mad'] is not None:
                        mad_results.append({
                            'trace1': trace1['name'],
                            'trace2': trace2['name'],
                            'mad': mad_result['mad'],
                            'categorization': mad_result['categorization'],
                            'common_years': mad_result['common_years'],
                            'n_points': mad_result['n_points'],
                            'warning': mad_result.get('warning', '')
                        })
                    else:
                        # Show why MAD couldn't be calculated
                        reason = mad_result.get('categorization', 'Cannot compare')
                        if mad_result.get('error'):
                            reason = mad_result['error']
                        mad_results.append({
                            'trace1': trace1['name'],
                            'trace2': trace2['name'],
                            'mad': None,
                            'categorization': reason,
                            'common_years': mad_result.get('common_years', 'N/A'),
                            'n_points': mad_result.get('n_points', 0),
                            'warning': ''
                        })
            
            print(f"DEBUG: MAD - Total MAD results generated: {len(mad_results)}")
            
            if not mad_results:
                print(f"DEBUG: MAD - No MAD results, returning None")
                return None
            
            # Create HTML table rows
            print(f"DEBUG: MAD HTML - Creating {len(mad_results)} table rows")
            table_rows = []
            for idx, result in enumerate(mad_results):
                print(f"DEBUG: MAD HTML - Row {idx}: {result['trace1'][:40]}... vs {result['trace2'][:40]}...")
                table_rows.append(
                    ui.tags.tr(
                        ui.tags.td(result['trace1'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: left; word-wrap: break-word;"),
                        ui.tags.td(result['trace2'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: left; word-wrap: break-word;"),
                        ui.tags.td(f"{result['mad']:.4f}" if result['mad'] is not None else "N/A", style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center; font-weight: bold;"),
                        ui.tags.td(result['categorization'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        ui.tags.td(result['common_years'], style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        ui.tags.td(f"{result['n_points']}", style="padding: 12px 8px; border-bottom: 1px solid #ddd; text-align: center;"),
                        style="background-color: #fff;" if idx % 2 == 0 else "background-color: #f8f9fa;"
                    )
                )
            
            # Create complete HTML table
            print(f"DEBUG: MAD HTML - Building final table with {len(table_rows)} rows")
            html_table = ui.div(
                ui.div(
                    ui.h4("Mean Absolute Difference Analysis", style="display: inline-block; margin-bottom: 5px; color: #333;"),
                    ui.tags.button(
                        "📥 Download as CSV",
                        onclick="downloadMADTableAsCSV()",
                        style="float: right; padding: 8px 16px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 14px; font-weight: bold;",
                        onmouseover="this.style.backgroundColor='#45a049'",
                        onmouseout="this.style.backgroundColor='#4CAF50'"
                    ),
                    style="text-align: center; margin-bottom: 10px; overflow: auto;"
                ),
                ui.p("Comparison of all trace combinations displayed on the main graph", 
                     style="text-align: center; margin-bottom: 15px; color: #666; font-size: 13px;"),
                ui.div(
                    ui.tags.table(
                        ui.tags.thead(
                            ui.tags.tr(
                                ui.tags.th("Trace 1", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: left; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Trace 2", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: left; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("MAD", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Match Quality", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Time Period", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;"),
                                ui.tags.th("Points", style="padding: 15px 8px; background-color: #E6F3FF; border-bottom: 2px solid #4CAF50; text-align: center; font-weight: bold; position: sticky; top: 0; z-index: 1;")
                            )
                        ),
                        ui.tags.tbody(*table_rows),
                        style="width: 100%; border-collapse: collapse; font-family: Arial, sans-serif; font-size: 14px;",
                        id="mad-table-content"
                    ),
                    style="max-height: 600px; overflow-y: auto; overflow-x: auto; border: 1px solid #ddd; border-radius: 5px;"
                ),
                ui.p("Scroll to see more combinations", style="text-align: center; margin-top: 10px; color: #999; font-size: 12px; font-style: italic;"),
                ui.tags.script("""
                    function downloadMADTableAsCSV() {
                        const table = document.getElementById('mad-table-content');
                        if (!table) return;
                        
                        let csv = [];
                        const rows = table.querySelectorAll('tr');
                        
                        for (let row of rows) {
                            let cols = row.querySelectorAll('td, th');
                            let csvRow = [];
                            for (let col of cols) {
                                let text = col.innerText.replace(/"/g, '""');
                                csvRow.push('"' + text + '"');
                            }
                            csv.push(csvRow.join(','));
                        }
                        
                        const csvContent = csv.join('\\n');
                        const blob = new Blob([csvContent], { type: 'text/csv' });
                        const url = window.URL.createObjectURL(blob);
                        const a = document.createElement('a');
                        a.href = url;
                        a.download = 'MAD_Analysis_' + new Date().toISOString().slice(0,10) + '.csv';
                        document.body.appendChild(a);
                        a.click();
                        document.body.removeChild(a);
                        window.URL.revokeObjectURL(url);
                    }
                """),
                style="padding: 20px; background-color: #fafafa; border-radius: 8px; margin-top: 0px; clear: both;"
            )
            
            return html_table
            
        except Exception as e:
            print(f"DEBUG: Error in mad_table: {e}")
            import traceback
            traceback.print_exc()
            # Return a helpful message instead of None so user knows what's happening
            return ui.div(
                ui.h5("Mean Absolute Difference Analysis", class_="mt-4 mb-3"),
                ui.div(
                    ui.p("MAD table is loading... Please wait a moment for all traces to be ready.", class_="text-info"),
                    class_="alert alert-info"
                ),
                class_="mt-3"
            )

    @output
    @render.ui
    def multi_plot_output():
        """DISABLED: Only main plot is used - additional traces are added to main plot"""
        # Always return None to disable separate plot creation
        return None
            
    # Event handlers for multi-plot actions
    @reactive.Effect
    @reactive.event(input.add_current_plot)
    def _add_current_plot():
        """Add the current multi-plot selection to the plots list"""
        multi_var = multi_plot_variable.get()
        multi_model = multi_plot_model.get()
        multi_frequency = multi_plot_frequency.get()
        multi_ssp = multi_plot_ssp.get()
        multi_action = multi_plot_action.get()
        
        if not (multi_var and multi_model and multi_frequency and multi_ssp):
            return
            
        # Get current plots list
        plots_list = list(multi_plots_list.get())
        
        # FIXED: Use multi-plot specific grid selection instead of global grid selection
        multi_grid_id = multi_plot_grid_id.get()
        if not multi_grid_id:
            print("DEBUG: No grid selected for multi-plot")
            return
            
        # Convert grid ID to lat/lon indices
        try:
            # Parse the grid ID to get lat_idx and lon_idx
            # Grid ID format is typically "lat_idx,lon_idx"
            if ',' in multi_grid_id:
                lat_idx, lon_idx = map(int, multi_grid_id.split(','))
            else:
                # Fallback: try to get from current global selection
                grid_indices = get_selected_grid_indices()
                if not grid_indices:
                    print("DEBUG: No grid selected for multi-plot")
                    return
                lat_idx, lon_idx = grid_indices
        except Exception as e:
            print(f"DEBUG: Error parsing grid ID '{multi_grid_id}': {e}")
            # Fallback: try to get from current global selection
            grid_indices = get_selected_grid_indices()
            if not grid_indices:
                print("DEBUG: No grid selected for multi-plot")
                return
            lat_idx, lon_idx = grid_indices
        
        print(f"DEBUG: _add_current_plot - Using grid coordinates: ({lat_idx}, {lon_idx}) from grid ID: {multi_grid_id}")
        
        # Check unit compatibility with existing plots
        filepaths_map = reactive_netcdf_filepaths.get()
        compatible_plots = []
        multi_freq = multi_plot_frequency.get()
        
        for plot_config in plots_list:
            existing_var = plot_config['variable']
            existing_freq = plot_config.get('frequency', 'annual')
            if check_unit_compatibility(multi_var, existing_var, filepaths_map, multi_freq, existing_freq):
                compatible_plots.append(plot_config)
                print(f"DEBUG: Compatible with existing plot: {existing_var} ({existing_freq})")
            else:
                print(f"DEBUG: Incompatible with existing plot: {existing_var} ({existing_freq}) vs {multi_var} ({multi_freq})")
        
        # Decide where to add based on selection (handle main plot even if no existing multi-plots)
        # Determine main plot compatibility
        main_compatible = False
        try:
            main_var = selected_netcdf_variable.get()
            main_freq = selected_netcdf_frequency.get()
            multi_freq = multi_plot_frequency.get()
            if main_var:
                # FIXED: Pass frequency information for proper precipitation compatibility check
                main_compatible = check_unit_compatibility(multi_var, main_var, filepaths_map, multi_freq, main_freq)
                print(f"DEBUG: Unit compatibility check (with freq) - {multi_var} ({multi_freq}) vs {main_var} ({main_freq}): {main_compatible}")
        except Exception as e:
            print(f"DEBUG: Error in compatibility check: {e}")
            main_compatible = False

        # Read user's target selection from dropdown if available
        target_choice = None
        try:
            target_choice = input.target_plot_id()
        except Exception:
            target_choice = None

        print(f"DEBUG: _add_current_plot decision -> multi_action={multi_action}, target_choice={target_choice}, main_compatible={main_compatible}, compatible_plots={len(compatible_plots)}")

        # If user chose add_to_existing but did not change dropdown, pick sensible default
        if (target_choice is None or target_choice == "new") and multi_action == "add_to_existing":
            if main_compatible:
                target_choice = "main"
            elif compatible_plots:
                target_choice = str(compatible_plots[0]['id'])

        # Case 1: Add to main plot
        if target_choice == "main" and main_compatible:
            try:
                # FIXED: Get traces from both current and persistent storage with error handling
                current_traces = list(main_plot_additional_traces.get() or [])
                persistent_traces = list(main_plot_persistent_traces.get() or [])
                
                # Create new trace configuration
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_frequency,
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx
                }
                
                # Check if this trace already exists to prevent duplicates
                trace_exists = False
                for existing_trace in current_traces + persistent_traces:
                    if (existing_trace.get('variable') == multi_var and 
                        existing_trace.get('model') == multi_model and 
                        existing_trace.get('ssp') == multi_ssp and
                        existing_trace.get('lat_idx') == lat_idx and 
                        existing_trace.get('lon_idx') == lon_idx):
                        trace_exists = True
                        break
                
                if not trace_exists:
                    # Add to both current and persistent storage
                    current_traces.append(new_trace)
                    persistent_traces.append(new_trace)
                    
                    # Safe reactive value updates with error handling
                    try:
                        main_plot_additional_traces.set(current_traces)
                        main_plot_persistent_traces.set(persistent_traces)
                        
                        # Increment trigger safely
                        current_trigger = main_plot_update_trigger.get()
                        main_plot_update_trigger.set(current_trigger + 1)
                        
                        multi_plot_status_message.set(f"✅ Added {get_variable_display_name(multi_var)} to main plot")
                        print(f"DEBUG: Added trace to main plot: {new_trace}")
                    except Exception as update_error:
                        print(f"DEBUG: Error updating reactive values: {update_error}")
                        # Fallback: try individual updates
                        try:
                            main_plot_additional_traces.set(current_traces)
                        except:
                            pass
                        try:
                            main_plot_persistent_traces.set(persistent_traces)
                        except:
                            pass
                        try:
                            multi_plot_status_message.set(f"✅ Added {get_variable_display_name(multi_var)} to main plot")
                        except:
                            pass
            except Exception as trace_error:
                print(f"DEBUG: Error managing additional traces: {trace_error}")
                multi_plot_status_message.set(f"⚠️ Error adding trace: {str(trace_error)}")
                
                # Clear reactive values first
                multi_plot_grid_id.set(None)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                
                # Reset multi-plot step completion flags so user can make new selections
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                
                # Force UI update by triggering multi-plot update
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                
                print("DEBUG: Reset multi-plot selections and triggered UI update after adding to main plot")
            else:
                multi_plot_status_message.set(f"ℹ️ {get_variable_display_name(multi_var)} is already on the main plot")
                print(f"DEBUG: Trace already exists on main plot: {new_trace}")
            
            # Reset all selections after successfully adding to main plot
            multi_plot_step_1_completed.set(False)
            multi_plot_step_2_completed.set(False)
            multi_plot_step_3_completed.set(False)
            multi_plot_step_4_completed.set(False)
            multi_plot_step_5_completed.set(False)
            multi_plot_step_6_completed.set(False)
            multi_plot_variable.set(None)
            multi_plot_model.set(None)
            multi_plot_frequency.set(None)
            multi_plot_ssp.set(None)
            multi_plot_action.set(None)
            multi_plot_grid_id.set(None)
            
            # Trigger plot update to show the new trace
            main_plot_update_trigger.set(main_plot_update_trigger.get() + 1)
            
            multi_plot_status_message.set(f"✅ Added {get_variable_display_name(multi_var)} to main plot. Selections reset for next addition.")
            print("DEBUG: Reset all selections after adding to main plot")
            return

        # Case 2: Add to an existing multi-plot by ID
        if target_choice and target_choice not in (None, "new", "main"):
            try:
                target_plot_id = int(target_choice)
            except Exception:
                target_plot_id = compatible_plots[0]['id'] if compatible_plots else None
            if target_plot_id is not None:
                updated_plots_list = []
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_frequency,
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx
                }
                for plot_config in plots_list:
                    if plot_config['id'] == target_plot_id:
                        plot_copy = dict(plot_config)
                        additional = list(plot_copy.get('additional_traces', []))
                        additional.append(new_trace)
                        plot_copy['additional_traces'] = additional
                        updated_plots_list.append(plot_copy)
                    else:
                        updated_plots_list.append(plot_config)
                multi_plots_list.set(updated_plots_list)
                multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                
                # Reset all selections after successfully adding to existing plot
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                multi_plot_grid_id.set(None)
                
                multi_plot_status_message.set(f"✅ Added to selected plot. Selections reset for next addition.")
                print("DEBUG: Reset all selections after adding to existing plot")
                return

        # Case 3: Check if we can create a new plot (only if no existing plots or units are compatible)
        if plots_list:
            # Check if there are any existing plots with incompatible units
            has_incompatible_plots = False
            for plot_config in plots_list:
                existing_var = plot_config['variable']
                existing_freq = plot_config.get('frequency', 'annual')
                if not check_unit_compatibility(multi_var, existing_var, filepaths_map, multi_freq, existing_freq):
                    has_incompatible_plots = True
                    print(f"DEBUG: Found incompatible plot: {existing_var} ({existing_freq}) vs {multi_var} ({multi_freq})")
                    break
            
            if has_incompatible_plots:
                multi_plot_status_message.set("❌ Cannot create new plot: Units are not compatible with existing plots. Only one plot is allowed on the gridbox map.")
                # Reset all selections when units are incompatible
                multi_plot_step_1_completed.set(False)
                multi_plot_step_2_completed.set(False)
                multi_plot_step_3_completed.set(False)
                multi_plot_step_4_completed.set(False)
                multi_plot_step_5_completed.set(False)
                multi_plot_step_6_completed.set(False)
                multi_plot_variable.set(None)
                multi_plot_model.set(None)
                multi_plot_frequency.set(None)
                multi_plot_ssp.set(None)
                multi_plot_action.set(None)
                multi_plot_grid_id.set(None)
                print("DEBUG: Reset all selections due to incompatible units")
                return
        
        # Create new plot only if no existing plots or all units are compatible
        plot_id = current_multi_plot_id.get()
        new_plot_config = {
            'id': plot_id,
            'variable': multi_var,
            'model': multi_model,
            'frequency': multi_frequency,
            'ssp': multi_ssp,
            'action': 'new_plot',
            'lat_idx': lat_idx,
            'lon_idx': lon_idx,
            'grid_id': multi_grid_id.get()  # Preserve the grid context
        }
        
        # Check unit compatibility with existing plots
        filepaths_map = reactive_netcdf_filepaths.get()
        plots_list = multi_plots_list.get() or []
        
        # Check if this exact trace already exists (to prevent duplicates)
        trace_signature = (multi_var, multi_model, multi_frequency, multi_ssp, lat_idx, lon_idx)
        existing_signatures = [(p['variable'], p['model'], p['frequency'], p['ssp'], p['lat_idx'], p['lon_idx']) for p in plots_list]
        
        if trace_signature in existing_signatures:
            print(f"DEBUG: Duplicate trace detected - not adding: {trace_signature}")
            multi_plot_status_message.set(f"⚠️ Trace already exists: {get_variable_display_name(multi_var)} ({multi_model}, {multi_frequency})")
            # Don't proceed with adding or incrementing plot ID
            return
        
        # If no existing plots, add the new plot
        if not plots_list:
            multi_plots_list.set([new_plot_config])
            print(f"DEBUG: Added first multi-plot: {new_plot_config}")
        else:
            # Check compatibility with existing plots (with frequency check for precipitation)
            compatible_plots = []
            for existing_plot in plots_list:
                existing_freq = existing_plot.get('frequency', 'annual')
                if check_unit_compatibility(multi_var, existing_plot['variable'], filepaths_map, multi_freq, existing_freq):
                    compatible_plots.append(existing_plot)
                    print(f"DEBUG: Compatible with existing plot: {existing_plot['variable']} ({existing_freq})")
                else:
                    print(f"DEBUG: Incompatible with existing plot: {existing_plot['variable']} ({existing_freq}) vs {multi_var} ({multi_freq})")
            
            # Add new plot if compatible
            if compatible_plots or len(plots_list) == 0:
                updated_plots_list = plots_list + [new_plot_config]
                multi_plots_list.set(updated_plots_list)
                print(f"DEBUG: Added compatible multi-plot: {new_plot_config}")
            else:
                print(f"DEBUG: Incompatible units - not adding plot: {new_plot_config}")
                return
        
        # Initialize time range for this plot based on SSP
        time_ranges = dict(multi_plot_time_ranges.get())
        if multi_var in ("obs_tmin", "obs_tmax", "obs_pr"):
            time_ranges[plot_id] = (1950, 2013)
        elif multi_ssp == "historical":
            time_ranges[plot_id] = (1950, 2014)
        else:
            time_ranges[plot_id] = (2015, 2100)
        multi_plot_time_ranges.set(time_ranges)
        
        current_multi_plot_id.set(plot_id + 1)
        multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
        multi_plot_status_message.set(f"✅ Created new plot {plot_id + 1} with {get_variable_display_name(multi_var)}")
        
        print(f"DEBUG: Created new plot {plot_id} with grid coordinates ({lat_idx}, {lon_idx})")
        
        # Clear reactive values first
        multi_plot_grid_id.set(None)
        multi_plot_variable.set(None)
        multi_plot_model.set(None)
        multi_plot_frequency.set(None)
        multi_plot_ssp.set(None)
        multi_plot_action.set(None)
        
        # Reset multi-plot step completion flags so user can make new selections
        multi_plot_step_1_completed.set(False)
        multi_plot_step_2_completed.set(False)
        multi_plot_step_3_completed.set(False)
        multi_plot_step_4_completed.set(False)
        multi_plot_step_5_completed.set(False)
        multi_plot_step_6_completed.set(False)
        
        # Force UI update by triggering multi-plot update
        multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
        
        print("DEBUG: Reset multi-plot selections and triggered UI update after creating new plot")
        
        print("DEBUG: Reset all selections after creating new plot")

    # Auto-add a multi-plot when the progressive dropdown is completed
    @reactive.Effect
    @reactive.event(multi_plot_step_5_completed)
    def _auto_add_plot_when_complete():
        try:
            print("DEBUG: _auto_add_plot_when_complete triggered")
            if not multi_plot_step_5_completed.get():
                print("DEBUG: step 5 not completed; exiting auto-add")
                return
            
            # Add protection against recursive calls
            if hasattr(_auto_add_plot_when_complete, '_processing'):
                print("DEBUG: Already processing auto-add; skipping to prevent recursion")
                return
            _auto_add_plot_when_complete._processing = True
            
            # Check if we've already processed this selection to prevent duplicates
            multi_var = multi_plot_variable.get()
            multi_model = multi_plot_model.get()
            multi_ssp = multi_plot_ssp.get()
            
            # For observed variables, model and ssp might be None or set to defaults
            if not multi_var:
                print("DEBUG: auto-add missing variable; exiting")
                return
            
            # Handle observed variables that don't have explicit model/ssp
            if multi_var.startswith('obs_'):
                if not multi_model:
                    multi_model = 'Observed'
                if not multi_ssp:
                    multi_ssp = 'historical'
            elif not (multi_model and multi_ssp):
                print("DEBUG: auto-add missing model/ssp for non-observed variable; exiting")
                return
            
            # Defer duplicate checks until after we resolve grid indices, so
            # same variable/model/ssp from different grid cells are allowed
            
            # FIXED: Also check if this selection is already in the current plot combinations
            # This prevents auto-add from running when the trace is already being processed by the plot function
            current_plot_combinations = []
            try:
                # Get the current plot combinations from the main plot function
                main_var = selected_netcdf_variable.get()
                main_model = selected_netcdf_model.get()
                main_freq = selected_netcdf_frequency.get()
                main_ssp = selected_netcdf_ssp.get()
                
                if main_var and main_model and main_freq and main_ssp:
                    current_plot_combinations.append((main_var, main_model, main_freq, main_ssp))
                    
                    # Check if the current multi-plot selection is already included
                    multi_freq = multi_plot_frequency.get() or 'annual'
                    current_multi_combo = (multi_var, multi_model, multi_freq, multi_ssp)
                    
                    # Allow adding the same variable/model/ssp if it's from a different grid location
                    multi_grid_id = multi_plot_grid_id.get()
                    main_grid_id = get_selected_grid_indices()
                    if main_grid_id and multi_grid_id:
                        try:
                            multi_lat, multi_lon = map(int, multi_grid_id.split(","))
                            main_lat, main_lon = main_grid_id
                            if (multi_lat, multi_lon) == (main_lat, main_lon) and current_multi_combo in current_plot_combinations:
                                print("DEBUG: auto-add selection already in current plot combinations (same grid); exiting to prevent duplicates")
                                return
                        except Exception as e:
                            print(f"DEBUG: Error comparing grid locations in auto-add: {e}")
                            if current_multi_combo in current_plot_combinations:
                                print("DEBUG: auto-add selection already in current plot combinations; exiting to prevent duplicates")
                                return
                    else:
                        if current_multi_combo in current_plot_combinations:
                            print("DEBUG: auto-add selection already in current plot combinations; exiting to prevent duplicates")
                            return
            except Exception as e:
                print(f"DEBUG: Error checking current plot combinations: {e}")
            
            # Always add to main plot - no action selection needed
            print(f"DEBUG: auto-add selections -> var={multi_var}, model={multi_model}, ssp={multi_ssp}")
            # Check main plot compatibility using the standard compatibility function
            filepaths_map = reactive_netcdf_filepaths.get()
            main_var = selected_netcdf_variable.get()
            main_model = selected_netcdf_model.get()
            main_freq = selected_netcdf_frequency.get()
            main_ssp = selected_netcdf_ssp.get()
            multi_freq = multi_plot_frequency.get()
            try:
                # FIXED: Pass frequency information for proper precipitation compatibility check
                main_compatible = check_unit_compatibility(multi_var, main_var, filepaths_map, multi_freq, main_freq) if main_var else False
                print(f"DEBUG: auto-add compatibility check (with freq) - {multi_var} ({multi_freq}) vs {main_var} ({main_freq}): {main_compatible}")
            except Exception as e:
                print(f"DEBUG: Error in auto-add compatibility check: {e}")
                main_compatible = False
            print(f"DEBUG: auto-add main_compatible: {main_compatible} (main={main_var}, {main_model}, {main_ssp})")
            # Always try to add to main plot - check compatibility first
            if main_compatible:
                # Determine grid coordinates for the additional trace
                multi_grid_id = multi_plot_grid_id.get()
                if multi_grid_id and "," in multi_grid_id:
                    try:
                        lat_idx, lon_idx = map(int, multi_grid_id.split(","))
                        print(f"DEBUG: auto-add to main using multi-plot grid coordinates: ({lat_idx}, {lon_idx})")
                    except Exception:
                        lat_idx = lon_idx = None
                        print(f"DEBUG: auto-add to main error parsing multi_grid_id: {multi_grid_id}")
                else:
                    # Fallback to main plot grid if multi-plot grid not available
                    grid_indices = get_selected_grid_indices()
                    if grid_indices:
                        lat_idx, lon_idx = grid_indices
                        print(f"DEBUG: auto-add to main fallback to main plot grid coordinates: ({lat_idx}, {lon_idx})")
                    else:
                        lat_idx = lon_idx = None
                if lat_idx is None or lon_idx is None:
                    print("DEBUG: auto-add to main missing grid indices; exiting")
                    return

                # Prepare new trace configuration for main plot
                new_trace = {
                    'variable': multi_var,
                    'model': multi_model,
                    'frequency': multi_plot_frequency.get() or 'annual',
                    'ssp': multi_ssp,
                    'lat_idx': lat_idx,
                    'lon_idx': lon_idx
                }

                # Append to main plot additional traces if not already present
                try:
                    current_traces = list(main_plot_additional_traces.get() or [])
                    persistent_traces = list(main_plot_persistent_traces.get() or [])
                    trace_exists = False
                    for existing_trace in current_traces + persistent_traces:
                        if (existing_trace.get('variable') == new_trace['variable'] and
                            existing_trace.get('model') == new_trace['model'] and
                            existing_trace.get('ssp') == new_trace['ssp'] and
                            existing_trace.get('lat_idx') == new_trace['lat_idx'] and
                            existing_trace.get('lon_idx') == new_trace['lon_idx']):
                            trace_exists = True
                            break
                    if not trace_exists:
                        current_traces.append(new_trace)
                        persistent_traces.append(new_trace)
                        
                        # Safe reactive value updates with error handling
                        try:
                            main_plot_additional_traces.set(current_traces)
                            main_plot_persistent_traces.set(persistent_traces)
                            print(f"DEBUG: auto-added trace to main plot: {new_trace}")
                            multi_plot_status_message.set(f"✅ Added {get_variable_display_name(multi_var)} to main plot")
                            # Trigger a plot update safely
                            current_main_trigger = main_plot_update_trigger.get()
                            main_plot_update_trigger.set(current_main_trigger + 1)
                            current_plot_trigger = plot_update_trigger.get()
                            plot_update_trigger.set(current_plot_trigger + 1)
                        except Exception as update_error:
                            print(f"DEBUG: Error updating reactive values in auto-add: {update_error}")
                            # Fallback: try individual updates
                            try:
                                main_plot_additional_traces.set(current_traces)
                            except:
                                pass
                            try:
                                main_plot_persistent_traces.set(persistent_traces)
                            except:
                                pass
                            try:
                                multi_plot_status_message.set(f"✅ Added {get_variable_display_name(multi_var)} to main plot")
                            except:
                                pass
                    else:
                        multi_plot_status_message.set(f"ℹ️ {get_variable_display_name(multi_var)} with the same options is already on the main plot. Try selecting a different frequency to add another trace.")
                        print(f"DEBUG: auto-add to main skipped duplicate trace: {new_trace}")
                                
                except Exception as auto_add_error:
                    print(f"DEBUG: Error in auto-add trace logic: {auto_add_error}")
                    multi_plot_status_message.set(f"⚠️ Error auto-adding trace: {str(auto_add_error)}")
                    
                # Reset multi-plot selections and steps after successful or failed add
                # This allows the user to add another trace
                    multi_plot_grid_id.set(None)
                    multi_plot_variable.set(None)
                    multi_plot_model.set(None)
                    multi_plot_frequency.set(None)
                    multi_plot_ssp.set(None)
                    multi_plot_action.set(None)
                    
                    # Reset multi-plot step completion flags so user can make new selections
                    multi_plot_step_1_completed.set(False)
                    multi_plot_step_2_completed.set(False)
                    multi_plot_step_3_completed.set(False)
                    multi_plot_step_4_completed.set(False)
                    multi_plot_step_5_completed.set(False)
                    multi_plot_step_6_completed.set(False)
                    
                    # Force UI update by triggering multi-plot update
                    multi_plot_update_trigger.set(multi_plot_update_trigger.get() + 1)
                    
                    print("DEBUG: Reset multi-plot selections and triggered UI update after auto-adding to main plot")
                return
            # Clear all selections after successful auto-add (this should not be reached due to early returns above)
            multi_plot_variable.set(None)
            multi_plot_model.set(None)
            multi_plot_frequency.set(None)
            multi_plot_ssp.set(None)
            multi_plot_action.set(None)
            multi_plot_grid_id.set(None)
            
            print("DEBUG: auto-add reset steps and cleared selections for next multi-plot selection")
        except Exception as e:
            print(f"DEBUG: _auto_add_plot_when_complete error: {e}")
        finally:
            # Clear the processing flag
            if hasattr(_auto_add_plot_when_complete, '_processing'):
                delattr(_auto_add_plot_when_complete, '_processing')

    # Event handlers for individual time series sliders
    @reactive.Effect
    @reactive.event(
        input.multi_plot_time_range_0, input.multi_plot_time_range_1, input.multi_plot_time_range_2,
        input.multi_plot_time_range_3, input.multi_plot_time_range_4, input.multi_plot_time_range_5,
        input.multi_plot_time_range_6, input.multi_plot_time_range_7, input.multi_plot_time_range_8,
        input.multi_plot_time_range_9, input.multi_plot_time_range_10, input.multi_plot_time_range_11,
        input.multi_plot_time_range_12, input.multi_plot_time_range_13, input.multi_plot_time_range_14,
        input.multi_plot_time_range_15, input.multi_plot_time_range_16, input.multi_plot_time_range_17,
        input.multi_plot_time_range_18, input.multi_plot_time_range_19, input.multi_plot_time_range_20,
        # Add more sliders dynamically if needed
    )
    def _update_multi_plot_time_range():
        """Update time range for a specific multi-plot"""
        # Determine which slider triggered this effect via reactive context
        try:
            # Get the current reactive context to determine which input changed
            context = reactive.get_current_context()
            if context and hasattr(context, 'name'):
                input_name = context.name
                print(f"DEBUG: Multi-plot time range slider changed: {input_name}")
                
                # Extract plot ID from input name (e.g., "multi_plot_time_range_0" -> 0)
                if "_" in input_name and input_name.endswith(("_0", "_1", "_2", "_3", "_4", "_5", "_6", "_7", "_8", "_9")):
                    plot_id_str = input_name.split("_")[-1]
                    try:
                        plot_id = int(plot_id_str)
                        print(f"DEBUG: Updating time range for multi-plot {plot_id}")
                        
                        # Get the current time range value
                        time_range = input[input_name]()
                        if time_range and len(time_range) == 2:
                            start_year, end_year = time_range
                            print(f"DEBUG: New time range for plot {plot_id}: {start_year}-{end_year}")
                            
                            # Update the specific plot's time range
                            plots_list = multi_plots_list.get()
                            if plots_list and plot_id < len(plots_list):
                                plot_config = plots_list[plot_id]
                                plot_config['time_range'] = [start_year, end_year]
                                
                                # Update the plots list
                                updated_plots = list(plots_list)
                                updated_plots[plot_id] = plot_config
                                multi_plots_list.set(updated_plots)
                                
                                # Trigger plot update
                                plot_update_trigger.set(plot_update_trigger.get() + 1)
                                print(f"DEBUG: Updated time range for plot {plot_id} to {start_year}-{end_year}")
                            else:
                                print(f"DEBUG: Plot {plot_id} not found in plots list")
                        else:
                            print(f"DEBUG: Invalid time range format: {time_range}")
                    except ValueError:
                        print(f"DEBUG: Could not parse plot ID from input name: {input_name}")
                else:
                    print(f"DEBUG: Unexpected input name format: {input_name}")
            else:
                print("DEBUG: Could not determine which input changed")
        except Exception as e:
            print(f"DEBUG: Error updating multi-plot time range: {e}")

    # Event handlers for individual trace time series sliders within multi-plots
    @reactive.Effect
    @reactive.event(
        input.trace_time_range_0_0, input.trace_time_range_0_1, input.trace_time_range_0_2, input.trace_time_range_0_3, input.trace_time_range_0_4,
        input.trace_time_range_1_0, input.trace_time_range_1_1, input.trace_time_range_1_2, input.trace_time_range_1_3, input.trace_time_range_1_4,
        input.trace_time_range_2_0, input.trace_time_range_2_1, input.trace_time_range_2_2, input.trace_time_range_2_3, input.trace_time_range_2_4,
        input.trace_time_range_3_0, input.trace_time_range_3_1, input.trace_time_range_3_2, input.trace_time_range_3_3, input.trace_time_range_3_4,
        input.trace_time_range_4_0, input.trace_time_range_4_1, input.trace_time_range_4_2, input.trace_time_range_4_3, input.trace_time_range_4_4,
        input.trace_time_range_5_0, input.trace_time_range_5_1, input.trace_time_range_5_2, input.trace_time_range_5_3, input.trace_time_range_5_4,
        input.trace_time_range_6_0, input.trace_time_range_6_1, input.trace_time_range_6_2, input.trace_time_range_6_3, input.trace_time_range_6_4,
        input.trace_time_range_7_0, input.trace_time_range_7_1, input.trace_time_range_7_2, input.trace_time_range_7_3, input.trace_time_range_7_4,
        input.trace_time_range_8_0, input.trace_time_range_8_1, input.trace_time_range_8_2, input.trace_time_range_8_3, input.trace_time_range_8_4,
        input.trace_time_range_9_0, input.trace_time_range_9_1, input.trace_time_range_9_2, input.trace_time_range_9_3, input.trace_time_range_9_4
    )
    def _update_trace_time_range():
        """Update time range for a specific trace within a multi-plot"""
        # Determine which slider triggered this effect via reactive context
        try:
            # Get the current reactive context to determine which input changed
            context = reactive.get_current_context()
            if context and hasattr(context, 'name'):
                input_name = context.name
                print(f"DEBUG: Trace time range slider changed: {input_name}")
                
                # Extract plot ID and trace ID from input name (e.g., "trace_time_range_0_1" -> plot 0, trace 1)
                if "_" in input_name:
                    parts = input_name.split("_")
                    if len(parts) >= 5 and parts[0] == "trace" and parts[1] == "time" and parts[2] == "range":
                        try:
                            plot_id = int(parts[3])
                            trace_id = int(parts[4])
                            print(f"DEBUG: Updating time range for plot {plot_id}, trace {trace_id}")
                            
                            # Get the current time range value
                            time_range = input[input_name]()
                            if time_range and len(time_range) == 2:
                                start_year, end_year = time_range
                                print(f"DEBUG: New time range for plot {plot_id}, trace {trace_id}: {start_year}-{end_year}")
                                
                                # Update the specific trace's time range
                                plots_list = multi_plots_list.get()
                                if plots_list and plot_id < len(plots_list):
                                    plot_config = plots_list[plot_id]
                                    additional_traces = plot_config.get('additional_traces', [])
                                    
                                    if trace_id < len(additional_traces):
                                        trace_config = additional_traces[trace_id]
                                        trace_config['time_range'] = [start_year, end_year]
                                        
                                        # Update the plots list
                                        updated_plots = list(plots_list)
                                        updated_plots[plot_id] = plot_config
                                        multi_plots_list.set(updated_plots)
                                        
                                        # Trigger plot update
                                        plot_update_trigger.set(plot_update_trigger.get() + 1)
                                        print(f"DEBUG: Updated time range for plot {plot_id}, trace {trace_id} to {start_year}-{end_year}")
                                    else:
                                        print(f"DEBUG: Trace {trace_id} not found in plot {plot_id}")
                                else:
                                    print(f"DEBUG: Plot {plot_id} not found in plots list")
                            else:
                                print(f"DEBUG: Invalid time range format: {time_range}")
                        except ValueError:
                            print(f"DEBUG: Could not parse plot/trace IDs from input name: {input_name}")
                    else:
                        print(f"DEBUG: Unexpected input name format: {input_name}")
                else:
                    print(f"DEBUG: Unexpected input name format: {input_name}")
            else:
                print("DEBUG: Could not determine which input changed")
        except Exception as e:
            print(f"DEBUG: Error updating trace time range: {e}")
    # Function to generate multi-plot visualizations
    def generate_multi_plot(plot_config, plot_id):
        """Generate a single multi-plot visualization"""
        try:
            print(f"DEBUG: generate_multi_plot called for plot {plot_id}")
            
            # Extract plot configuration
            plot_var = plot_config.get('variable')
            plot_model = plot_config.get('model')
            plot_frequency = plot_config.get('frequency', 'annual')
            plot_ssp = plot_config.get('ssp')
            plot_lat_idx = plot_config.get('lat_idx')
            plot_lon_idx = plot_config.get('lon_idx')
            additional_traces = plot_config.get('additional_traces', [])
            
            print(f"DEBUG: generate_multi_plot - Plot {plot_id}: {plot_var}, {plot_model}, {plot_frequency}, {plot_ssp}, grid ({plot_lat_idx}, {plot_lon_idx})")
            print(f"DEBUG: generate_multi_plot - Additional traces: {len(additional_traces)}")
            
            if not all([plot_var, plot_model, plot_ssp, plot_lat_idx is not None, plot_lon_idx is not None]):
                print(f"DEBUG: generate_multi_plot - Incomplete plot configuration for plot {plot_id}")
                return None
            
            # Determine file path for main plot
            filepaths_map = reactive_netcdf_filepaths.get()
            if not filepaths_map:
                print(f"DEBUG: generate_multi_plot - No filepaths available")
                return None
            
            # Handle observed variables
            if plot_var.startswith('obs_'):
                # For observed data, use the observed dataset loading function
                ds = load_observed_dataset(plot_var)
                if ds is None:
                    print(f"DEBUG: generate_multi_plot - Could not load observed dataset for {plot_var}")
                    return None
                file_path = None  # Observed data doesn't use file paths
                print(f"DEBUG: generate_multi_plot - Loaded observed dataset for {plot_var}")
            else:
                # For projected data, find the appropriate file
                file_key = (plot_var, plot_model, plot_ssp)
                if file_key not in filepaths_map:
                    print(f"DEBUG: generate_multi_plot - File not found for key: {file_key}")
                    return None
                
                file_path = filepaths_map[file_key]
                print(f"DEBUG: generate_multi_plot - Using file: {file_path}")
                
                # Load the dataset
                try:
                    ds = xr.open_dataset(file_path, engine="netcdf4", decode_times=False)
                    print(f"DEBUG: generate_multi_plot - Successfully loaded dataset")
                except Exception as e:
                    print(f"DEBUG: generate_multi_plot - Error loading dataset: {e}")
                    return None
            
            # Extract data for the specified grid cell
            try:
                if plot_var.startswith('obs_'):
                    # For observed data, extract using the observed data function
                    df_processed = extract_timeseries(ds, plot_lat_idx, plot_lon_idx, plot_var)
                    if df_processed is None or df_processed.empty:
                        print(f"DEBUG: generate_multi_plot - No observed data found for grid ({plot_lat_idx}, {plot_lon_idx})")
                        return None
                else:
                    # For projected data, extract using the standard function
                    df_processed = extract_timeseries(ds, plot_lat_idx, plot_lon_idx, plot_var)
                    if df_processed is None or df_processed.empty:
                        print(f"DEBUG: generate_multi_plot - No projected data found for grid ({plot_lat_idx}, {plot_lon_idx})")
                        return None
                
                print(f"DEBUG: generate_multi_plot - Extracted {len(df_processed)} rows of data")
            except Exception as e:
                print(f"DEBUG: generate_multi_plot - Error extracting data: {e}")
                return None
            
            # Process the data based on frequency
            try:
                if plot_frequency == 'daily':
                    # Daily data - no resampling needed
                    df_freq = df_processed.copy()
                    print(f"DEBUG: generate_multi_plot - Using daily data with {len(df_freq)} rows")
                elif plot_frequency == 'monthly':
                    # For precipitation, sum to get monthly totals; for other variables, average
                    if plot_var in ["pr", "obs_pr"]:
                        df_freq = df_processed.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                        print(f"DEBUG: generate_multi_plot - Created monthly total precipitation data with {len(df_freq)} rows")
                    else:
                        df_freq = df_processed.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                        print(f"DEBUG: generate_multi_plot - Created monthly average data with {len(df_freq)} rows")
                    df_freq["Year"] = df_freq["Date"].dt.year
                elif plot_frequency == 'annual':
                    # For precipitation, sum to get annual totals; for other variables, average
                    if plot_var in ["pr", "obs_pr"]:
                        df_freq = df_processed.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                        print(f"DEBUG: generate_multi_plot - Created annual total precipitation data with {len(df_freq)} rows")
                    else:
                        df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                        print(f"DEBUG: generate_multi_plot - Created annual average data with {len(df_freq)} rows")
                    df_freq["Year"] = df_freq["Date"].dt.year
                else:
                    # Default to annual if frequency not specified
                    df_freq = df_processed.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                    df_freq["Year"] = df_freq["Date"].dt.year
                    print(f"DEBUG: generate_multi_plot - Defaulted to annual averages with {len(df_freq)} rows")
                
                # Handle temperature unit conversion for observed data
                if plot_var in ("obs_tmin", "obs_tmax"):
                    # Observed temperature to °F - Livneh data is always in Celsius
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    # Livneh observed data is in Celsius - always convert to Fahrenheit
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    print(f"DEBUG: generate_multi_plot - Sample values before conversion: {sample_values}")
                    print(f"DEBUG: generate_multi_plot - Average sample value: {avg_value:.2f}")
                    
                    # Convert from Celsius to Fahrenheit for observed data
                    if src_units in ['c', 'degc', 'celsius', '°c', 'deg_c', 'degrees_celsius']:
                        print(f"DEBUG: generate_multi_plot - Units explicitly indicate Celsius - converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] * 9/5) + 32
                        print(f"DEBUG: generate_multi_plot - Sample values after Celsius conversion: {df_freq[plot_var].head().tolist()}")
                    elif src_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                        print(f"DEBUG: generate_multi_plot - Units explicitly indicate Kelvin - converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                        print(f"DEBUG: generate_multi_plot - Sample values after Kelvin conversion: {df_freq[plot_var].head().tolist()}")
                    elif src_units in ['f', 'degf', 'fahrenheit', '°f', 'deg_f', 'degrees_fahrenheit']:
                        print(f"DEBUG: generate_multi_plot - Units explicitly indicate Fahrenheit - no conversion needed")
                        # Data is already in Fahrenheit, no conversion needed
                    else:
                        # For Livneh observed data, if no units specified, assume Celsius (as confirmed by user)
                        print(f"DEBUG: generate_multi_plot - No units specified - Livneh data is in Celsius, converting to Fahrenheit")
                        df_freq[plot_var] = (df_freq[plot_var] * 9/5) + 32
                        print(f"DEBUG: generate_multi_plot - Sample values after Celsius conversion: {df_freq[plot_var].head().tolist()}")
                    display_units = "°F"
                
                # Handle precipitation unit conversion for observed data
                elif plot_var == "obs_pr":
                    # Observed precipitation to inches - Livneh data is in mm/day
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    # Livneh observed precipitation data is in mm/day - always convert to inches
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    print(f"DEBUG: generate_multi_plot - Sample precipitation values before conversion: {sample_values}")
                    print(f"DEBUG: generate_multi_plot - Average sample precipitation value: {avg_value:.2f}")
                    
                    # Convert from mm/day to inches
                    if src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                        df_freq[plot_var] = df_freq[plot_var] / 25.4
                        print(f"DEBUG: generate_multi_plot - Converted from mm/day to inches")
                        display_units = "inches/day" # Always inches for all frequencies
                    elif src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        print(f"DEBUG: generate_multi_plot - Converted from kg m-2 s-1 to inches")
                        display_units = "inches/day" # Always inches for all frequencies
                    else:
                        # For Livneh observed data, if no units specified, assume mm/day (as confirmed by user)
                        print(f"DEBUG: generate_multi_plot - No units specified - Livneh precipitation data is in mm/day, converting to inches")
                        df_freq[plot_var] = (df_freq[plot_var] / 25.4)
                        print(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day" # Always inches for all frequencies
                
                # Handle projected precipitation unit conversion
                elif plot_var == "pr":
                    # Projected precipitation conversion - typically in kg m-2 s-1
                    try:
                        # Get the actual data variable name from the dataset
                        data_vars = [v for v in ds.data_vars.keys() if v != 'spatial_ref']
                        if data_vars:
                            src_units = ds[data_vars[0]].attrs.get('units', '').lower()
                        else:
                            src_units = ''
                    except Exception:
                        src_units = ''
                    
                    sample_values = df_freq[plot_var].dropna().head(10).tolist()
                    avg_value = np.mean(sample_values) if sample_values else 0
                    print(f"DEBUG: generate_multi_plot - Sample projected precipitation values before conversion: {sample_values}")
                    print(f"DEBUG: generate_multi_plot - Average sample projected precipitation value: {avg_value:.2f}")
                    print(f"DEBUG: generate_multi_plot - Projected precipitation units: {src_units}")
                    
                    if src_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        print(f"DEBUG: generate_multi_plot - Converted projected precipitation from kg m-2 s-1 to inches")
                        print(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                    elif src_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                        df_freq[plot_var] = df_freq[plot_var] / 25.4
                        print(f"DEBUG: generate_multi_plot - Converted projected precipitation from mm/day to inches")
                        print(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                    else:
                        # Default: assume kg m-2 s-1
                        print(f"DEBUG: generate_multi_plot - No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                        df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                        print(f"DEBUG: generate_multi_plot - Sample values after conversion: {df_freq[plot_var].head().tolist()}")
                        display_units = "inches/day"
                
                # Handle projected data unit conversion
                elif not plot_var.startswith('obs_'):
                    # Projected data unit conversion
                    if plot_var in ["tasmin", "tasmax"]:
                        # Temperature from Kelvin to Fahrenheit
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                            df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                            display_units = "°F"
                        elif original_units in ['c', 'degc', 'celsius', '°c', 'deg_c', 'degrees_celsius']:
                            df_freq[plot_var] = df_freq[plot_var] * 9/5 + 32
                            display_units = "°F"
                        elif original_units in ['f', 'degf', 'fahrenheit', '°f', 'deg_f', 'degrees_fahrenheit']:
                            display_units = "°F"
                        else:
                            # Default: assume Kelvin
                            df_freq[plot_var] = (df_freq[plot_var] - 273.15) * 9/5 + 32
                            display_units = "°F"
                    elif plot_var == "pr":
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['kg m-2 s-1', 'kg/m2/s']:
                            df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                            # All precipitation data should be in "inches" - daily totals, monthly averages, annual averages
                            display_units = "inches/day"
                        else:
                            # Default: assume kg m-2 s-1
                            df_freq[plot_var] = df_freq[plot_var] * 86400 / 25.4
                            display_units = "inches/day"
                    elif plot_var in ["hursmax", "hursmin"]:
                        # Relative humidity conversion - typically in % but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['%', 'percent', 'pct']:
                            # Already in percentage, no conversion needed
                            print(f"DEBUG: generate_multi_plot - {plot_var} already in percentage - no conversion needed")
                            display_units = "%"
                        elif original_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                            # Convert from fraction to percentage
                            df_freq[plot_var] = df_freq[plot_var] * 100
                            print(f"DEBUG: generate_multi_plot - Converted {plot_var} from fraction to percentage")
                            display_units = "%"
                        else:
                            # Default: assume percentage
                            print(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in percentage")
                            display_units = "%"
                    elif plot_var == "huss":
                        # Specific humidity conversion - typically in kg/kg but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                            # Already in kg/kg, no conversion needed
                            print(f"DEBUG: generate_multi_plot - {plot_var} already in kg/kg - no conversion needed")
                            display_units = "kg/kg"
                        elif original_units in ['g/kg', 'g kg-1', 'g per kg']:
                            # Convert from g/kg to kg/kg
                            df_freq[plot_var] = df_freq[plot_var] / 1000
                            print(f"DEBUG: generate_multi_plot - Converted {plot_var} from g/kg to kg/kg")
                            display_units = "kg/kg"
                        else:
                            # Default: assume kg/kg
                            print(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in kg/kg")
                            display_units = "kg/kg"
                    elif plot_var == "rsds":
                        # Radiation conversion - typically in W/m² but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['w/m2', 'w m-2', 'watt/m2', 'watt m-2', 'w/m²']:
                            # Already in W/m², no conversion needed
                            print(f"DEBUG: generate_multi_plot - {plot_var} already in W/m² - no conversion needed")
                            display_units = "W/m²"
                        elif original_units in ['w/m2/day', 'w m-2 day-1', 'watt/m2/day']:
                            # Convert from W/m²/day to W/m² (divide by 86400 seconds)
                            df_freq[plot_var] = df_freq[plot_var] / 86400
                            print(f"DEBUG: generate_multi_plot - Converted {plot_var} from W/m²/day to W/m²")
                            display_units = "W/m²"
                        else:
                            # Default: assume W/m²
                            print(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in W/m²")
                            display_units = "W/m²"
                    elif plot_var == "wspeed":
                        # Wind speed conversion - typically in m/s but may need conversion
                        original_units = ds[plot_var].attrs.get('units', '').lower()
                        if original_units in ['m/s', 'm s-1', 'meter/s', 'meter s-1', 'ms-1']:
                            # Already in m/s, no conversion needed
                            print(f"DEBUG: generate_multi_plot - {plot_var} already in m/s - no conversion needed")
                            display_units = "m/s"
                        elif original_units in ['km/h', 'kmh', 'km per hour']:
                            # Convert from km/h to m/s
                            df_freq[plot_var] = df_freq[plot_var] * 1000 / 3600
                            print(f"DEBUG: generate_multi_plot - Converted {plot_var} from km/h to m/s")
                            display_units = "m/s"
                        elif original_units in ['knots', 'kt', 'kts']:
                            # Convert from knots to m/s
                            df_freq[plot_var] = df_freq[plot_var] * 0.514444
                            print(f"DEBUG: generate_multi_plot - Converted {plot_var} from knots to m/s")
                            display_units = "m/s"
                        else:
                            # Default: assume m/s
                            print(f"DEBUG: generate_multi_plot - No units specified - assuming {plot_var} is in m/s")
                            display_units = "m/s"
                    else:
                        # For other variables, use the units from the dataset
                        display_units = ds[plot_var].attrs.get('units', '')
                
                print(f"DEBUG: generate_multi_plot - Data processing complete, display units: {display_units}")
                
            except Exception as e:
                print(f"DEBUG: generate_multi_plot - Error processing data: {e}")
                return None
            
            # Create the plot
            try:
                fig = go.Figure()
                
                # Get plot metadata
                plot_meta = get_variable_metadata(plot_var)
                plot_title = f"{get_variable_display_name(plot_var)} - Grid {plot_lat_idx},{plot_lon_idx}"
                
                # Add main trace
                if plot_frequency == 'daily':
                    x_values = df_freq["Date"]
                    y_values = df_freq[plot_var]
                    x_label = "Date"
                else:
                    x_values = df_freq["Year"]
                    y_values = df_freq[plot_var]
                    x_label = "Year"
                
                # Add main trace
                fig.add_trace(go.Scatter(
                    x=x_values, 
                    y=y_values,
                    mode='lines+markers',
                    name=f"{get_variable_display_name(plot_var)} ({plot_model}, {plot_frequency}, {plot_ssp})",
                    line=dict(color=plot_meta['color'], width=2),
                    marker=dict(size=4)
                ))
                
                # Add additional traces if any
                for trace_idx, trace_config in enumerate(additional_traces):
                    try:
                        trace_var = trace_config.get('variable')
                        trace_model = trace_config.get('model')
                        trace_frequency = trace_config.get('frequency', 'annual')
                        trace_ssp = trace_config.get('ssp')
                        trace_lat_idx = trace_config.get('lat_idx')
                        trace_lon_idx = trace_config.get('lon_idx')
                        
                        if not all([trace_var, trace_model, trace_ssp, trace_lat_idx is not None, trace_lon_idx is not None]):
                            print(f"DEBUG: generate_multi_plot - Incomplete trace configuration for trace {trace_idx}")
                            continue
                        
                        # Load trace data
                        if trace_var.startswith('obs_'):
                            trace_ds = load_observed_dataset(trace_var)
                            if trace_ds is None:
                                print(f"DEBUG: generate_multi_plot - Could not load observed dataset for trace {trace_var}")
                                continue
                            trace_file_path = None
                        else:
                            trace_file_key = (trace_var, trace_model, trace_ssp)
                            if trace_file_key not in filepaths_map:
                                print(f"DEBUG: generate_multi_plot - File not found for trace key: {trace_file_key}")
                                continue
                            trace_file_path = filepaths_map[trace_file_key]
                            try:
                                trace_ds = xr.open_dataset(trace_file_path, engine="netcdf4", decode_times=False)
                            except Exception as e:
                                print(f"DEBUG: generate_multi_plot - Error loading trace dataset: {e}")
                                continue
                    
                        # Extract trace data
                        if trace_var.startswith('obs_'):
                            trace_df = extract_timeseries(trace_ds, trace_lat_idx, trace_lon_idx, trace_var)
                        else:
                            trace_df = extract_timeseries(trace_ds, trace_var, trace_lat_idx, trace_lon_idx)
                        
                        if trace_df is None or trace_df.empty:
                            print(f"DEBUG: generate_multi_plot - No data found for trace {trace_idx}")
                            continue
                        
                        # Process trace data based on frequency
                        if trace_frequency == 'daily':
                            trace_df_freq = trace_df.copy()
                            # Add Year column for consistency
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        elif trace_frequency == 'monthly':
                            # For precipitation, sum to get monthly totals; for other variables, average
                            if trace_var in ["pr", "obs_pr"]:
                                trace_df_freq = trace_df.set_index("Date").resample('M').sum(numeric_only=True).reset_index()
                                print(f"DEBUG: Multi-plot trace - Created monthly total precipitation data with {len(trace_df_freq)} rows")
                            else:
                                trace_df_freq = trace_df.set_index("Date").resample('M').mean(numeric_only=True).reset_index()
                                print(f"DEBUG: Multi-plot trace - Created monthly average data with {len(trace_df_freq)} rows")
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        elif trace_frequency == 'annual':
                            # For precipitation, sum to get annual totals; for other variables, average
                            if trace_var in ["pr", "obs_pr"]:
                                trace_df_freq = trace_df.set_index("Date").resample('Y').sum(numeric_only=True).reset_index()
                                print(f"DEBUG: Multi-plot trace - Created annual total precipitation data with {len(trace_df_freq)} rows")
                            else:
                                trace_df_freq = trace_df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                                print(f"DEBUG: Multi-plot trace - Created annual average data with {len(trace_df_freq)} rows")
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                        else:
                            trace_df_freq = trace_df.set_index("Date").resample('Y').mean(numeric_only=True).reset_index()
                            trace_df_freq["Year"] = trace_df_freq["Date"].dt.year
                            print(f"DEBUG: Multi-plot trace - Defaulted to annual averages with {len(trace_df_freq)} rows")
                        
                        # Handle trace unit conversion for all climate variables
                        if trace_var in ("obs_tmin", "obs_tmax"):
                            # Observed temperature conversion - Livneh data is always in Celsius
                            print(f"DEBUG: Multi-plot trace - Converting observed temperature {trace_var} from Celsius to Fahrenheit")
                            trace_df_freq[trace_var] = (trace_df_freq[trace_var] * 9/5) + 32
                        elif trace_var == "obs_pr":
                            # Observed precipitation conversion - Livneh data is in mm/day
                            print(f"DEBUG: Multi-plot trace - Converting observed precipitation from mm/day to inches")
                            trace_df_freq[trace_var] = trace_df_freq[trace_var] / 25.4
                        elif not trace_var.startswith('obs_'):
                            # Projected data conversion for all climate variables
                            if trace_var in ["tasmin", "tasmax"]:
                                # Projected temperature conversion - typically in Kelvin
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['k', 'kelvin', 'degk', 'deg_k', 'degrees_kelvin']:
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] - 273.15) * 9/5 + 32
                                    print(f"DEBUG: Multi-plot trace - Converted projected temperature {trace_var} from Kelvin to Fahrenheit")
                                elif original_units in ['c', 'celsius', '°c']:
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] * 9/5) + 32
                                    print(f"DEBUG: Multi-plot trace - Converted projected temperature {trace_var} from Celsius to Fahrenheit")
                                else:
                                    # Default: assume Kelvin for projected temperature
                                    trace_df_freq[trace_var] = (trace_df_freq[trace_var] - 273.15) * 9/5 + 32
                                    print(f"DEBUG: Multi-plot trace - No units specified - assuming projected temperature {trace_var} is in Kelvin, converting to Fahrenheit")
                            elif trace_var == "pr":
                                # Projected precipitation conversion - typically in kg m-2 s-1
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['kg m-2 s-1', 'kg/m2/s', 'kg m^-2 s^-1']:
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 86400 / 25.4
                                    print(f"DEBUG: Multi-plot trace - Converted projected precipitation from kg m-2 s-1 to inches")
                                elif original_units in ['mm/day', 'mm d-1', 'mm day-1', 'mm/d', 'mm per day']:
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] / 25.4
                                    print(f"DEBUG: Multi-plot trace - Converted projected precipitation from mm/day to inches")
                                else:
                                    # Default: assume kg m-2 s-1 for projected precipitation
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 86400 / 25.4
                                    print(f"DEBUG: Multi-plot trace - No units specified - assuming projected precipitation is in kg m-2 s-1, converting to inches")
                            elif trace_var in ["hursmax", "hursmin"]:
                                # Relative humidity conversion - typically in % but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['%', 'percent', 'pct']:
                                    # Already in percentage, no conversion needed
                                    print(f"DEBUG: Multi-plot trace - {trace_var} already in percentage - no conversion needed")
                                elif original_units in ['fraction', 'ratio', '0-1', '0 to 1']:
                                    # Convert from fraction to percentage
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 100
                                    print(f"DEBUG: Multi-plot trace - Converted {trace_var} from fraction to percentage")
                                else:
                                    # Default: assume percentage
                                    print(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in percentage")
                            elif trace_var == "huss":
                                # Specific humidity conversion - typically in kg/kg but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['kg/kg', 'kg kg-1', 'kg per kg']:
                                    # Already in kg/kg, no conversion needed
                                    print(f"DEBUG: Multi-plot trace - {trace_var} already in kg/kg - no conversion needed")
                                elif original_units in ['g/kg', 'g kg-1', 'g per kg']:
                                    # Convert from g/kg to kg/kg
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] / 1000
                                    print(f"DEBUG: Multi-plot trace - Converted {trace_var} from g/kg to kg/kg")
                                else:
                                    # Default: assume kg/kg
                                    print(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in kg/kg")
                            elif trace_var == "rsds":
                                # Radiation - typically already in W/m² (no conversion needed)
                                # NOTE: Climate model radiation data is standardized to W/m² (watts per square meter)
                                print(f"DEBUG: Multi-plot trace - {trace_var} assumed to be in W/m² (standard for climate models) - no conversion needed")
                            elif trace_var == "wspeed":
                                # Wind speed conversion - typically in m/s but may need conversion
                                original_units = trace_ds[trace_var].attrs.get('units', '').lower()
                                if original_units in ['m/s', 'm s-1', 'm per s', 'meter/s', 'meter per second']:
                                    # Already in m/s, no conversion needed
                                    print(f"DEBUG: Multi-plot trace - {trace_var} already in m/s - no conversion needed")
                                elif original_units in ['km/h', 'km h-1', 'km per h', 'kilometer/h', 'kilometer per hour']:
                                    # Convert from km/h to m/s
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 1000 / 3600
                                    print(f"DEBUG: Multi-plot trace - Converted {trace_var} from km/h to m/s")
                                elif original_units in ['knots', 'kt', 'kts']:
                                    # Convert from knots to m/s
                                    trace_df_freq[trace_var] = trace_df_freq[trace_var] * 0.514444
                                    print(f"DEBUG: Multi-plot trace - Converted {trace_var} from knots to m/s")
                                else:
                                    # Default: assume m/s
                                    print(f"DEBUG: Multi-plot trace - No units specified - assuming {trace_var} is in m/s")
                        
                        # Add trace to plot
                            if trace_frequency == 'daily':
                                trace_x_values = trace_df_freq["Date"]
                            else:
                                trace_x_values = trace_df_freq["Year"]
                            trace_y_values = trace_df_freq[trace_var]
                        
                        trace_meta = get_variable_metadata(trace_var)
                        trace_name = f"{get_variable_display_name(trace_var)} ({trace_model}, {trace_frequency}, {trace_ssp}) - Grid {trace_lat_idx},{trace_lon_idx}"
                        
                        fig.add_trace(go.Scatter(
                            x=trace_x_values, 
                            y=trace_y_values,
                            mode='lines+markers',
                            name=trace_name,
                            line=dict(color=trace_meta['color'], width=2, dash='dash'),
                            marker=dict(size=3)
                        ))
                        
                        print(f"DEBUG: generate_multi_plot - Added trace {trace_idx}: {trace_name}")
                        
                    except Exception as e:
                        print(f"DEBUG: generate_multi_plot - Error processing trace {trace_idx}: {e}")
                        continue
                
                # Set up plot layout
                fig.update_layout(
                    title=plot_title,
                    xaxis_title=x_label,
                    yaxis_title=f"{get_variable_display_name(plot_var)} ({display_units})",
                    xaxis=dict(
                        range=[1950, 2100],  # Fixed x-axis range from 1950-2100
                        automargin=True,
                        dtick=10,  # Show ticks every 10 years
                        tick0=1950,  # Start ticks at 1950
                        tickmode='linear'  # Linear tick spacing
                    ),
                    yaxis=dict(
                        autorange=True,  # Automatically adjust y-axis to show full data range
                        automargin=True  # Automatically adjust margins
                    ),
                    height=600,  # Match the 350px container height
                    showlegend=True,
                    legend=dict(
                        orientation="h",  # Horizontal below plot for unlimited traces
                        yanchor="top",
                        y=-0.12,  # Below plot area
                        xanchor="center",
                        x=0.5,  # Centered below plot
                        bgcolor="rgba(255,255,255,0.95)",
                        bordercolor="rgba(0,0,0,0.3)",
                        borderwidth=1,
                        font=dict(size=9),  # Compact font for many traces
                        itemsizing="constant",
                        itemclick="toggle",  # Click to toggle traces
                        itemdoubleclick="toggleothers",  # Double-click to isolate
                        tracegroupgap=5,  # Small gap between items
                        traceorder="normal"  # Keep trace order
                    ),
                    margin=dict(l=60, t=80, r=60, b=150)  # Extra bottom margin for legend
                )
                
                # Note: Download legend configuration removed from main display
                # Download legend positioning should be handled separately if needed
                
                print(f"DEBUG: generate_multi_plot - Successfully created plot {plot_id}")
                return fig
                
            except Exception as e:
                print(f"DEBUG: generate_multi_plot - Error creating plot: {e}")
                return None
                
        except Exception as e:
            print(f"DEBUG: generate_multi_plot - Unexpected error: {e}")
            return None

    # Initialize startup sequence
    startup_trigger.set(1)
    
    # Variable Maps page functions - temporarily commented out
    # @output
    # @render.ui
    # def variable_map_controls():
    #     """Render controls for the variable maps page"""
    #     filepaths_map = reactive_netcdf_filepaths.get()
    #     if not filepaths_map:
    #         return ui.div(
    #             ui.p("Loading data...", class_="text-muted"),
    #             class_="text-center"
    #         )
    #     
    #     # Get available variables, models, and scenarios
    #     variables = sorted(list(set([var for var, _, _ in filepaths_map.keys()])))
    #     models = sorted(list(set([model for _, model, _ in filepaths_map.keys()])))
    #     scenarios = sorted(list(set([ssp for _, _, ssp in filepaths_map.keys()])))
    #     
    #     return ui.div(
    #         ui.h5("Select Climate Variable"),
    #         ui.input_select(
    #             "variable_map_var",
    #             "Variable",
    #             choices={var: get_variable_display_name(var) for var in variables},
    #             selected=variables[0] if variables else None
    #         ),
    #         ui.h5("Select Climate Model"),
    #         ui.input_select(
    #             "variable_map_model", 
    #             "Model",
    #             choices={model: model for model in models},
    #             selected=models[0] if models else None
    #         ),
    #         ui.h5("Select SSP Scenario"),
    #         ui.input_select(
    #             "variable_map_ssp",
    #             "Scenario", 
    #             choices={ssp: ssp for ssp in scenarios},
    #             selected=scenarios[0] if scenarios else None
    #         ),
    #         ui.h5("Select Time Period"),
    #         ui.input_slider(
    #             "variable_map_year",
    #             "Year",
    #             min=1950,
    #             max=2100,
    #             value=2050,
    #             step=10
    #         ),
    #         class_="p-3"
    #     )
    # 
    # @output
    # @render_widget
    # def variable_spatial_map():
    #     """Render spatial map for climate variables"""
    #     try:
    #         # Get selections
    #         variable = input.variable_map_var()
    #         model = input.variable_map_model()
    #         ssp = input.variable_map_ssp()
    #         year = input.variable_map_year()
    #         
    #         if not all([variable, model, ssp]):
    #             return create_empty_map("Please select variable, model, and scenario")
    #         
    #         # Get file path
    #         filepaths_map = reactive_netcdf_filepaths.get()
    #         file_key = (variable, model, ssp)
    #         
    #         if file_key not in filepaths_map:
    #             return create_empty_map(f"No data available for {variable}, {model}, {ssp}")
    #         
    #         # Load dataset
    #         cached_datasets = _cached_datasets.get()
    #         ds = cached_datasets.get(file_key)
    #         
    #         if ds is None:
    #             try:
    #                 file_path = filepaths_map[file_key]
    #                 ds = xr.open_dataset(file_path)
    #                 # Cache the dataset
    #                 cached_datasets[file_key] = ds
    #                 _cached_datasets.set(cached_datasets)
    #             except Exception as e:
    #                 return create_empty_map(f"Error loading data: {str(e)}")
    #         
    #         # Extract data for the selected year
    #         try:
    #             if 'time' in ds.coords:
    #                 # Find closest time to selected year
    #                 year_data = ds.sel(time=str(year), method='nearest')
    #             else:
    #                 # If no time dimension, use the entire dataset
    #                 year_data = ds
    #             
    #             # Get the data variable
    #             var_name = variable
    #             if var_name not in year_data.data_vars:
    #                 var_name = list(year_data.data_vars)[0]
    #             
    #             data = year_data[var_name]
    #             
    #             # Convert to numpy arrays
    #             if hasattr(data, 'values'):
    #                 values = data.values
    #             else:
    #                 values = data
    #             
    #             # Get coordinates
    #             if 'lat' in data.coords and 'lon' in data.coords:
    #                 lats = data.lat.values
    #                 lons = data.lon.values
    #             elif 'latitude' in data.coords and 'longitude' in data.coords:
    #                 lats = data.latitude.values
    #                 lons = data.longitude.values
    #             else:
    #                 return create_empty_map("Unable to extract coordinates")
    #             
    #             # Filter to Santa Barbara region
    #             sb_mask = (
    #                 (lats >= SB_LAT_MIN) & (lats <= SB_LAT_MAX) &
    #                 (lons >= SB_LON_MIN) & (lons <= SB_LON_MAX)
    #             )
    #             
    #             if not np.any(sb_mask):
    #                 return create_empty_map("No data available for Santa Barbara region")
    #             
    #             # Create the map
    #             fig = go.Figure()
    #             
    #             # Add heatmap
    #             fig.add_trace(go.Heatmap(
    #                 z=values,
    #                 x=lons,
    #                 y=lats,
    #                 colorscale='Viridis',
    #                 showscale=True,
    #                 colorbar=dict(
    #                     title=get_variable_metadata(variable)['units']
    #                 )
    #             ))
    #             
    #             # Update layout
    #             fig.update_layout(
    #                 title=f"{get_variable_display_name(variable)} - {model} ({ssp}) - {year}",
    #                 mapbox=dict(
    #                     style="open-street-map",
    #                     center=dict(lat=(SB_LAT_MIN + SB_LAT_MAX) / 2, lon=(SB_LON_MIN + SB_LON_MAX) / 2),
    #                     zoom=8
    #                 ),
    #                 height=600,
    #                 margin=dict(l=0, r=0, t=50, b=0)
    #             )
    #             
    #             return fig
    #             
    #         except Exception as e:
    #             return create_empty_map(f"Error processing data: {str(e)}")
    #             
    #     except Exception as e:
    #         return create_empty_map(f"Error creating map: {str(e)}")

# Create the Shiny app instance
app = App(app_ui, server)

if __name__ == "__main__":
    import subprocess
    import socket
    
    def force_kill_port(port):
        """Forcefully kill any process on a port"""
        try:
            result = subprocess.run(
                ['lsof', '-ti', f':{port}'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    subprocess.run(['kill', '-9', pid], capture_output=True)
                return True
        except:
            pass
        return False
    
    def aggressive_cleanup():
        """Aggressively clean up all ports and processes"""
        print("🔧 Cleaning up ports and processes...")
        
        # FIXED: Only clean up old processes, not the current one
        try:
            # First, get current process PID to avoid killing ourselves
            current_pid = os.getpid()
            
            # Find all Python processes running app.py
            result = subprocess.run(
                ['pgrep', '-f', 'python.*app.py'],
                capture_output=True,
                text=True,
                timeout=3
            )
            
            if result.stdout.strip():
                pids = result.stdout.strip().split('\n')
                for pid in pids:
                    try:
                        pid_int = int(pid.strip())
                        # FIXED: Skip current process and ensure PID is valid
                        if pid_int != current_pid and pid_int > 0:
                            subprocess.run(['kill', '-9', str(pid_int)], capture_output=True, timeout=1)
                    except (ValueError, Exception):
                        # Skip invalid PIDs
                        pass
        except:
            pass
        
        # FIXED: Only kill specific problematic ports, not a wide range that might affect startup
        # Focus on commonly used ports that might be stuck
        problematic_ports = [8080, 8000, 8888]
        for port in problematic_ports:
            try:
                # Check if port is in use before trying to kill
                result = subprocess.run(
                    ['lsof', '-ti', f':{port}'],
                    capture_output=True,
                    text=True,
                    timeout=2
                )
                if result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        try:
                            pid_int = int(pid.strip())
                            # Ensure we're not killing ourselves
                            if pid_int != os.getpid():
                                subprocess.run(['kill', '-9', str(pid_int)], capture_output=True, timeout=1)
                        except:
                            pass
            except:
                pass
        
        # Shorter wait time
        time.sleep(0.5)
        print("✓ Cleanup complete\n")
    
    def get_available_port(preferred=8080):
        """Get an available port"""
        for port in range(preferred, preferred + 200):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    s.bind(('0.0.0.0', port))
                    return port
            except:
                continue
        return None
    
    try:
        print("\n" + "=" * 70)
        print("  Santa Barbara Climate Dashboard")
        print("=" * 70 + "\n")
        
        # AGGRESSIVE CLEANUP
        aggressive_cleanup()
        
        # Find available port
        port = get_available_port(8080)
        
        if not port:
            print("❌ ERROR: No available ports found")
            exit(1)
        
        print(f"🚀 Starting server on port {port}...")
        print(f"📍 Open your browser to: http://localhost:{port}\n")
        print("Press Ctrl+C to stop\n")
        print("=" * 70 + "\n")
        
        app.run(host="0.0.0.0", port=port)
        
    except KeyboardInterrupt:
        print("\n\n👋 Server stopped")
    except Exception as e:
        print(f"\n❌ Error: {e}")
        import traceback
        traceback.print_exc()
